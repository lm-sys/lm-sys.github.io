<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>OME: Revolutionizing LLM Infrastructure with Model-Driven Architecture | LMSYS Org</title><meta name="title" content="OME: Revolutionizing LLM Infrastructure with Model-Driven Architecture | LMSYS Org"/><meta property="og:title" content="OME: Revolutionizing LLM Infrastructure with Model-Driven Architecture | LMSYS Org"/><meta name="twitter:title" content="OME: Revolutionizing LLM Infrastructure with Model-Driven Architecture | LMSYS Org"/><meta name="description" content="&lt;h2&gt;&lt;a id=&quot;the-tale-of-two-teams-why-model-serving-is-broken&quot; class=&quot;anchor&quot; href=&quot;#the-tale-of-two-teams-why-model-serving-is-broken&quot; aria-hidden=&quot;true&quot;&gt;&lt;sv..."/><meta property="og:description" content="&lt;h2&gt;&lt;a id=&quot;the-tale-of-two-teams-why-model-serving-is-broken&quot; class=&quot;anchor&quot; href=&quot;#the-tale-of-two-teams-why-model-serving-is-broken&quot; aria-hidden=&quot;true&quot;&gt;&lt;sv..."/><meta name="twitter:description" content="&lt;h2&gt;&lt;a id=&quot;the-tale-of-two-teams-why-model-serving-is-broken&quot; class=&quot;anchor&quot; href=&quot;#the-tale-of-two-teams-why-model-serving-is-broken&quot; aria-hidden=&quot;true&quot;&gt;&lt;sv..."/><meta property="og:image" content="https://lmsys.org/images/blog/ome/ome.jpg"/><meta name="twitter:image" content="https://lmsys.org/images/blog/ome/ome.jpg"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2025-07-08-ome"/><meta name="twitter:url" content="https://lmsys.org/blog/2025-07-08-ome"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-eef2afd147d8eda9.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/z7lqLJFZXcwp_rAQo_vTP/_buildManifest.js" defer=""></script><script src="/_next/static/z7lqLJFZXcwp_rAQo_vTP/_ssgManifest.js" defer=""></script><script src="/_next/static/z7lqLJFZXcwp_rAQo_vTP/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">OME: Revolutionizing LLM Infrastructure with Model-Driven Architecture</h1><p class="text-xl pt-2 pb-2">by: <!-- -->The Oracle Team<!-- -->,<!-- --> <!-- -->Jul 08, 2025<!-- --></p><hr/><div class="pt-2 article"><h2><a id="the-tale-of-two-teams-why-model-serving-is-broken" class="anchor" href="#the-tale-of-two-teams-why-model-serving-is-broken" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Tale of Two Teams: Why Model Serving Is Broken</h2>
<p>In any large organization deploying LLMs, two distinct teams emerge with conflicting needs:</p>
<p><strong>The ML Engineers</strong> spend months benchmarking models, experimenting with serving technologies, and crafting optimal deployment strategies. Each model demands different configurations—tensor parallelism for Llama 70B, expert parallelism for DeepSeek V3/R1, specialized settings for multimodal models. The parameters are endless: batch sizes, KV cache configurations, quantization levels. Worse, these configurations shift dramatically across GPU types (H100 vs A100 vs L40S).</p>
<p><strong>The Production Engineers and Data Scientists</strong> just want to deploy models. They shouldn’t need to understand the intricacies of tensor parallelism or why a particular model needs 4 GPUs with NVLink. They have customers waiting, applications to build, and business value to deliver.</p>
<p>This gap creates a fundamental problem: MLEs need a way to encode their hard-won serving knowledge into reusable blueprints. Production teams need to deploy models without becoming distributed systems experts. <strong>The missing link? A system that understands models as first-class citizens.</strong></p>
<h2><a id="the-birth-of-ome" class="anchor" href="#the-birth-of-ome" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Birth of OME</h2>
<p>The Oracle Cloud Infrastructure (OCI) GenAI team faced this exact challenge at scale. Supporting numerous models across diverse GPU hardware, they watched deployment cycles stretch into months. Each new model meant:</p>
<ul>
<li>Weeks of MLE experimentation to find optimal configurations</li>
<li>Complex documentation that production teams struggled to follow</li>
<li>Deployment failures due to misconfiguration</li>
<li>Inability to reuse knowledge across similar models</li>
</ul>
<p>The breakthrough came from a simple insight: <strong>The model itself should drive the deployment.</strong></p>
<p>A Llama model isn’t just a file—it contains metadata about its architecture, parameter count, and requirements. By making the system model-aware rather than deployment-driven, they could bridge the gap between ML expertise and production simplicity.</p>
<p>This led to OME (Open Model Engine): a Kubernetes operator that treats models as first-class resources. The results were dramatic:</p>
<ul>
<li>Model onboarding time: <strong>Months → Days</strong></li>
<li>Configuration errors: <strong>Dramatically reduced</strong></li>
<li>MLE knowledge: <strong>Captured and reused automatically</strong></li>
<li>Production deployment: <strong>Simple YAML with just a few lines</strong></li>
</ul>
<p>But here’s what makes it revolutionary: the model-driven architecture makes it easy to encode and reuse sophisticated deployment strategies:</p>
<ul>
<li><strong>Multi-node serving</strong>: Deploy massive models like DeepSeek V3 (685B) across multiple nodes with a simple configuration</li>
<li><strong>Prefill-decode disaggregation</strong>: Separate compute-intensive prefill from memory-bound decode, with each component scaling independently</li>
<li><strong>Flexible architectures</strong>: Both prefill and decode can run in single-node or multi-node configurations based on your needs</li>
<li><strong>Serverless deployment</strong>: Scale-to-zero for cost efficiency when models aren’t in use</li>
<li><strong>Business-driven scaling</strong>: Complex autoscaling based on KV cache, tokens/second, latency targets, or any custom metric</li>
</ul>
<p>The model-driven approach doesn’t constrain you—it liberates you. Because OME understands models deeply, it can support any deployment pattern your MLEs design while keeping the interface simple for production teams.</p>
<p><strong>Enter OME</strong>: A Kubernetes-native platform where models become first-class citizens. Let’s explore how OME’s architecture transforms the chaos of LLM deployment into an elegant, scalable system that serves everyone from ML researchers to production engineers.</p>
<h2><a id="the-ome-architecture-models-at-the-center" class="anchor" href="#the-ome-architecture-models-at-the-center" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The OME Architecture: Models at the Center</h2>
<p><img src="/images/blog/ome/ome-architecture.svg" alt="ome-architecture.svg"></p>
<h3><a id="layer-1-kubernetes-api-layer" class="anchor" href="#layer-1-kubernetes-api-layer" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Layer 1: Kubernetes API Layer</strong></h3>
<p>While users—MLEs, data scientists, production engineers, and applications—interact with OME through simple interfaces, the real magic happens in the Kubernetes API layer below.</p>
<h3><a id="custom-resources---the-foundation-of-model-driven-architecture" class="anchor" href="#custom-resources---the-foundation-of-model-driven-architecture" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Custom Resources - The Foundation of Model-Driven Architecture</strong></h3>
<p>At the heart of OME lies its Custom Resource Definitions (CRDs), which transform Kubernetes from a generic container orchestrator into an ML platform. These aren’t just configuration files—they’re the language through which you express your ML requirements.</p>
<h3><a id="basemodelclusterbasemodel-models-as-first-class-citizens" class="anchor" href="#basemodelclusterbasemodel-models-as-first-class-citizens" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>BaseModel/ClusterBaseModel: Models as First-Class Citizens</strong></h3>
<p><strong>What is a Base Model?</strong></p>
<p>A Base Model in OME is a Kubernetes resource that represents a foundation AI model (like GPT, Llama, or Mistral) that you want to use for inference workloads. Think of it as a blueprint that tells OME where to find your model, how to download it, and where to store it on your cluster nodes.</p>
<p>When you create a BaseModel resource, OME automatically handles the complex process of downloading the model files, parsing the model’s configuration to understand its capabilities, and making it available across your cluster nodes where AI workloads can use it.</p>
<p><strong>BaseModel vs ClusterBaseModel</strong></p>
<p>OME provides two types of model resources:</p>
<ul>
<li><strong>BaseModel</strong> is namespace-scoped, meaning it exists within a specific Kubernetes namespace. If you create a BaseModel in the “team-a” namespace, only workloads in that namespace can use it. This is perfect for team-specific models or when you want to isolate model access.</li>
<li><strong>ClusterBaseModel</strong> is cluster-scoped, meaning it’s available to workloads in any namespace across your entire cluster. This is ideal for organization-wide models that multiple teams need to access, like a shared Llama-3 model that everyone uses.</li>
</ul>
<p>Both types use exactly the same specification format—the only difference is their visibility scope.</p>
<p>Traditional platforms treat models as static files to be downloaded and mounted. OME revolutionizes this by making models intelligent, versioned resources that understand their own requirements:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">ome.io/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterBaseModel</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">llama-3-70b-instruct</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">vendor:</span> <span class="hljs-string">meta</span>
  <span class="hljs-attr">modelType:</span> <span class="hljs-string">llama</span>
  <span class="hljs-attr">modelArchitecture:</span> <span class="hljs-string">LlamaForCausalLM</span>
  <span class="hljs-attr">modelParameterSize:</span> <span class="hljs-string">&quot;70B&quot;</span>
  <span class="hljs-attr">quantization:</span> <span class="hljs-string">fp16</span>
  <span class="hljs-attr">storage:</span>
    <span class="hljs-attr">storageUri:</span> <span class="hljs-string">&quot;hf://meta-llama/Llama-3.3-70B-Instruct&quot;</span>
    <span class="hljs-attr">path:</span> <span class="hljs-string">&quot;/models/llama-3.3-70b&quot;</span>
    <span class="hljs-attr">nodeSelector:</span>
      <span class="hljs-attr">gpu.memory:</span> <span class="hljs-string">&quot;80Gi&quot;</span>  <span class="hljs-comment"># Only download to nodes with sufficient GPU memory</span>
</code></pre>
<p>When you create a BaseModel resource, OME’s control plane and data plane components work together to make the model available across your cluster. The BaseModel CRD acts as the declarative specification, while the actual work of downloading, parsing, and distributing models happens in the data plane through the Model Agent.</p>
<h3><a id="servingruntime-the-brain-of-runtime-selection" class="anchor" href="#servingruntime-the-brain-of-runtime-selection" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>ServingRuntime: The Brain of Runtime Selection</strong></h3>
<p>ClusterServingRuntime is a cluster-scoped resource that manages the runtime environment for model serving. A ClusterServingRuntime defines the templates for Pods that can serve one or more particular models. Each ClusterServingRuntime defines key information such as the container image of the runtime and a list of the models that the runtime supports. Other configuration settings for the runtime can be conveyed through environment variables in the container specification.</p>
<p>These CRDs allow for improved flexibility and extensibility, enabling users to quickly define or customize reusable runtimes without having to modify any controller code or any resources in the controller namespace. The only difference between ServingRuntime and ClusterServingRuntime is that one is namespace-scoped and the other is cluster-scoped.</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">ome.io/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterServingRuntime</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">sglang-llama-70b</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">supportedModelFormats:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">modelFormat:</span>
        <span class="hljs-attr">name:</span> <span class="hljs-string">safetensors</span>
      <span class="hljs-attr">modelArchitecture:</span> <span class="hljs-string">LlamaForCausalLM</span>
      <span class="hljs-attr">modelSizeRange:</span>
        <span class="hljs-attr">min:</span> <span class="hljs-string">&quot;65B&quot;</span>
        <span class="hljs-attr">max:</span> <span class="hljs-string">&quot;75B&quot;</span>
      <span class="hljs-attr">autoSelect:</span> <span class="hljs-literal">true</span>
      <span class="hljs-attr">priority:</span> <span class="hljs-number">100</span>
</code></pre>
<p>Full runtime specifications for advanced deployments can be found in the OME repository:</p>
<ul>
<li><a href="https://github.com/sgl-project/ome/blob/main/config/runtimes/srt/llama-4-maverick-17b-128e-instruct-fp8-pd-rt.yaml">Llama 4 Maverick PD Runtime</a> - Prefill-decode disaggregated configuration</li>
<li><a href="https://github.com/sgl-project/ome/blob/main/config/runtimes/srt/deepseek-rdma-pd-rt.yaml">DeepSeek RDMA PD Runtime</a> - Multi-node expert parallel serving with RDMA</li>
</ul>
<p>ServingRuntimes define how to serve different model types, with the actual runtime selection logic handled by the control plane when you create an InferenceService.</p>
<p><strong>Advanced Deployment Architectures</strong></p>
<p>ServingRuntimes serve as blueprints for how router, engine, and decoder components are deployed. Each component (except router) can be configured for single-node, serverless, or multi-node deployment. This flexibility enables cutting-edge serving patterns:
<strong>PD-Disaggregated Serving</strong> - The state-of-the-art for high-performance LLM serving at scale</p>
<p>This isn’t just incremental improvement—it’s a fundamental advancement in serving architecture that OME makes accessible through simple runtime configuration.</p>
<h3><a id="inferenceservice-orchestrating-model-deployments-and-ingress" class="anchor" href="#inferenceservice-orchestrating-model-deployments-and-ingress" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>InferenceService: Orchestrating Model Deployments and Ingress</strong></h3>
<p>An InferenceService is the central Kubernetes resource in OME that orchestrates the complete lifecycle of model serving. It acts as a declarative specification that describes how you want your AI models deployed, scaled, and served across your cluster.</p>
<p>Think of InferenceService as the “deployment blueprint” for your AI workloads. It brings together models (defined by BaseModel/ClusterBaseModel), runtimes (defined by ServingRuntime/ClusterServingRuntime), and infrastructure configuration to create a complete serving solution. InferenceService is what puts models, runtimes, as well as traditional Kubernetes services, complex ingress, scheduling, auto scaling, and permission controls all together to form a complete cluster serving fleet.</p>
<p><strong>Architecture Overview</strong></p>
<p>OME uses a component-based architecture where InferenceService can be composed of multiple specialized components:</p>
<ul>
<li><strong>Model</strong>: References the AI model to serve (BaseModel/ClusterBaseModel)</li>
<li><strong>Runtime</strong>: References the serving runtime environment (ServingRuntime/ClusterServingRuntime)</li>
<li><strong>Engine</strong>: Main inference component that processes requests, typically an OpenAI-compatible server handling request processing, tool parsing, and model backend operations</li>
<li><strong>Decoder</strong>: Optional component for disaggregated serving (prefill-decode separation)</li>
<li><strong>Router</strong>: A standalone high-performance component that enables data parallelism across inference instances, supporting advanced load balancing algorithms (cache-aware, power of two, random, round robin) and acting as a specialized load balancer for prefill-decode disaggregated serving architectures</li>
</ul>
<pre><code class="hljs language-yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">ome.io/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">InferenceService</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">production-chat-service</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">model:</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">llama-3-70b-instruct</span>
  <span class="hljs-attr">engine:</span>
    <span class="hljs-attr">minReplicas:</span> <span class="hljs-number">2</span>
    <span class="hljs-attr">maxReplicas:</span> <span class="hljs-number">10</span>
  <span class="hljs-attr">decoder:</span>  <span class="hljs-comment"># Only created for disaggregated deployments</span>
    <span class="hljs-attr">minReplicas:</span> <span class="hljs-number">4</span>
    <span class="hljs-attr">maxReplicas:</span> <span class="hljs-number">20</span>
  <span class="hljs-attr">router:</span>  <span class="hljs-comment"># Optional optimal serving routing layer</span>
    <span class="hljs-attr">minReplicas:</span> <span class="hljs-number">2</span>
</code></pre>
<p>This component architecture enables sophisticated optimizations impossible with monolithic deployments:</p>
<ul>
<li><strong>Independent Scaling</strong>: Scale compute-heavy prefill separately from memory-bound decode</li>
<li><strong>Resource Optimization</strong>: Routers don’t need GPUs, saving precious accelerator resources</li>
<li><strong>Failure Isolation</strong>: Component failures don’t bring down the entire service</li>
<li><strong>Performance Tuning</strong>: Each component can be optimized for its specific workload</li>
</ul>
<h3><a id="benchmarkjob-performance-testing-as-a-first-class-operation" class="anchor" href="#benchmarkjob-performance-testing-as-a-first-class-operation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>BenchmarkJob: Performance Testing as a First-Class Operation</strong></h3>
<p>OME is the only platform that treats performance testing as a core primitive:</p>
<pre><code class="hljs language-yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">ome.io/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">BenchmarkJob</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">llama-70b-production-benchmark</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-comment"># Target service to benchmark</span>
  <span class="hljs-attr">endpoint:</span>
    <span class="hljs-attr">inferenceService:</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">llama-chat-optimized</span>
  <span class="hljs-attr">outputLocation:</span>
    <span class="hljs-attr">storageUri:</span> <span class="hljs-string">&quot;oci://n/benchmark-results/b/prod/o/llama-70b-bench&quot;</span>
</code></pre>
<p>This isn’t just about running load tests. BenchmarkJob provides:</p>
<ul>
<li><strong>genai-bench integration</strong>: Industry-standard benchmarking tool</li>
<li><strong>Realistic traffic patterns</strong>: Normal distributions, fixed patterns, long-context scenarios</li>
<li><strong>Comprehensive metrics</strong>: Tokens/second, TTFT, latency percentiles</li>
<li><strong>Multi-cloud storage</strong>: Results stored for historical analysis</li>
<li><strong>Service metadata tracking</strong>: GPU types, engine versions for fair comparisons</li>
</ul>
<h3><a id="admission-webhooks-validation-and-mutation" class="anchor" href="#admission-webhooks-validation-and-mutation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Admission Webhooks: Validation and Mutation</strong></h3>
<p>OME’s admission webhooks act as gatekeepers in the API layer:</p>
<ol>
<li><strong>Validating Webhooks</strong> ensure model-runtime compatibility before resources are created, preventing runtime failures</li>
<li><strong>Mutating Webhooks</strong> inject optimal configurations based on model characteristics</li>
<li><strong>Pod Mutating Webhooks</strong> handle complex scenarios like:
<ul>
<li>RDMA configuration for multi-node deployments</li>
<li>GPU affinity rules for optimal memory bandwidth</li>
<li>Security contexts for model encryption</li>
</ul>
</li>
</ol>
<h3><a id="layer-2-control-plane---the-orchestrator" class="anchor" href="#layer-2-control-plane---the-orchestrator" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Layer 2: Control Plane - The Orchestrator</strong></h3>
<p>The control plane is where OME’s main operation lives. This isn’t just CRUD operations on Kubernetes resources—it’s a sophisticated system that makes optimal decisions based on model characteristics, hardware availability, and business requirements.</p>
<h3><a id="ome-controller-manager-the-orchestration-brain" class="anchor" href="#ome-controller-manager-the-orchestration-brain" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>OME Controller Manager: The Orchestration Brain</strong></h3>
<p>The controller manager coordinates all OME operations with a reconciliation loop that’s aware of ML-specific concerns.</p>
<p><strong>Runtime Selection Algorithm</strong></p>
<p>When you deploy a model through an InferenceService, the controller:</p>
<ul>
<li>Matches model characteristics against all available ServingRuntimes</li>
<li>Scores each runtime based on compatibility and optimization potential</li>
<li>Uses <strong>Model Size Range</strong> matching—when multiple runtimes support a model, OME selects the one with the closest size range for optimal performance</li>
<li>Handles edge cases like quantized models or models requiring specific GPU features</li>
</ul>
<p><strong>Component Orchestration</strong></p>
<p>The InferenceService controller orchestrates multiple components based on your deployment requirements:</p>
<pre><code class="hljs language-go"><span class="hljs-comment">// Simplified reconciliation logic showing component-based orchestration</span>
<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *Reconciler)</span></span> Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, <span class="hljs-type">error</span>) {
    <span class="hljs-comment">// 1. Fetch InferenceService and determine deployment mode</span>
    inferenceService := &amp;omev1beta1.InferenceService{}
    deploymentMode := r.inferDeploymentMode(isvc)
    
    <span class="hljs-comment">// 2. Select optimal runtime if not specified</span>
    <span class="hljs-keyword">if</span> isvc.Spec.Runtime == <span class="hljs-literal">nil</span> {
        runtime := r.selectOptimalRuntime(isvc.Spec.Model, deploymentMode)
        isvc.Spec.Runtime = runtime
    }
    
    <span class="hljs-comment">// 3. Reconcile components based on deployment mode</span>
    <span class="hljs-keyword">switch</span> deploymentMode {
    <span class="hljs-keyword">case</span> PDDisaggregated:
        <span class="hljs-comment">// Deploy separate engine (prefill) and decoder components</span>
        r.reconcileRouter(isvc)    <span class="hljs-comment">// Cache-aware routing</span>
        r.reconcileEngine(isvc)    <span class="hljs-comment">// Prefill processing</span>
        r.reconcileDecoder(isvc)   <span class="hljs-comment">// Token generation</span>
        
    <span class="hljs-keyword">case</span> MultiNode:
        <span class="hljs-comment">// Deploy using LeaderWorkerSet for distributed serving</span>
        r.reconcileMultiNodeComponents(isvc)
        
    <span class="hljs-keyword">default</span>:
        <span class="hljs-comment">// Standard single-component deployment</span>
        r.reconcileEngine(isvc)    <span class="hljs-comment">// Handles both prefill and decode</span>
        <span class="hljs-keyword">if</span> isvc.Spec.Router != <span class="hljs-literal">nil</span> {
            r.reconcileRouter(isvc) <span class="hljs-comment">// Optional routing layer</span>
        }
    }
}
</code></pre>
<p><strong>Deployment Mode Decision Logic</strong></p>
<p>The controller automatically determines the optimal deployment pattern:</p>
<ul>
<li><strong>RawDeployment</strong>: Single engine for models that fit on one node</li>
<li><strong>PDDisaggregated</strong>: Separate prefill/decode for high-throughput scenarios</li>
<li><strong>MultiNode</strong>: Distributed serving for massive models (e.g., DeepSeek V3 685B)</li>
<li><strong>Serverless</strong>: Scale-to-zero for cost optimization (via Knative integration)</li>
</ul>
<h3><a id="layer-3-data-plane---where-models-come-to-life" class="anchor" href="#layer-3-data-plane---where-models-come-to-life" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Layer 3: Data Plane - Where Models Come to Life</strong></h3>
<p>The data plane is where OME’s architectural decisions deliver real value. This layer handles the actual model serving with sophisticated optimizations.</p>
<h3><a id="model-agent-model-distribution" class="anchor" href="#model-agent-model-distribution" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Model Agent: Model Distribution</strong></h3>
<p>The Model Agent is OME’s data plane component responsible for making models available across your cluster. When you create a BaseModel resource, the Model Agent springs into action:</p>
<p><strong>What Makes Model Distribution Powerful:</strong></p>
<ol>
<li><strong>Automatic Model Parsing</strong>: Downloads and parses the model’s <code>config.json</code> and <code>safetensors</code> file headers, extracting architecture details, parameter counts, supported features, and optimal serving configurations. No more manual specification of model characteristics.</li>
<li><strong>Multi-Cloud Storage Abstraction</strong>: The <code>hf://</code> prefix in your BaseModel isn’t just syntactic sugar. OME supports multiple storage backends with a unified interface. Switch from HuggingFace to OCI Object Storage by changing one line—no code modifications needed.</li>
<li><strong>Node-Aware Distribution</strong>: Models aren’t blindly copied everywhere. The Model Agent runs as a DaemonSet, honoring node selectors and affinity rules, only downloading models to nodes that match your specifications. This saves precious NVMe space and reduces download times.</li>
<li><strong>Lifecycle Management</strong>: Models are tracked, versioned, and health-checked. If a node goes down, OME ensures model availability on other nodes. When you delete a BaseModel, cleanup happens automatically across all nodes.</li>
</ol>
<p><img src="/images/blog/ome/mode-agent.svg" alt="mode-agent.svg"></p>
<p><strong>The Scout-Gopher Architecture</strong></p>
<p>OME’s Model Agent employs a sophisticated producer-consumer pattern:</p>
<p><strong>1. Scout Component: The Distribution Layer</strong></p>
<p>The Scout acts as the brain of model distribution, continuously monitoring the Kubernetes API for BaseModel and ClusterBaseModel resources.</p>
<ul>
<li><strong>Node-Aware Filtering</strong>: Scout evaluates node selectors and affinity rules, ensuring models are only downloaded to appropriate nodes.</li>
<li><strong>Graceful Deletion Handling</strong>: When models are deleted, Scout ensures complete cleanup across all nodes before releasing resources, preventing orphaned multi-gigabyte files.</li>
</ul>
<p><strong>2. Gopher Component: The Task Engine</strong></p>
<p><strong>Storage Backend Performance</strong>:</p>
<ul>
<li><strong>OCI Object Storage</strong>: Achieves GB/s download speeds through parallel chunk downloads and 20-thread concurrency. A 140GB Llama 3 70B model downloads in minutes.</li>
<li><strong>HuggingFace Hub</strong>: Production-grade Golang client with automatic retry, rate limit handling, and resume support for interrupted downloads.</li>
<li><strong>Unified Interface</strong>: Switch between storage providers by changing one URI prefix—no code changes needed.</li>
</ul>
<p><strong>3. Model Configuration Parser</strong></p>
<p>The parser automatically extracts model metadata from config.json and safetensors files, determining exact parameter counts and capabilities. This eliminates manual configuration for 30+ supported model architectures.</p>
<p><strong>4. State Management &amp; Cleanup</strong></p>
<p>OME provides self-healing state management through:</p>
<ul>
<li><strong>ConfigMap Reconciliation</strong>: Automatically recreates deleted ConfigMaps through internal cache, ensuring model states are never lost</li>
<li><strong>Node Labels</strong>: Enable pod scheduling decisions with labels like <code>models.ome.io/basemodel.llama-3-70b=Ready</code></li>
<li><strong>Finalizer-Based Cleanup</strong>: Ensures complete model removal across all nodes before deletion, even handling node failures gracefully</li>
</ul>
<p><strong>The Result: Production-Grade Model Management</strong></p>
<p>This architecture delivers capabilities unmatched by traditional approaches:</p>
<ul>
<li><strong>Scale</strong>: Tested with large multi-gigabyte models, supporting multiple nodes downloading multiple models simultaneously</li>
<li><strong>Efficiency</strong>: Models download once per node, not per pod—saving petabytes of bandwidth</li>
<li><strong>Reliability</strong>: Self-healing ConfigMaps, automatic retries, and graceful error handling ensure models are always available</li>
<li><strong>Performance</strong>: GB/s download speeds with OCI Object Storage, 20x faster than naive implementations</li>
<li><strong>Intelligence</strong>: Automatic model understanding eliminates manual configuration errors</li>
</ul>
<h3><a id="inference-workloads" class="anchor" href="#inference-workloads" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Inference Workloads</strong></h3>
<p>Based on your InferenceService specification, OME deploys different components optimized for specific workload patterns, including PD-disaggregated serving, multi-node serving, standard deployment, and serverless deployment.</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>When Used</th>
<th>Primary Function</th>
<th>Key Optimizations</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Engine</strong></td>
<td>All deployments</td>
<td>Inference server (prefill in PD mode, full inference otherwise)</td>
<td>• Compute optimization• Batch processing• Tensor parallelism</td>
</tr>
<tr>
<td><strong>Decoder</strong></td>
<td>PD-disaggregated only</td>
<td>Token generation with KV cache from engine</td>
<td>• Memory bandwidth optimization• Efficient cache management</td>
</tr>
<tr>
<td><strong>Router</strong></td>
<td>When specified or PD mode</td>
<td>Optimized request distribution</td>
<td>• Cache-aware routing• Connection pooling• Health monitoring</td>
</tr>
<tr>
<td><strong>Ingress</strong></td>
<td>Automatically created</td>
<td>External API access</td>
<td>• TLS termination• Rate limiting• Request routing</td>
</tr>
</tbody>
</table>
<p>The beauty of this architecture is its flexibility—start with a simple engine-only deployment and progressively adopt advanced patterns as your needs grow.</p>
<h3><a id="layer-4-external-integrations---ecosystem-power" class="anchor" href="#layer-4-external-integrations---ecosystem-power" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Layer 4: External Integrations - Ecosystem Power</strong></h3>
<p>OME doesn’t reinvent the wheel—it deeply integrates with the Kubernetes ecosystem:</p>
<p><strong>Kubernetes Ecosystem Integration:</strong> Deep integration with modern Kubernetes components including <a href="https://kueue.sigs.k8s.io/">Kueue</a> for gang scheduling of multi-pod workloads, <a href="https://github.com/kubernetes-sigs/lws">LeaderWorkerSet</a> for resilient multi-node deployments, <a href="https://keda.sh/">KEDA</a> for advanced custom metrics-based autoscaling, <a href="https://gateway-api.sigs.k8s.io/">K8s Gateway API</a> for sophisticated traffic routing, and <a href="https://gateway-api-inference-extension.sigs.k8s.io/">Gateway API Inference Extension</a> for standardized inference endpoints.</p>
<h2><a id="sglang-first-class-runtime-support" class="anchor" href="#sglang-first-class-runtime-support" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>SGLang: First-Class Runtime Support</h2>
<p>SGLang is the primary runtime in OME, with deep native integration that showcases OME’s model-driven architecture capabilities.</p>
<h3><a id="native-router-integration" class="anchor" href="#native-router-integration" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Native Router Integration</h3>
<p>OME provides native integration with SGLang’s router component, implementing:</p>
<p><strong>Kubernetes Service Discovery</strong>: The router automatically discovers engine and decoder pods through Kubernetes APIs, adjusting to scaling events and pod lifecycle changes without manual intervention.</p>
<p><strong>Least-Privilege RBAC</strong>: Each router receives minimal permissions—only the ability to list, get, and watch pods in its namespace. This prevents cross-tenant information leakage while enabling dynamic discovery.</p>
<h3><a id="load-balancing-capabilities" class="anchor" href="#load-balancing-capabilities" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Load Balancing Capabilities</h3>
<p>OME supports SGLang’s advanced load balancing strategies:</p>
<p><strong>For PD-Disaggregated Deployments</strong>: The router distributes requests between prefill (engine) and decode (decoder) components, maintaining KV cache coherency and optimizing throughput.</p>
<p><strong>For Standard Deployments</strong>: Cache-aware load balancing tracks KV cache state across workers using RadixAttention, routing requests to workers most likely to have relevant cached prefixes.</p>
<h3><a id="deployment-flexibility" class="anchor" href="#deployment-flexibility" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Deployment Flexibility</h3>
<p>Through OME’s ServingRuntime configurations, SGLang supports:</p>
<ul>
<li><strong>Single-node serving</strong> for models that fit on one GPU</li>
<li><strong>Multi-node serving</strong> with tensor/pipeline parallelism for large models</li>
<li><strong>PD-disaggregated serving</strong> separating compute-intensive prefill from memory-bound decode</li>
<li><strong>Expert parallelism</strong> for MoE models like DeepSeek V3</li>
</ul>
<p>The integration between OME and SGLang demonstrates how a model-driven architecture enables sophisticated serving patterns while maintaining operational simplicity.</p>
<h2><a id="production-grade-features-built-for-scale" class="anchor" href="#production-grade-features-built-for-scale" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Production-Grade Features: Built for Scale</h2>
<h3><a id="native-benchmarking" class="anchor" href="#native-benchmarking" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Native Benchmarking</h3>
<p>OME includes BenchmarkJob as a core CRD, enabling systematic performance testing with realistic traffic patterns, concurrency sweeps, and automated result storage. This allows teams to compare different model configurations, runtime settings, and hardware choices with standardized, reproducible benchmarks.</p>
<h3><a id="multi-lora-serving-one-model-many-adapters" class="anchor" href="#multi-lora-serving-one-model-many-adapters" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Multi-LoRA Serving: One Model, Many Adapters</h3>
<p>Deploy a single base model with multiple LoRA adapters for different use cases:</p>
<p>Each request can specify which adapter to use, enabling multi-tenant serving with a single deployment.</p>
<h3><a id="high-performance-serving-at-scale" class="anchor" href="#high-performance-serving-at-scale" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>High-Performance Serving at Scale</h3>
<p><strong>Prefill-Decode Disaggregation</strong>: Separate compute-intensive prefill from memory-bound decode operations, achieving 2.5x throughput improvement for mixed workloads.</p>
<p><strong>Multi-Node Serving</strong>: Seamlessly scale models across multiple nodes with RDMA support, enabling deployment of frontier models like DeepSeek V3 (685B parameters).</p>
<p><strong>GB/s Download Speeds</strong>: Native OCI Object Storage integration delivers multi-gigabyte per second download speeds, getting models into production faster.</p>
<h3><a id="enterprise-security-defense-in-depth" class="anchor" href="#enterprise-security-defense-in-depth" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Enterprise Security: Defense in Depth</h3>
<p><strong>Optional Model Double Encryption</strong>: OME supports double encryption for models, integrating with OCI KMS and OCI Vault for enhanced security when required.</p>
<p><strong>Fine-Grained Access Control</strong>: RBAC policies control who can deploy which models, with namespace isolation and audit logging.</p>
<p><strong>Secure Multi-Tenancy</strong>: Complete isolation between different teams’ models and serving infrastructure.</p>
<h2><a id="real-world-impact-from-months-to-days" class="anchor" href="#real-world-impact-from-months-to-days" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Real-World Impact: From Months to Days</h2>
<p>OME is currently powering production workloads at Oracle Cloud Infrastructure (OCI), where it has transformed their LLM operations:</p>
<h3><a id="operational-transformation-at-scale" class="anchor" href="#operational-transformation-at-scale" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Operational Transformation at Scale</h3>
<p><strong>Before OME:</strong></p>
<ul>
<li>Model onboarding: Months of manual configuration</li>
<li>Runtime selection: Trial and error approach</li>
<li>Benchmarking: Ad-hoc, inconsistent processes</li>
<li>Serving configuration: Specialized knowledge required</li>
</ul>
<p><strong>With OME:</strong></p>
<ul>
<li>Model onboarding: Days with automated workflows</li>
<li>Runtime selection: Automatic and optimal</li>
<li>Benchmarking: Standardized and reproducible</li>
<li>Serving configuration: Declarative and simple</li>
</ul>
<p>The impact has been dramatic—what previously required months of specialized engineering effort now happens in days with standardized, automated processes. Teams can focus on evaluating model capabilities rather than wrestling with infrastructure complexity.</p>
<h2><a id="closing-the-gap-how-ome-bridges-two-worlds" class="anchor" href="#closing-the-gap-how-ome-bridges-two-worlds" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Closing the Gap: How OME Bridges Two Worlds</h2>
<p>Remember the two teams we started with? The ML Engineers perfecting model serving, and the Production Engineers trying to deploy models quickly?</p>
<p>OME closes this gap through its model-driven architecture:</p>
<p><strong>For ML Engineers</strong>: Your optimizations are captured in ServingRuntimes. Your benchmarking results guide future deployments. Your expertise becomes reusable infrastructure.</p>
<p><strong>For Production Teams</strong>: Deploy any model with a simple InferenceService. The platform handles runtime selection, scaling, and optimization automatically.</p>
<p><strong>For Organizations</strong>: Standardized workflows, reduced time to production, and the ability to leverage cutting-edge serving techniques without deep expertise.</p>
<p>The model-driven approach transforms the chaos of LLM deployment into a systematic, scalable process. By making models first-class citizens, OME enables both innovation and operational excellence.</p>
<h2><a id="the-path-forward-challenges-ahead" class="anchor" href="#the-path-forward-challenges-ahead" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Path Forward: Challenges Ahead</h2>
<p>While OME has transformed model serving at OCI, significant challenges remain as we build toward a truly universal LLM platform:</p>
<h3><a id="accelerator-aware-runtime-selection" class="anchor" href="#accelerator-aware-runtime-selection" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Accelerator-Aware Runtime Selection</h3>
<p>Currently, mixed GPU clusters (H100, A100, L40S) require separate runtime configurations for each accelerator type, leading to runtime proliferation and operational complexity. We’re working on AcceleratorClass abstractions to enable single runtimes that adapt to available hardware automatically.</p>
<h3><a id="multi-cloud-support" class="anchor" href="#multi-cloud-support" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Multi-Cloud Support</h3>
<p>OME’s current implementation is tightly coupled to OCI. As organizations increasingly adopt multi-cloud strategies, we need to decouple provider-specific logic and create a unified interface that supports AWS, Azure, GCP, and on-premises deployments without code duplication.</p>
<h3><a id="multi-cluster-management" class="anchor" href="#multi-cluster-management" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Multi-Cluster Management</h3>
<p>Future architectures will support a single management cluster running OME that can orchestrate model deployments across multiple worker clusters, enabling federation across regions and cloud providers.</p>
<p>The journey to simplify LLM deployment continues. Each challenge presents an opportunity to make model serving more accessible, efficient, and powerful for organizations worldwide.</p>
<h2><a id="join-the-revolution" class="anchor" href="#join-the-revolution" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Join the Revolution</h2>
<p>OME represents a paradigm shift from deployment-driven to model-driven infrastructure. By abstracting complexity while exposing powerful capabilities, it enables organizations to focus on delivering AI value rather than wrestling with infrastructure.</p>
<p><strong>Get Involved:</strong></p>
<ul>
<li><strong>GitHub</strong>: <a href="https://github.com/sgl-project/ome">github.com/sgl-project/ome</a></li>
<li><strong>Documentation</strong>: <a href="https://docs.sglang.ai/ome/">docs.sglang.ai/ome</a></li>
<li><strong>Community</strong>: Join our <a href="https://slack.sglang.ai/">Slack</a> for support and discussions</li>
<li><strong>Contribute</strong>: We welcome contributions from the community</li>
</ul>
<h2><a id="acknowledgments" class="anchor" href="#acknowledgments" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgments</h2>
<p>We would like to express our heartfelt gratitude to the following teams and collaborators:</p>
<p><strong>SGLang Community</strong> — Yineng Zhang, Ying Sheng, Lianmin Zheng — for their groundbreaking work on SGLang and continuous collaboration on making it the flagship runtime for OME.</p>
<p><strong>Oracle Cloud Infrastructure Team</strong> — Simo Lin, Chang Su, Beiwen Guo, Yifeng Liu, David Nahm, Feng Gao, Frank Zhou, Weiwei Zheng, Arthur Cheng, Chao Yang, Varun Shenoy, Jinguo Zhang, Wei Gao, Jun Qian, Jingqiao Zhang and colleagues — for driving the vision of model-driven infrastructure and validating OME in production at scale.</p>
<p>Thank you all for your invaluable support and collaboration.</p>
<hr>
<p><em>OME is transforming how organizations deploy and manage LLMs at scale. Join leading enterprises already using OME to power their AI infrastructure with model-driven simplicity and cutting-edge performance.</em></p>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"OME: Revolutionizing LLM Infrastructure with Model-Driven Architecture","author":"The Oracle Team","date":"July 8, 2025","previewImg":"/images/blog/ome/ome.jpg"},"content":"\n## The Tale of Two Teams: Why Model Serving Is Broken\n\nIn any large organization deploying LLMs, two distinct teams emerge with conflicting needs:\n\n**The ML Engineers** spend months benchmarking models, experimenting with serving technologies, and crafting optimal deployment strategies. Each model demands different configurations—tensor parallelism for Llama 70B, expert parallelism for DeepSeek V3/R1, specialized settings for multimodal models. The parameters are endless: batch sizes, KV cache configurations, quantization levels. Worse, these configurations shift dramatically across GPU types (H100 vs A100 vs L40S).\n\n**The Production Engineers and Data Scientists** just want to deploy models. They shouldn’t need to understand the intricacies of tensor parallelism or why a particular model needs 4 GPUs with NVLink. They have customers waiting, applications to build, and business value to deliver.\n\nThis gap creates a fundamental problem: MLEs need a way to encode their hard-won serving knowledge into reusable blueprints. Production teams need to deploy models without becoming distributed systems experts. **The missing link? A system that understands models as first-class citizens.**\n\n## The Birth of OME\n\nThe Oracle Cloud Infrastructure (OCI) GenAI team faced this exact challenge at scale. Supporting numerous models across diverse GPU hardware, they watched deployment cycles stretch into months. Each new model meant:\n- Weeks of MLE experimentation to find optimal configurations\n- Complex documentation that production teams struggled to follow\n- Deployment failures due to misconfiguration\n- Inability to reuse knowledge across similar models\n\nThe breakthrough came from a simple insight: **The model itself should drive the deployment.**\n\nA Llama model isn’t just a file—it contains metadata about its architecture, parameter count, and requirements. By making the system model-aware rather than deployment-driven, they could bridge the gap between ML expertise and production simplicity.\n\nThis led to OME (Open Model Engine): a Kubernetes operator that treats models as first-class resources. The results were dramatic:\n- Model onboarding time: **Months → Days**\n- Configuration errors: **Dramatically reduced**\n- MLE knowledge: **Captured and reused automatically**\n- Production deployment: **Simple YAML with just a few lines**\n\nBut here’s what makes it revolutionary: the model-driven architecture makes it easy to encode and reuse sophisticated deployment strategies:\n- **Multi-node serving**: Deploy massive models like DeepSeek V3 (685B) across multiple nodes with a simple configuration\n- **Prefill-decode disaggregation**: Separate compute-intensive prefill from memory-bound decode, with each component scaling independently\n- **Flexible architectures**: Both prefill and decode can run in single-node or multi-node configurations based on your needs\n- **Serverless deployment**: Scale-to-zero for cost efficiency when models aren’t in use\n- **Business-driven scaling**: Complex autoscaling based on KV cache, tokens/second, latency targets, or any custom metric\n\nThe model-driven approach doesn’t constrain you—it liberates you. Because OME understands models deeply, it can support any deployment pattern your MLEs design while keeping the interface simple for production teams.\n\n**Enter OME**: A Kubernetes-native platform where models become first-class citizens. Let’s explore how OME’s architecture transforms the chaos of LLM deployment into an elegant, scalable system that serves everyone from ML researchers to production engineers.\n\n## The OME Architecture: Models at the Center\n\n![ome-architecture.svg](/images/blog/ome/ome-architecture.svg)\n\n### **Layer 1: Kubernetes API Layer**\n\nWhile users—MLEs, data scientists, production engineers, and applications—interact with OME through simple interfaces, the real magic happens in the Kubernetes API layer below.\n\n### **Custom Resources - The Foundation of Model-Driven Architecture**\n\nAt the heart of OME lies its Custom Resource Definitions (CRDs), which transform Kubernetes from a generic container orchestrator into an ML platform. These aren’t just configuration files—they’re the language through which you express your ML requirements.\n\n### **BaseModel/ClusterBaseModel: Models as First-Class Citizens**\n\n**What is a Base Model?**\n\nA Base Model in OME is a Kubernetes resource that represents a foundation AI model (like GPT, Llama, or Mistral) that you want to use for inference workloads. Think of it as a blueprint that tells OME where to find your model, how to download it, and where to store it on your cluster nodes.\n\nWhen you create a BaseModel resource, OME automatically handles the complex process of downloading the model files, parsing the model’s configuration to understand its capabilities, and making it available across your cluster nodes where AI workloads can use it.\n\n**BaseModel vs ClusterBaseModel**\n\nOME provides two types of model resources:\n\n- **BaseModel** is namespace-scoped, meaning it exists within a specific Kubernetes namespace. If you create a BaseModel in the “team-a” namespace, only workloads in that namespace can use it. This is perfect for team-specific models or when you want to isolate model access.\n- **ClusterBaseModel** is cluster-scoped, meaning it’s available to workloads in any namespace across your entire cluster. This is ideal for organization-wide models that multiple teams need to access, like a shared Llama-3 model that everyone uses.\n\nBoth types use exactly the same specification format—the only difference is their visibility scope.\n\nTraditional platforms treat models as static files to be downloaded and mounted. OME revolutionizes this by making models intelligent, versioned resources that understand their own requirements:\n\n```yaml\napiVersion: ome.io/v1beta1\nkind: ClusterBaseModel\nmetadata:\n  name: llama-3-70b-instruct\nspec:\n  vendor: meta\n  modelType: llama\n  modelArchitecture: LlamaForCausalLM\n  modelParameterSize: \"70B\"\n  quantization: fp16\n  storage:\n    storageUri: \"hf://meta-llama/Llama-3.3-70B-Instruct\"\n    path: \"/models/llama-3.3-70b\"\n    nodeSelector:\n      gpu.memory: \"80Gi\"  # Only download to nodes with sufficient GPU memory\n```\n\nWhen you create a BaseModel resource, OME’s control plane and data plane components work together to make the model available across your cluster. The BaseModel CRD acts as the declarative specification, while the actual work of downloading, parsing, and distributing models happens in the data plane through the Model Agent.\n\n### **ServingRuntime: The Brain of Runtime Selection**\n\nClusterServingRuntime is a cluster-scoped resource that manages the runtime environment for model serving. A ClusterServingRuntime defines the templates for Pods that can serve one or more particular models. Each ClusterServingRuntime defines key information such as the container image of the runtime and a list of the models that the runtime supports. Other configuration settings for the runtime can be conveyed through environment variables in the container specification.\n\nThese CRDs allow for improved flexibility and extensibility, enabling users to quickly define or customize reusable runtimes without having to modify any controller code or any resources in the controller namespace. The only difference between ServingRuntime and ClusterServingRuntime is that one is namespace-scoped and the other is cluster-scoped.\n\n```yaml\napiVersion: ome.io/v1beta1\nkind: ClusterServingRuntime\nmetadata:\n  name: sglang-llama-70b\nspec:\n  supportedModelFormats:\n    - modelFormat:\n        name: safetensors\n      modelArchitecture: LlamaForCausalLM\n      modelSizeRange:\n        min: \"65B\"\n        max: \"75B\"\n      autoSelect: true\n      priority: 100\n```\n\nFull runtime specifications for advanced deployments can be found in the OME repository:\n- [Llama 4 Maverick PD Runtime](https://github.com/sgl-project/ome/blob/main/config/runtimes/srt/llama-4-maverick-17b-128e-instruct-fp8-pd-rt.yaml) - Prefill-decode disaggregated configuration\n- [DeepSeek RDMA PD Runtime](https://github.com/sgl-project/ome/blob/main/config/runtimes/srt/deepseek-rdma-pd-rt.yaml) - Multi-node expert parallel serving with RDMA\n\nServingRuntimes define how to serve different model types, with the actual runtime selection logic handled by the control plane when you create an InferenceService.\n\n**Advanced Deployment Architectures**\n\nServingRuntimes serve as blueprints for how router, engine, and decoder components are deployed. Each component (except router) can be configured for single-node, serverless, or multi-node deployment. This flexibility enables cutting-edge serving patterns:\n**PD-Disaggregated Serving** - The state-of-the-art for high-performance LLM serving at scale\n\nThis isn’t just incremental improvement—it’s a fundamental advancement in serving architecture that OME makes accessible through simple runtime configuration.\n\n### **InferenceService: Orchestrating Model Deployments and Ingress**\n\nAn InferenceService is the central Kubernetes resource in OME that orchestrates the complete lifecycle of model serving. It acts as a declarative specification that describes how you want your AI models deployed, scaled, and served across your cluster.\n\nThink of InferenceService as the “deployment blueprint” for your AI workloads. It brings together models (defined by BaseModel/ClusterBaseModel), runtimes (defined by ServingRuntime/ClusterServingRuntime), and infrastructure configuration to create a complete serving solution. InferenceService is what puts models, runtimes, as well as traditional Kubernetes services, complex ingress, scheduling, auto scaling, and permission controls all together to form a complete cluster serving fleet.\n\n**Architecture Overview**\n\nOME uses a component-based architecture where InferenceService can be composed of multiple specialized components:\n\n- **Model**: References the AI model to serve (BaseModel/ClusterBaseModel)\n- **Runtime**: References the serving runtime environment (ServingRuntime/ClusterServingRuntime)\n- **Engine**: Main inference component that processes requests, typically an OpenAI-compatible server handling request processing, tool parsing, and model backend operations\n- **Decoder**: Optional component for disaggregated serving (prefill-decode separation)\n- **Router**: A standalone high-performance component that enables data parallelism across inference instances, supporting advanced load balancing algorithms (cache-aware, power of two, random, round robin) and acting as a specialized load balancer for prefill-decode disaggregated serving architectures\n\n```yaml\napiVersion: ome.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: production-chat-service\nspec:\n  model:\n    name: llama-3-70b-instruct\n  engine:\n    minReplicas: 2\n    maxReplicas: 10\n  decoder:  # Only created for disaggregated deployments\n    minReplicas: 4\n    maxReplicas: 20\n  router:  # Optional optimal serving routing layer\n    minReplicas: 2\n```\n\nThis component architecture enables sophisticated optimizations impossible with monolithic deployments:\n- **Independent Scaling**: Scale compute-heavy prefill separately from memory-bound decode\n- **Resource Optimization**: Routers don’t need GPUs, saving precious accelerator resources\n- **Failure Isolation**: Component failures don’t bring down the entire service\n- **Performance Tuning**: Each component can be optimized for its specific workload\n\n### **BenchmarkJob: Performance Testing as a First-Class Operation**\n\nOME is the only platform that treats performance testing as a core primitive:\n\n```yaml\napiVersion: ome.io/v1beta1\nkind: BenchmarkJob\nmetadata:\n  name: llama-70b-production-benchmark\nspec:\n  # Target service to benchmark\n  endpoint:\n    inferenceService:\n      name: llama-chat-optimized\n  outputLocation:\n    storageUri: \"oci://n/benchmark-results/b/prod/o/llama-70b-bench\"\n```\n\nThis isn’t just about running load tests. BenchmarkJob provides:\n- **genai-bench integration**: Industry-standard benchmarking tool\n- **Realistic traffic patterns**: Normal distributions, fixed patterns, long-context scenarios\n- **Comprehensive metrics**: Tokens/second, TTFT, latency percentiles\n- **Multi-cloud storage**: Results stored for historical analysis\n- **Service metadata tracking**: GPU types, engine versions for fair comparisons\n\n### **Admission Webhooks: Validation and Mutation**\n\nOME’s admission webhooks act as gatekeepers in the API layer:\n\n1. **Validating Webhooks** ensure model-runtime compatibility before resources are created, preventing runtime failures\n2. **Mutating Webhooks** inject optimal configurations based on model characteristics\n3. **Pod Mutating Webhooks** handle complex scenarios like:\n    - RDMA configuration for multi-node deployments\n    - GPU affinity rules for optimal memory bandwidth\n    - Security contexts for model encryption\n\n### **Layer 2: Control Plane - The Orchestrator**\n\nThe control plane is where OME’s main operation lives. This isn’t just CRUD operations on Kubernetes resources—it’s a sophisticated system that makes optimal decisions based on model characteristics, hardware availability, and business requirements.\n\n### **OME Controller Manager: The Orchestration Brain**\n\nThe controller manager coordinates all OME operations with a reconciliation loop that’s aware of ML-specific concerns.\n\n**Runtime Selection Algorithm**\n\nWhen you deploy a model through an InferenceService, the controller:\n- Matches model characteristics against all available ServingRuntimes\n- Scores each runtime based on compatibility and optimization potential\n- Uses **Model Size Range** matching—when multiple runtimes support a model, OME selects the one with the closest size range for optimal performance\n- Handles edge cases like quantized models or models requiring specific GPU features\n\n**Component Orchestration**\n\nThe InferenceService controller orchestrates multiple components based on your deployment requirements:\n\n```go\n// Simplified reconciliation logic showing component-based orchestration\nfunc (r *Reconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {\n    // 1. Fetch InferenceService and determine deployment mode\n    inferenceService := \u0026omev1beta1.InferenceService{}\n    deploymentMode := r.inferDeploymentMode(isvc)\n    \n    // 2. Select optimal runtime if not specified\n    if isvc.Spec.Runtime == nil {\n        runtime := r.selectOptimalRuntime(isvc.Spec.Model, deploymentMode)\n        isvc.Spec.Runtime = runtime\n    }\n    \n    // 3. Reconcile components based on deployment mode\n    switch deploymentMode {\n    case PDDisaggregated:\n        // Deploy separate engine (prefill) and decoder components\n        r.reconcileRouter(isvc)    // Cache-aware routing\n        r.reconcileEngine(isvc)    // Prefill processing\n        r.reconcileDecoder(isvc)   // Token generation\n        \n    case MultiNode:\n        // Deploy using LeaderWorkerSet for distributed serving\n        r.reconcileMultiNodeComponents(isvc)\n        \n    default:\n        // Standard single-component deployment\n        r.reconcileEngine(isvc)    // Handles both prefill and decode\n        if isvc.Spec.Router != nil {\n            r.reconcileRouter(isvc) // Optional routing layer\n        }\n    }\n}\n```\n\n**Deployment Mode Decision Logic**\n\nThe controller automatically determines the optimal deployment pattern:\n\n- **RawDeployment**: Single engine for models that fit on one node\n- **PDDisaggregated**: Separate prefill/decode for high-throughput scenarios\n- **MultiNode**: Distributed serving for massive models (e.g., DeepSeek V3 685B)\n- **Serverless**: Scale-to-zero for cost optimization (via Knative integration)\n\n### **Layer 3: Data Plane - Where Models Come to Life**\n\nThe data plane is where OME’s architectural decisions deliver real value. This layer handles the actual model serving with sophisticated optimizations.\n\n### **Model Agent: Model Distribution**\n\nThe Model Agent is OME’s data plane component responsible for making models available across your cluster. When you create a BaseModel resource, the Model Agent springs into action:\n\n**What Makes Model Distribution Powerful:**\n\n1. **Automatic Model Parsing**: Downloads and parses the model’s `config.json` and `safetensors` file headers, extracting architecture details, parameter counts, supported features, and optimal serving configurations. No more manual specification of model characteristics.\n2. **Multi-Cloud Storage Abstraction**: The `hf://` prefix in your BaseModel isn’t just syntactic sugar. OME supports multiple storage backends with a unified interface. Switch from HuggingFace to OCI Object Storage by changing one line—no code modifications needed.\n3. **Node-Aware Distribution**: Models aren’t blindly copied everywhere. The Model Agent runs as a DaemonSet, honoring node selectors and affinity rules, only downloading models to nodes that match your specifications. This saves precious NVMe space and reduces download times.\n4. **Lifecycle Management**: Models are tracked, versioned, and health-checked. If a node goes down, OME ensures model availability on other nodes. When you delete a BaseModel, cleanup happens automatically across all nodes.\n\n![mode-agent.svg](/images/blog/ome/mode-agent.svg)\n\n**The Scout-Gopher Architecture**\n\nOME’s Model Agent employs a sophisticated producer-consumer pattern:\n\n**1. Scout Component: The Distribution Layer**\n\nThe Scout acts as the brain of model distribution, continuously monitoring the Kubernetes API for BaseModel and ClusterBaseModel resources.\n\n- **Node-Aware Filtering**: Scout evaluates node selectors and affinity rules, ensuring models are only downloaded to appropriate nodes.\n- **Graceful Deletion Handling**: When models are deleted, Scout ensures complete cleanup across all nodes before releasing resources, preventing orphaned multi-gigabyte files.\n\n**2. Gopher Component: The Task Engine**\n\n**Storage Backend Performance**:\n\n- **OCI Object Storage**: Achieves GB/s download speeds through parallel chunk downloads and 20-thread concurrency. A 140GB Llama 3 70B model downloads in minutes.\n- **HuggingFace Hub**: Production-grade Golang client with automatic retry, rate limit handling, and resume support for interrupted downloads.\n- **Unified Interface**: Switch between storage providers by changing one URI prefix—no code changes needed.\n\n**3. Model Configuration Parser**\n\nThe parser automatically extracts model metadata from config.json and safetensors files, determining exact parameter counts and capabilities. This eliminates manual configuration for 30+ supported model architectures.\n\n**4. State Management \u0026 Cleanup**\n\nOME provides self-healing state management through:\n\n- **ConfigMap Reconciliation**: Automatically recreates deleted ConfigMaps through internal cache, ensuring model states are never lost\n- **Node Labels**: Enable pod scheduling decisions with labels like `models.ome.io/basemodel.llama-3-70b=Ready`\n- **Finalizer-Based Cleanup**: Ensures complete model removal across all nodes before deletion, even handling node failures gracefully\n\n**The Result: Production-Grade Model Management**\n\nThis architecture delivers capabilities unmatched by traditional approaches:\n\n- **Scale**: Tested with large multi-gigabyte models, supporting multiple nodes downloading multiple models simultaneously\n- **Efficiency**: Models download once per node, not per pod—saving petabytes of bandwidth\n- **Reliability**: Self-healing ConfigMaps, automatic retries, and graceful error handling ensure models are always available\n- **Performance**: GB/s download speeds with OCI Object Storage, 20x faster than naive implementations\n- **Intelligence**: Automatic model understanding eliminates manual configuration errors\n\n### **Inference Workloads**\n\nBased on your InferenceService specification, OME deploys different components optimized for specific workload patterns, including PD-disaggregated serving, multi-node serving, standard deployment, and serverless deployment.\n\n| Component | When Used | Primary Function | Key Optimizations |\n| --- | --- | --- | --- |\n| **Engine** | All deployments | Inference server (prefill in PD mode, full inference otherwise) | • Compute optimization• Batch processing• Tensor parallelism |\n| **Decoder** | PD-disaggregated only | Token generation with KV cache from engine | • Memory bandwidth optimization• Efficient cache management |\n| **Router** | When specified or PD mode | Optimized request distribution | • Cache-aware routing• Connection pooling• Health monitoring |\n| **Ingress** | Automatically created | External API access | • TLS termination• Rate limiting• Request routing |\n\nThe beauty of this architecture is its flexibility—start with a simple engine-only deployment and progressively adopt advanced patterns as your needs grow.\n\n### **Layer 4: External Integrations - Ecosystem Power**\n\nOME doesn’t reinvent the wheel—it deeply integrates with the Kubernetes ecosystem:\n\n**Kubernetes Ecosystem Integration:** Deep integration with modern Kubernetes components including [Kueue](https://kueue.sigs.k8s.io/) for gang scheduling of multi-pod workloads, [LeaderWorkerSet](https://github.com/kubernetes-sigs/lws) for resilient multi-node deployments, [KEDA](https://keda.sh/) for advanced custom metrics-based autoscaling, [K8s Gateway API](https://gateway-api.sigs.k8s.io/) for sophisticated traffic routing, and [Gateway API Inference Extension](https://gateway-api-inference-extension.sigs.k8s.io/) for standardized inference endpoints.\n\n## SGLang: First-Class Runtime Support\n\nSGLang is the primary runtime in OME, with deep native integration that showcases OME’s model-driven architecture capabilities.\n\n### Native Router Integration\n\nOME provides native integration with SGLang’s router component, implementing:\n\n**Kubernetes Service Discovery**: The router automatically discovers engine and decoder pods through Kubernetes APIs, adjusting to scaling events and pod lifecycle changes without manual intervention.\n\n**Least-Privilege RBAC**: Each router receives minimal permissions—only the ability to list, get, and watch pods in its namespace. This prevents cross-tenant information leakage while enabling dynamic discovery.\n\n### Load Balancing Capabilities\n\nOME supports SGLang’s advanced load balancing strategies:\n\n**For PD-Disaggregated Deployments**: The router distributes requests between prefill (engine) and decode (decoder) components, maintaining KV cache coherency and optimizing throughput.\n\n**For Standard Deployments**: Cache-aware load balancing tracks KV cache state across workers using RadixAttention, routing requests to workers most likely to have relevant cached prefixes.\n\n### Deployment Flexibility\n\nThrough OME’s ServingRuntime configurations, SGLang supports:\n\n- **Single-node serving** for models that fit on one GPU\n- **Multi-node serving** with tensor/pipeline parallelism for large models\n- **PD-disaggregated serving** separating compute-intensive prefill from memory-bound decode\n- **Expert parallelism** for MoE models like DeepSeek V3\n\nThe integration between OME and SGLang demonstrates how a model-driven architecture enables sophisticated serving patterns while maintaining operational simplicity.\n\n## Production-Grade Features: Built for Scale\n\n### Native Benchmarking\n\nOME includes BenchmarkJob as a core CRD, enabling systematic performance testing with realistic traffic patterns, concurrency sweeps, and automated result storage. This allows teams to compare different model configurations, runtime settings, and hardware choices with standardized, reproducible benchmarks.\n\n### Multi-LoRA Serving: One Model, Many Adapters\n\nDeploy a single base model with multiple LoRA adapters for different use cases:\n\nEach request can specify which adapter to use, enabling multi-tenant serving with a single deployment.\n\n### High-Performance Serving at Scale\n\n**Prefill-Decode Disaggregation**: Separate compute-intensive prefill from memory-bound decode operations, achieving 2.5x throughput improvement for mixed workloads.\n\n**Multi-Node Serving**: Seamlessly scale models across multiple nodes with RDMA support, enabling deployment of frontier models like DeepSeek V3 (685B parameters).\n\n**GB/s Download Speeds**: Native OCI Object Storage integration delivers multi-gigabyte per second download speeds, getting models into production faster.\n\n### Enterprise Security: Defense in Depth\n\n**Optional Model Double Encryption**: OME supports double encryption for models, integrating with OCI KMS and OCI Vault for enhanced security when required.\n\n**Fine-Grained Access Control**: RBAC policies control who can deploy which models, with namespace isolation and audit logging.\n\n**Secure Multi-Tenancy**: Complete isolation between different teams’ models and serving infrastructure.\n\n## Real-World Impact: From Months to Days\n\nOME is currently powering production workloads at Oracle Cloud Infrastructure (OCI), where it has transformed their LLM operations:\n\n### Operational Transformation at Scale\n\n**Before OME:**\n- Model onboarding: Months of manual configuration\n- Runtime selection: Trial and error approach\n- Benchmarking: Ad-hoc, inconsistent processes\n- Serving configuration: Specialized knowledge required\n\n**With OME:**\n- Model onboarding: Days with automated workflows\n- Runtime selection: Automatic and optimal\n- Benchmarking: Standardized and reproducible\n- Serving configuration: Declarative and simple\n\nThe impact has been dramatic—what previously required months of specialized engineering effort now happens in days with standardized, automated processes. Teams can focus on evaluating model capabilities rather than wrestling with infrastructure complexity.\n\n## Closing the Gap: How OME Bridges Two Worlds\n\nRemember the two teams we started with? The ML Engineers perfecting model serving, and the Production Engineers trying to deploy models quickly?\n\nOME closes this gap through its model-driven architecture:\n\n**For ML Engineers**: Your optimizations are captured in ServingRuntimes. Your benchmarking results guide future deployments. Your expertise becomes reusable infrastructure.\n\n**For Production Teams**: Deploy any model with a simple InferenceService. The platform handles runtime selection, scaling, and optimization automatically.\n\n**For Organizations**: Standardized workflows, reduced time to production, and the ability to leverage cutting-edge serving techniques without deep expertise.\n\nThe model-driven approach transforms the chaos of LLM deployment into a systematic, scalable process. By making models first-class citizens, OME enables both innovation and operational excellence.\n\n## The Path Forward: Challenges Ahead\n\nWhile OME has transformed model serving at OCI, significant challenges remain as we build toward a truly universal LLM platform:\n\n### Accelerator-Aware Runtime Selection\n\nCurrently, mixed GPU clusters (H100, A100, L40S) require separate runtime configurations for each accelerator type, leading to runtime proliferation and operational complexity. We’re working on AcceleratorClass abstractions to enable single runtimes that adapt to available hardware automatically.\n\n### Multi-Cloud Support\n\nOME’s current implementation is tightly coupled to OCI. As organizations increasingly adopt multi-cloud strategies, we need to decouple provider-specific logic and create a unified interface that supports AWS, Azure, GCP, and on-premises deployments without code duplication.\n\n### Multi-Cluster Management\n\nFuture architectures will support a single management cluster running OME that can orchestrate model deployments across multiple worker clusters, enabling federation across regions and cloud providers.\n\nThe journey to simplify LLM deployment continues. Each challenge presents an opportunity to make model serving more accessible, efficient, and powerful for organizations worldwide.\n\n## Join the Revolution\n\nOME represents a paradigm shift from deployment-driven to model-driven infrastructure. By abstracting complexity while exposing powerful capabilities, it enables organizations to focus on delivering AI value rather than wrestling with infrastructure.\n\n**Get Involved:**\n- **GitHub**: [github.com/sgl-project/ome](https://github.com/sgl-project/ome)\n- **Documentation**: [docs.sglang.ai/ome](https://docs.sglang.ai/ome/)\n- **Community**: Join our [Slack](https://slack.sglang.ai/) for support and discussions\n- **Contribute**: We welcome contributions from the community\n\n## Acknowledgments\n\nWe would like to express our heartfelt gratitude to the following teams and collaborators:\n\n**SGLang Community** — Yineng Zhang, Ying Sheng, Lianmin Zheng — for their groundbreaking work on SGLang and continuous collaboration on making it the flagship runtime for OME.\n\n**Oracle Cloud Infrastructure Team** — Simo Lin, Chang Su, Beiwen Guo, Yifeng Liu, David Nahm, Feng Gao, Frank Zhou, Weiwei Zheng, Arthur Cheng, Chao Yang, Varun Shenoy, Jinguo Zhang, Wei Gao, Jun Qian, Jingqiao Zhang and colleagues — for driving the vision of model-driven infrastructure and validating OME in production at scale.\n\nThank you all for your invaluable support and collaboration.\n\n---\n\n*OME is transforming how organizations deploy and manage LLMs at scale. Join leading enterprises already using OME to power their AI infrastructure with model-driven simplicity and cutting-edge performance.*\n","slug":"2025-07-08-ome"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2025-07-08-ome"},"buildId":"z7lqLJFZXcwp_rAQo_vTP","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>