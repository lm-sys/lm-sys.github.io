<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>Towards Deterministic Inference in SGLang and Reproducible RL Training | LMSYS Org</title><meta name="title" content="Towards Deterministic Inference in SGLang and Reproducible RL Training | LMSYS Org"/><meta property="og:title" content="Towards Deterministic Inference in SGLang and Reproducible RL Training | LMSYS Org"/><meta name="twitter:title" content="Towards Deterministic Inference in SGLang and Reproducible RL Training | LMSYS Org"/><meta name="description" content="&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: This post shares our efforts to enable deterministic inference in SGLang and our collaboration with &lt;a href=&quot;https://github.com/TH..."/><meta property="og:description" content="&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: This post shares our efforts to enable deterministic inference in SGLang and our collaboration with &lt;a href=&quot;https://github.com/TH..."/><meta name="twitter:description" content="&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: This post shares our efforts to enable deterministic inference in SGLang and our collaboration with &lt;a href=&quot;https://github.com/TH..."/><meta property="og:image" content="https://lmsys.org/images/blog/deterministic/logo.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/deterministic/logo.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2025-09-22-sglang-deterministic"/><meta name="twitter:url" content="https://lmsys.org/blog/2025-09-22-sglang-deterministic"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-0bb93d4b49319e30.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/gGneV6uQXQMqd7OD7RXYJ/_buildManifest.js" defer=""></script><script src="/_next/static/gGneV6uQXQMqd7OD7RXYJ/_ssgManifest.js" defer=""></script><script src="/_next/static/gGneV6uQXQMqd7OD7RXYJ/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">Towards Deterministic Inference in SGLang and Reproducible RL Training</h1><p class="text-xl pt-2 pb-2">by: <!-- -->The SGLang Team<!-- -->,<!-- --> <!-- -->Sep 22, 2025<!-- --></p><hr/><div class="pt-2 article"><p><strong>TL;DR</strong>: This post shares our efforts to enable deterministic inference in SGLang and our collaboration with <a href="https://github.com/THUDM/slime">slime</a> to work towards reproducible RL training.</p>
<br />
<p>Recently, the Thinking Machines Lab published a <a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/">blog</a> detailing their findings. Since this blog was published, the industry has responded enthusiastically, eagerly awaiting open-source inference engines to implement stable and usable deterministic inference, or even further, to achieve fully reproducible RL training. Now, SGLang and slime together provide the answer.</p>
<p>Building on Thinking Machines Lab's batch-invariant operators, SGLang achieves fully deterministic inference while maintaining compatibility with <strong>chunked prefill</strong>, <strong>CUDA graphs</strong>, <strong>radix cache</strong>, and <strong>non-greedy sampling</strong>. With CUDA graphs, SGLang delivers <strong>2.8x acceleration</strong> and reduces performance overhead to just <strong>34.35%</strong> (vs. TML's <strong>61.5%</strong>).</p>
<p>Taking this deterministic inference capability further, SGLang collaborated with the slime team to unlock <strong>100% reproducible RL training</strong> - a breakthrough achieved with minimal code changes. Our validation experiments on Qwen3-8B demonstrate perfect reproducibility: <strong>two independent training runs produce identical curves</strong>, providing the reliability needed for rigorous scientific experimentation.</p>
<p><img src="/images/blog/deterministic/slime.png" alt="slime"><small><center><a href="https://thudm.github.io/slime/_examples_synced/reproducibility/README.html#reproducibility"><em>Reproducible Guide</em></a></center></small></p>
<br />
<p>Now let's dive into the some technical details.</p>
<h2><a id="why-deterministic-inference-matters" class="anchor" href="#why-deterministic-inference-matters" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why Deterministic Inference Matters</h2>
<p>The ability to achieve consistent outputs from large language models (LLMs) inference is increasingly important. For example, the indeterminism of inference results can implicitly transform on-policy reinforcement learning (RL) into off-policy RL as <a href="https://fengyao.notion.site/off-policy-rl">researchers pointed out</a>. However, even if we turn the temperature down to 0 in SGLang, the sampling is still not deterministic due to the use of dynamic batching and radix cache (past discussions <a href="https://docs.sglang.ai/references/faq.html#the-results-are-not-deterministic-even-with-a-temperature-of-0">here</a>) .</p>
<p>As mentioned in the <a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/">blog</a>, the largest source of nondeterminism is the varying batch sizes: Even when a user repeatedly submits the same prompt, the output can vary across runs, since the request may be batched together with other users' requests, and differences in batch size can lead to nondeterministic inference results.</p>
<p>To explain more, different batch sizes will influence the reduction splitting process of kernels. This leads to  varying order and size for each reduction block, which can cause nondeterministic outputs due to the non-associativity of floating-point arithmetic. To fix this, they replaced reduction kernels (RMSNorm, matrix multiplication, attention, etc…) with a batch-invariant implementation. These kernels were also released as <a href="https://github.com/thinking-machines-lab/batch_invariant_ops">a companion library</a> for external integration.</p>
<p><img src="/images/blog/deterministic/deterministic_intro.png" alt="figure1"><small><center><em>He, Horace and Thinking Machines Lab, &quot;Defeating Nondeterminism in LLM Inference&quot;,
Thinking Machines Lab: Connectionism, Sep 2025.</em></center></small></p>
<p>Building on the work of Thinking Machines Lab, SGLang delivers a robust, high-throughput solution for deterministic LLM inference, combining batch-invariant kernels, CUDA graphs, radix cache, and chunked prefill with efficient performance. Determinism has been extensively validated through comprehensive tests and RL training experiments.</p>
<p>Key enhancements include:</p>
<ul>
<li><strong>Integration of batch-invariant kernels</strong> from Thinking Machines Lab, including mean, log-softmax, and matrix multiplication kernels.</li>
<li><strong>Implementation of batch-invariant attention kernels</strong> with fixed split-KV size. Multiple backends are supported, including FlashInfer, FlashAttention 3, and Triton.</li>
<li><strong>Full compatibility with common inference features</strong>, such as chunked prefill, CUDA graph, radix cache, all of which remain supported when deterministic inference is enabled.</li>
<li><strong>Expose a per-request seed</strong> in sampling arguments, allowing users to enable deterministic inference even when temperature &gt; 0.</li>
<li><strong>Better performance</strong>: Compared to the <strong>61.5%</strong> slowdown reported in TML's blog, SGLang achieves an average slowdown of only <strong>34.35%</strong> with the FlashInfer and FlashAttention 3 backends, representing a significant improvement. With CUDA graphs, 2.8x speedup can be achieved compared to the minimal integration.</li>
</ul>
<h2><a id="results" class="anchor" href="#results" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Results</h2>
<h3><a id="verifying-deterministic-behavior" class="anchor" href="#verifying-deterministic-behavior" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Verifying Deterministic Behavior</h3>
<p>We introduce <a href="https://github.com/sgl-project/sglang/blob/f1d789231896da438749b395f7bf007a5b0819c0/python/sglang/test/test_deterministic.py">a deterministic test suite</a> to verify whether inference results remain consistent under different batching conditions. The test covers three subtests, progressing from simple to more challenging:</p>
<ul>
<li>Single: Run the same prompt across varying batch sizes and check if outputs remain identical.</li>
<li>Mixed: Mix different types of prompts (short prompts and long prompts) within the same batch and verify consistency.</li>
<li>Prefix: Use prompts derived from the same long text with different prefix lengths, batch them randomly, and test whether results are reproducible across trials.</li>
</ul>
<p>Here are the results from 50 sampling trials. The numbers indicate the count of unique outputs observed for each subtest (lower = more deterministic).</p>
<table>
<thead>
<tr>
<th>Attention Backend</th>
<th>Mode</th>
<th>Single Test</th>
<th>Mixed Test (P1/P2/Long)</th>
<th>Prefix Test (prefix_len=1/511/2048/4097)</th>
</tr>
</thead>
<tbody>
<tr>
<td>FlashInfer</td>
<td>Normal</td>
<td>4</td>
<td>3 / 3 / 2</td>
<td>5 / 8 / 18 / 2</td>
</tr>
<tr>
<td>FlashInfer</td>
<td>Deterministic</td>
<td>1</td>
<td>1 / 1 / 1</td>
<td>1 / 1 / 1 / 1</td>
</tr>
<tr>
<td>FA3</td>
<td>Normal</td>
<td>3</td>
<td>3 / 2 / 2</td>
<td>4 / 4 / 10 / 1</td>
</tr>
<tr>
<td>FA3</td>
<td>Deterministic</td>
<td>1</td>
<td>1 / 1 / 1</td>
<td>1 / 1 / 1 / 1</td>
</tr>
<tr>
<td>Triton</td>
<td>Normal</td>
<td>3</td>
<td>2 / 3 / 1</td>
<td>5 / 4 / 13 / 2</td>
</tr>
<tr>
<td>Triton</td>
<td>Deterministic</td>
<td>1</td>
<td>1 / 1 / 1</td>
<td>1 / 1 / 1 / 1</td>
</tr>
</tbody>
</table>
<hr>
<p><small>*Tested on QWen3-8B</small></p>
<p><small>* Cuda graph, chunked prefill are enabled. Radix cache is disabled for Flashinfer and Triton since their support is still in progress. </small></p>
<h3><a id="cuda-graph-acceleration" class="anchor" href="#cuda-graph-acceleration" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>CUDA Graph Acceleration</h3>
<p>CUDA graphs can accelerate the inference process by consolidating multiple kernel launches into a single launch. Our evaluation compared the total throughput of deterministic inference with and without CUDA graphs enabled. The test workload consisted of 16 requests, each with an input length of 1024 and an output length of 1024. The results show an at least 2.79x speedup across all attention kernels when CUDA graphs is utilized.</p>
<table>
<thead>
<tr>
<th>Attention Backend</th>
<th>CUDA Graph</th>
<th>Throughput (tokens/s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>FlashInfer</td>
<td>Disabled</td>
<td>441.73</td>
</tr>
<tr>
<td>FlashInfer</td>
<td>Enabled</td>
<td>1245.51 (2.82x)</td>
</tr>
<tr>
<td>FA3</td>
<td>Disabled</td>
<td>447.64</td>
</tr>
<tr>
<td>FA3</td>
<td>Enabled</td>
<td>1247.64 (2.79x)</td>
</tr>
<tr>
<td>Triton</td>
<td>Disabled</td>
<td>419.64</td>
</tr>
<tr>
<td>Triton</td>
<td>Enabled</td>
<td>1228.36 (2.93x)</td>
</tr>
</tbody>
</table>
<hr>
<p><small>*Setup: QWen3-8B, TP1, H100 80GB  </small></p>
<p><small>*We disabled radix cache for all performance benchmarks since FlashInfer and Triton Radix Cache support is still in progress. </small></p>
<h3><a id="measuring-offline-inference-performance" class="anchor" href="#measuring-offline-inference-performance" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Measuring Offline Inference Performance</h3>
<p>We measured end-to-end latency for both non-deterministic and deterministic modes using three common RL rollout workloads (256 requests with varying input/output lengths).</p>
<p>Deterministic inference is generally usable, with most slowdowns ranging from 25% to 45%, and average slowdown of FlashInfer and FlashAttention 3 backends being 34.35%. The majority of this overhead comes from unoptimized batch-invariant kernels (matrix multiplication and attention), indicating significant room for performance improvements.</p>
<table>
<thead>
<tr>
<th>Attention Backend</th>
<th>Mode</th>
<th>Input 1024 Output 1024</th>
<th>Input 4096 Output 4096</th>
<th>Input 8192 Output 8192</th>
</tr>
</thead>
<tbody>
<tr>
<td>FlashInfer</td>
<td>Normal</td>
<td>30.85</td>
<td>332.32</td>
<td>1623.87</td>
</tr>
<tr>
<td>FlashInfer</td>
<td>Deterministic</td>
<td>43.99 (+42.6%)</td>
<td>485.16 (+46.0%)</td>
<td>2020.13 (+24.4%)</td>
</tr>
<tr>
<td>FA3</td>
<td>Normal</td>
<td>34.70</td>
<td>379.85</td>
<td>1438.41</td>
</tr>
<tr>
<td>FA3</td>
<td>Deterministic</td>
<td>44.14 (+27.2%)</td>
<td>494.56 (+30.2%)</td>
<td>1952.92 (+35.7%)</td>
</tr>
<tr>
<td>Triton</td>
<td>Normal</td>
<td>36.91</td>
<td>400.59</td>
<td>1586.05</td>
</tr>
<tr>
<td>Triton</td>
<td>Deterministic</td>
<td>57.25 (+55.1%)</td>
<td>579.43 (+44.64%)</td>
<td>2296.60 (+44.80%)</td>
</tr>
</tbody>
</table>
<hr>
<p><small>*Setup: QWen3-8B, TP1, H200 140GB. </small></p>
<p><small>*We disabled radix cache for all performance benchmarks since FlashInfer and Triton Radix Cache support is still in progress. </small></p>
<p>We acknowledge that deterministic inference is significantly slower than normal mode. We recommend using it primarily for debugging and reproducibility. Future work will focus on accelerating deterministic inference, with the goal of reducing the performance gap to under 20%, or ideally achieving parity with normal mode.</p>
<h2><a id="usage" class="anchor" href="#usage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Usage</h2>
<h3><a id="environment-setup" class="anchor" href="#environment-setup" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Environment Setup</h3>
<p>To set up the environment, install SGLang with version &gt;=0.5.3</p>
<pre><code class="hljs language-bash">pip install <span class="hljs-string">&quot;sglang[all]&gt;=0.5.3&quot;</span>
</code></pre>
<h3><a id="launching-the-server" class="anchor" href="#launching-the-server" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Launching the Server</h3>
<p>SGLang supports deterministic inference across multiple models. For example, with Qwen3-8B you only need to add the <code>--enable-deterministic-inference</code> flag when launching the server:</p>
<pre><code class="hljs language-bash">python3 -m sglang.launch_server \
    --model-path Qwen/Qwen3-8B \
    --attention-backend &lt;flashinfer|fa3|triton&gt; \
    --enable-deterministic-inference
</code></pre>
<h2><a id="technical-details" class="anchor" href="#technical-details" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Technical Details</h2>
<h3><a id="chunked-prefill" class="anchor" href="#chunked-prefill" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Chunked Prefill</h3>
<p>SGLang's chunked prefill technique is designed to manage requests with long contexts. However, its default chunking strategy violates the determinism requirement for attention kernels.</p>
<p>As illustrated in the figure, consider two input sequences, <code>seq_a</code> and <code>seq_b</code>, each with a context length of 6,000. The maximum chunk size for chunk prefill is 8192, while the required split-KV size for deterministic attention is 2,048. Each sequence can be partitioned into three smaller units (<code>a1</code> to <code>a3</code> and <code>b1</code> to <code>b3</code>), with lengths of 2,048, 2,048, and 1,904, respectively. If these smaller units remain intact during chunk prefilling, then they can be processed by the same attention kernel and lead to deterministic reduction behavior.</p>
<img src="/images/blog/deterministic/chunked_prefill.png" style="width: 30vw; min-width: 200px;" />
<p>The standard chunking strategy operates on a &quot;best-effort&quot; principle. In this example, this strategy tries to generate a <code>chunk_1</code> of 8,192 tokens by splitting the <code>b2</code> unit of <code>seq_b</code> into two smaller parts. This can cause inconsistent truncation points since the length of <code>b2</code> after splitting depends on the length of <code>seq_a</code>. To address this, we adapted the chunking logic to <strong>align the truncation point with an integer multiple of the split_kv_size</strong>. This adjustment ensures that the processing of <code>b2</code> is deferred to a subsequent chunk, allowing it to be computed as a complete unit by the attention kernel.</p>
<h3><a id="attention-backends" class="anchor" href="#attention-backends" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Attention Backends</h3>
<p>Attention kernel is an important part of determinism. For different attention backends, we modified them in different ways to satisfy their usage requirements.</p>
<ul>
<li>For Flashinfer backend, we utilize the <code>fixed_split_size</code> and <code>disable_kv_split</code> arguments from <a href="https://github.com/flashinfer-ai/flashinfer/pull/1675">batch invariant FA2 kernels</a> to fix split sizes during kernel planning. Truncation of chunked prefill is aligned to the prefill split size. (<a href="https://github.com/sgl-project/sglang/pull/10645">PR link</a>)</li>
<li>For FlashAttention-3 backend, num-splits of flash attention kernel are fixed to 1 to ensure determinism. (<a href="https://github.com/sgl-project/sglang/pull/10651">PR link</a>)</li>
<li>For Triton backend, we fix the split size of decoding, and manually set the alignment size of chunked prefill. Deterministic inference can also run on <strong>AMD</strong> hardware with the extensibility of Triton backend. (<a href="https://github.com/sgl-project/sglang/pull/10694">PR link</a>).</li>
</ul>
<h3><a id="reproducible-non-greedy-sampling" class="anchor" href="#reproducible-non-greedy-sampling" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Reproducible Non-Greedy Sampling</h3>
<p>To extend determinism beyond greedy decoding, we introduce a new sampling function: <a href="https://github.com/sgl-project/sglang/blob/e2ac7888b8cb1fd6c33a7ec58d27a5f5b5b24e0c/python/sglang/srt/layers/sampler.py#L268-L299">multinomial_with_seed</a>.</p>
<p>Instead of relying on <code>torch.multinomial</code>, which is inherently nondeterministic under batching, this operator perturbs logits with Gumbel noise generated from a <strong>seeded hash function</strong>. As a result, the same <code>(inputs, seed)</code> pair always yields the same sample, even when temperature &gt; 0.</p>
<p>This modification enables <strong>deterministic multinomial sampling</strong> while preserving the stochasticity required by reinforcement learning rollouts.</p>
<h3><a id="rl-framework-integration-slime" class="anchor" href="#rl-framework-integration-slime" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>RL Framework Integration (slime)</h3>
<p>We <a href="https://github.com/THUDM/slime/pull/361">integrated</a> deterministic inference with temperature &gt; 0 into slime's GRPO training recipe. In preliminary experiments, repeated RL training runs produced <strong>identical rollout responses and loss values for the first iterations</strong>, confirming that the rollout process itself is deterministic.</p>
<p>In a follow-up <a href="https://github.com/THUDM/slime/pull/370">PR</a>, we further enabled full training reproducibility by implementing the following key configurations:</p>
<ul>
<li><strong>Flash Attention</strong>: Use Flash Attention v2 instead of v3 to enable deterministic backward passes</li>
<li><strong>Megatron</strong>: Set <code>--deterministic-mode</code> flag for deterministic training</li>
<li><strong>Environment Variables</strong>: Configure critical settings:
<ul>
<li><code>NCCL_ALGO=Ring</code></li>
<li><code>NVTE_ALLOW_NONDETERMINISTIC_ALGO=0</code></li>
<li><code>CUBLAS_WORKSPACE_CONFIG=:4096:8</code></li>
</ul>
</li>
<li><strong>PyTorch</strong>: Enable <code>torch.use_deterministic_algorithms(True, warn_only=False)</code></li>
</ul>
<p>With these comprehensive changes, we successfully achieved full training reproducibility for GRPO in slime, enabling truly deterministic end-to-end RL training pipelines.</p>
<h2><a id="future-work" class="anchor" href="#future-work" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Future Work</h2>
<p>Our future efforts will focus on enhancing deterministic inference by addressing the following key areas:</p>
<ul>
<li><strong>Faster batch invariant kernels</strong>: Batch invariant kernels are the bottleneck of performance, so we'll work on optimizing their configurations and potentially rewriting them to boost performance. This is also critical for improving the speed of RL rollouts.</li>
<li><strong>Support for MoE models</strong>: Currently we only support deterministic inference for dense models like QWen3-8B or LLaMa-3.1-8B. In the future we plan to expand our support to MoE models like Qwen3-30B-A3B or DeepSeek-V3.</li>
<li><strong>True On-Policy RL</strong>: We plan to further integrate deterministic inference into reinforcement learning frameworks (e.g., <a href="https://github.com/THUDM/slime">slime</a>) to enable reproducible sampling, with the ultimate goal of achieving true on-policy training.</li>
<li><strong>Enhancing Radix Cache Functionality</strong>: We will improve the radix tree to enable compatibility with a wider variety of attention kernels, moving beyond current limitation to the FlashAttention 3 backend.</li>
<li><strong>Tensor Parallelism</strong>: TP1 and TP2 are deterministic due to consistent floating-point addition order; larger TP setups require modifications to reduce kernels for determinism.</li>
<li><strong>FlexAttention Integration</strong>: Besides currently supported attention backends, we plan to extend our support of deterministic inference to FlexAttention in the future.</li>
<li>A <strong>roadmap</strong> for deterministic inference features can be found in <a href="https://github.com/sgl-project/sglang/issues/10278">this issue</a>.</li>
</ul>
<p>SGLang's deterministic inference and slime's reproducible training capabilities are currently under active development and improvement. We sincerely welcome users and developers to actively try out these features and provide us with valuable feedback. Your experience and suggestions will help us further optimize these important capabilities and advance the development of deterministic inference technology.</p>
<h2><a id="acknowledgement" class="anchor" href="#acknowledgement" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgement</h2>
<p>We would like to extend our heartfelt gratitude to the following teams and collaborators:</p>
<ul>
<li><strong>SGLang team and community</strong>: Baizhou Zhang, Biao He, Qiaolin Yu, Xinyuan Tong, Ke Bao, Yineng Zhang, Chi Zhang, Ying Sheng, Lianmin Zheng and many others</li>
<li><strong>Flashinfer team and community</strong>:  Wenxuan Tan, Yilong Zhao, Zihao Ye</li>
<li><strong>slime team and community</strong>: Zilin Zhu</li>
<li><strong>AMD</strong>: Yusheng Su</li>
<li><strong>Thinking Machines Lab</strong>: for their awesome <a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/">blog</a> and <a href="https://github.com/thinking-machines-lab/batch_invariant_ops">batch_invariant_ops library</a></li>
</ul>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Towards Deterministic Inference in SGLang and Reproducible RL Training","author":"The SGLang Team","date":"September 22, 2025 (Updated on September 24)","previewImg":"/images/blog/deterministic/logo.png"},"content":"\n\n**TL;DR**: This post shares our efforts to enable deterministic inference in SGLang and our collaboration with [slime](https://github.com/THUDM/slime) to work towards reproducible RL training.\n\n\u003cbr /\u003e\n\n\nRecently, the Thinking Machines Lab published a [blog](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/) detailing their findings. Since this blog was published, the industry has responded enthusiastically, eagerly awaiting open-source inference engines to implement stable and usable deterministic inference, or even further, to achieve fully reproducible RL training. Now, SGLang and slime together provide the answer.\n\nBuilding on Thinking Machines Lab's batch-invariant operators, SGLang achieves fully deterministic inference while maintaining compatibility with **chunked prefill**, **CUDA graphs**, **radix cache**, and **non-greedy sampling**. With CUDA graphs, SGLang delivers **2.8x acceleration** and reduces performance overhead to just **34.35%** (vs. TML's **61.5%**).\n \nTaking this deterministic inference capability further, SGLang collaborated with the slime team to unlock **100% reproducible RL training** - a breakthrough achieved with minimal code changes. Our validation experiments on Qwen3-8B demonstrate perfect reproducibility: **two independent training runs produce identical curves**, providing the reliability needed for rigorous scientific experimentation.\n\n![slime](/images/blog/deterministic/slime.png)\u003csmall\u003e\u003ccenter\u003e[*Reproducible Guide*](https://thudm.github.io/slime/_examples_synced/reproducibility/README.html#reproducibility)\u003c/center\u003e\u003c/small\u003e\n\n\n\u003cbr /\u003e\n\nNow let's dive into the some technical details.\n\n## Why Deterministic Inference Matters\n\nThe ability to achieve consistent outputs from large language models (LLMs) inference is increasingly important. For example, the indeterminism of inference results can implicitly transform on-policy reinforcement learning (RL) into off-policy RL as [researchers pointed out](https://fengyao.notion.site/off-policy-rl). However, even if we turn the temperature down to 0 in SGLang, the sampling is still not deterministic due to the use of dynamic batching and radix cache (past discussions [here](https://docs.sglang.ai/references/faq.html#the-results-are-not-deterministic-even-with-a-temperature-of-0)) .\n\nAs mentioned in the [blog](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/), the largest source of nondeterminism is the varying batch sizes: Even when a user repeatedly submits the same prompt, the output can vary across runs, since the request may be batched together with other users' requests, and differences in batch size can lead to nondeterministic inference results.\n\nTo explain more, different batch sizes will influence the reduction splitting process of kernels. This leads to  varying order and size for each reduction block, which can cause nondeterministic outputs due to the non-associativity of floating-point arithmetic. To fix this, they replaced reduction kernels (RMSNorm, matrix multiplication, attention, etc…) with a batch-invariant implementation. These kernels were also released as [a companion library](https://github.com/thinking-machines-lab/batch_invariant_ops) for external integration. \n\n![figure1](/images/blog/deterministic/deterministic_intro.png)\u003csmall\u003e\u003ccenter\u003e*He, Horace and Thinking Machines Lab, \"Defeating Nondeterminism in LLM Inference\", \nThinking Machines Lab: Connectionism, Sep 2025.*\u003c/center\u003e\u003c/small\u003e\n\n\nBuilding on the work of Thinking Machines Lab, SGLang delivers a robust, high-throughput solution for deterministic LLM inference, combining batch-invariant kernels, CUDA graphs, radix cache, and chunked prefill with efficient performance. Determinism has been extensively validated through comprehensive tests and RL training experiments.\n\nKey enhancements include:\n- **Integration of batch-invariant kernels** from Thinking Machines Lab, including mean, log-softmax, and matrix multiplication kernels.\n- **Implementation of batch-invariant attention kernels** with fixed split-KV size. Multiple backends are supported, including FlashInfer, FlashAttention 3, and Triton.\n- **Full compatibility with common inference features**, such as chunked prefill, CUDA graph, radix cache, all of which remain supported when deterministic inference is enabled.\n- **Expose a per-request seed** in sampling arguments, allowing users to enable deterministic inference even when temperature \u003e 0.\n- **Better performance**: Compared to the **61.5%** slowdown reported in TML's blog, SGLang achieves an average slowdown of only **34.35%** with the FlashInfer and FlashAttention 3 backends, representing a significant improvement. With CUDA graphs, 2.8x speedup can be achieved compared to the minimal integration.\n\n\n## Results\n\n\n### Verifying Deterministic Behavior\n\nWe introduce [a deterministic test suite](https://github.com/sgl-project/sglang/blob/f1d789231896da438749b395f7bf007a5b0819c0/python/sglang/test/test_deterministic.py) to verify whether inference results remain consistent under different batching conditions. The test covers three subtests, progressing from simple to more challenging:\n\n- Single: Run the same prompt across varying batch sizes and check if outputs remain identical.\n- Mixed: Mix different types of prompts (short prompts and long prompts) within the same batch and verify consistency.\n- Prefix: Use prompts derived from the same long text with different prefix lengths, batch them randomly, and test whether results are reproducible across trials.\n\nHere are the results from 50 sampling trials. The numbers indicate the count of unique outputs observed for each subtest (lower = more deterministic).\n\n| Attention Backend | Mode | Single Test | Mixed Test (P1/P2/Long) | Prefix Test (prefix_len=1/511/2048/4097) | \n| --- | --- | --- | --- | --- |\n| FlashInfer | Normal | 4| 3 / 3 / 2 | 5 / 8 / 18 / 2 |\n| FlashInfer | Deterministic | 1 | 1 / 1 / 1 | 1 / 1 / 1 / 1 |\n| FA3 | Normal | 3 | 3 / 2 / 2 | 4 / 4 / 10 / 1 |\n| FA3 | Deterministic | 1 | 1 / 1 / 1 | 1 / 1 / 1 / 1 |\n| Triton | Normal | 3 | 2 / 3 / 1 | 5 / 4 / 13 / 2 |\n| Triton | Deterministic | 1 | 1 / 1 / 1 | 1 / 1 / 1 / 1 |\n---\n\u003csmall\u003e*Tested on QWen3-8B\u003c/small\u003e\n\n\u003csmall\u003e* Cuda graph, chunked prefill are enabled. Radix cache is disabled for Flashinfer and Triton since their support is still in progress. \u003c/small\u003e\n\n\n### CUDA Graph Acceleration \n\nCUDA graphs can accelerate the inference process by consolidating multiple kernel launches into a single launch. Our evaluation compared the total throughput of deterministic inference with and without CUDA graphs enabled. The test workload consisted of 16 requests, each with an input length of 1024 and an output length of 1024. The results show an at least 2.79x speedup across all attention kernels when CUDA graphs is utilized.\n\n| Attention Backend | CUDA Graph | Throughput (tokens/s) |\n| --- | --- | --- |\n| FlashInfer | Disabled | 441.73 |\n| FlashInfer | Enabled | 1245.51 (2.82x) |\n| FA3 | Disabled | 447.64 |\n| FA3 | Enabled | 1247.64 (2.79x) |\n| Triton | Disabled | 419.64 |\n| Triton | Enabled | 1228.36 (2.93x) |\n---\n\u003csmall\u003e*Setup: QWen3-8B, TP1, H100 80GB  \u003c/small\u003e\n\n\u003csmall\u003e*We disabled radix cache for all performance benchmarks since FlashInfer and Triton Radix Cache support is still in progress. \u003c/small\u003e\n\n### Measuring Offline Inference Performance\n\nWe measured end-to-end latency for both non-deterministic and deterministic modes using three common RL rollout workloads (256 requests with varying input/output lengths).\n\nDeterministic inference is generally usable, with most slowdowns ranging from 25% to 45%, and average slowdown of FlashInfer and FlashAttention 3 backends being 34.35%. The majority of this overhead comes from unoptimized batch-invariant kernels (matrix multiplication and attention), indicating significant room for performance improvements.\n\n| Attention Backend | Mode | Input 1024 Output 1024| Input 4096 Output 4096 | Input 8192 Output 8192 | \n| --- | --- | --- | --- | --- |\n| FlashInfer | Normal | 30.85 | 332.32 | 1623.87 |\n| FlashInfer | Deterministic | 43.99 (+42.6%) | 485.16 (+46.0%) | 2020.13 (+24.4%) |\n| FA3 | Normal | 34.70 | 379.85 | 1438.41 |\n| FA3 | Deterministic | 44.14 (+27.2%) | 494.56 (+30.2%) | 1952.92 (+35.7%) |\n| Triton | Normal | 36.91 | 400.59 | 1586.05  |\n| Triton | Deterministic | 57.25 (+55.1%) | 579.43 (+44.64%) | 2296.60 (+44.80%) |\n---\n\u003csmall\u003e*Setup: QWen3-8B, TP1, H200 140GB. \u003c/small\u003e\n\n\u003csmall\u003e*We disabled radix cache for all performance benchmarks since FlashInfer and Triton Radix Cache support is still in progress. \u003c/small\u003e\n\nWe acknowledge that deterministic inference is significantly slower than normal mode. We recommend using it primarily for debugging and reproducibility. Future work will focus on accelerating deterministic inference, with the goal of reducing the performance gap to under 20%, or ideally achieving parity with normal mode.\n\n## Usage\n\n### Environment Setup\n\nTo set up the environment, install SGLang with version \u003e=0.5.3\n```bash\npip install \"sglang[all]\u003e=0.5.3\"\n```\n### Launching the Server\n\nSGLang supports deterministic inference across multiple models. For example, with Qwen3-8B you only need to add the `--enable-deterministic-inference` flag when launching the server:\n\n```bash\npython3 -m sglang.launch_server \\\n    --model-path Qwen/Qwen3-8B \\\n    --attention-backend \u003cflashinfer|fa3|triton\u003e \\\n    --enable-deterministic-inference\n```\n\n## Technical Details\n\n\n### Chunked Prefill\n\nSGLang's chunked prefill technique is designed to manage requests with long contexts. However, its default chunking strategy violates the determinism requirement for attention kernels.  \n\nAs illustrated in the figure, consider two input sequences, `seq_a` and `seq_b`, each with a context length of 6,000. The maximum chunk size for chunk prefill is 8192, while the required split-KV size for deterministic attention is 2,048. Each sequence can be partitioned into three smaller units (`a1` to `a3` and `b1` to `b3`), with lengths of 2,048, 2,048, and 1,904, respectively. If these smaller units remain intact during chunk prefilling, then they can be processed by the same attention kernel and lead to deterministic reduction behavior.\n\n\n\u003cimg src=\"/images/blog/deterministic/chunked_prefill.png\" style=\"width: 30vw; min-width: 200px;\" /\u003e\n\n\nThe standard chunking strategy operates on a \"best-effort\" principle. In this example, this strategy tries to generate a `chunk_1` of 8,192 tokens by splitting the `b2` unit of `seq_b` into two smaller parts. This can cause inconsistent truncation points since the length of `b2` after splitting depends on the length of `seq_a`. To address this, we adapted the chunking logic to **align the truncation point with an integer multiple of the split_kv_size**. This adjustment ensures that the processing of `b2` is deferred to a subsequent chunk, allowing it to be computed as a complete unit by the attention kernel. \n\n### Attention Backends\n\nAttention kernel is an important part of determinism. For different attention backends, we modified them in different ways to satisfy their usage requirements.\n- For Flashinfer backend, we utilize the `fixed_split_size` and `disable_kv_split` arguments from [batch invariant FA2 kernels](https://github.com/flashinfer-ai/flashinfer/pull/1675) to fix split sizes during kernel planning. Truncation of chunked prefill is aligned to the prefill split size. ([PR link](https://github.com/sgl-project/sglang/pull/10645))\n- For FlashAttention-3 backend, num-splits of flash attention kernel are fixed to 1 to ensure determinism. ([PR link](https://github.com/sgl-project/sglang/pull/10651))\n- For Triton backend, we fix the split size of decoding, and manually set the alignment size of chunked prefill. Deterministic inference can also run on **AMD** hardware with the extensibility of Triton backend. ([PR link](https://github.com/sgl-project/sglang/pull/10694)). \n\n\n### Reproducible Non-Greedy Sampling\nTo extend determinism beyond greedy decoding, we introduce a new sampling function: [multinomial_with_seed](https://github.com/sgl-project/sglang/blob/e2ac7888b8cb1fd6c33a7ec58d27a5f5b5b24e0c/python/sglang/srt/layers/sampler.py#L268-L299).\n\nInstead of relying on `torch.multinomial`, which is inherently nondeterministic under batching, this operator perturbs logits with Gumbel noise generated from a **seeded hash function**. As a result, the same `(inputs, seed)` pair always yields the same sample, even when temperature \u003e 0.\n\n\nThis modification enables **deterministic multinomial sampling** while preserving the stochasticity required by reinforcement learning rollouts.\n\n\n### RL Framework Integration (slime)\n\nWe [integrated](https://github.com/THUDM/slime/pull/361) deterministic inference with temperature \u003e 0 into slime's GRPO training recipe. In preliminary experiments, repeated RL training runs produced **identical rollout responses and loss values for the first iterations**, confirming that the rollout process itself is deterministic. \n\nIn a follow-up [PR](https://github.com/THUDM/slime/pull/370), we further enabled full training reproducibility by implementing the following key configurations:\n\n- **Flash Attention**: Use Flash Attention v2 instead of v3 to enable deterministic backward passes\n- **Megatron**: Set `--deterministic-mode` flag for deterministic training\n- **Environment Variables**: Configure critical settings:\n  - `NCCL_ALGO=Ring`\n  - `NVTE_ALLOW_NONDETERMINISTIC_ALGO=0`\n  - `CUBLAS_WORKSPACE_CONFIG=:4096:8`\n- **PyTorch**: Enable `torch.use_deterministic_algorithms(True, warn_only=False)`\n\nWith these comprehensive changes, we successfully achieved full training reproducibility for GRPO in slime, enabling truly deterministic end-to-end RL training pipelines.\n\n\n## Future Work\nOur future efforts will focus on enhancing deterministic inference by addressing the following key areas:\n- **Faster batch invariant kernels**: Batch invariant kernels are the bottleneck of performance, so we'll work on optimizing their configurations and potentially rewriting them to boost performance. This is also critical for improving the speed of RL rollouts.\n- **Support for MoE models**: Currently we only support deterministic inference for dense models like QWen3-8B or LLaMa-3.1-8B. In the future we plan to expand our support to MoE models like Qwen3-30B-A3B or DeepSeek-V3.\n- **True On-Policy RL**: We plan to further integrate deterministic inference into reinforcement learning frameworks (e.g., [slime](https://github.com/THUDM/slime)) to enable reproducible sampling, with the ultimate goal of achieving true on-policy training.\n- **Enhancing Radix Cache Functionality**: We will improve the radix tree to enable compatibility with a wider variety of attention kernels, moving beyond current limitation to the FlashAttention 3 backend.\n- **Tensor Parallelism**: TP1 and TP2 are deterministic due to consistent floating-point addition order; larger TP setups require modifications to reduce kernels for determinism.\n- **FlexAttention Integration**: Besides currently supported attention backends, we plan to extend our support of deterministic inference to FlexAttention in the future.\n- A **roadmap** for deterministic inference features can be found in [this issue](https://github.com/sgl-project/sglang/issues/10278).\n\nSGLang's deterministic inference and slime's reproducible training capabilities are currently under active development and improvement. We sincerely welcome users and developers to actively try out these features and provide us with valuable feedback. Your experience and suggestions will help us further optimize these important capabilities and advance the development of deterministic inference technology. \n\n## Acknowledgement\nWe would like to extend our heartfelt gratitude to the following teams and collaborators:\n- **SGLang team and community**: Baizhou Zhang, Biao He, Qiaolin Yu, Xinyuan Tong, Ke Bao, Yineng Zhang, Chi Zhang, Ying Sheng, Lianmin Zheng and many others\n- **Flashinfer team and community**:  Wenxuan Tan, Yilong Zhao, Zihao Ye\n- **slime team and community**: Zilin Zhu\n- **AMD**: Yusheng Su\n- **Thinking Machines Lab**: for their awesome [blog](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/) and [batch_invariant_ops library](https://github.com/thinking-machines-lab/batch_invariant_ops)","slug":"2025-09-22-sglang-deterministic"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2025-09-22-sglang-deterministic"},"buildId":"gGneV6uQXQMqd7OD7RXYJ","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>