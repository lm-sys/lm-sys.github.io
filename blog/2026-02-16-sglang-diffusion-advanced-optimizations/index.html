<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>SGLang-Diffusion: Advanced Optimizations for Production-Ready Video Generation | LMSYS Org</title><meta name="title" content="SGLang-Diffusion: Advanced Optimizations for Production-Ready Video Generation | LMSYS Org"/><meta property="og:title" content="SGLang-Diffusion: Advanced Optimizations for Production-Ready Video Generation | LMSYS Org"/><meta name="twitter:title" content="SGLang-Diffusion: Advanced Optimizations for Production-Ready Video Generation | LMSYS Org"/><meta name="description" content="&lt;p&gt;Following our &lt;a href=&quot;https://lmsys.org/blog/2026-01-16-sglang-diffusion/&quot;&gt;two-month progress update&lt;/a&gt;, we&#x27;re excited to share a
deeper dive into the a..."/><meta property="og:description" content="&lt;p&gt;Following our &lt;a href=&quot;https://lmsys.org/blog/2026-01-16-sglang-diffusion/&quot;&gt;two-month progress update&lt;/a&gt;, we&#x27;re excited to share a
deeper dive into the a..."/><meta name="twitter:description" content="&lt;p&gt;Following our &lt;a href=&quot;https://lmsys.org/blog/2026-01-16-sglang-diffusion/&quot;&gt;two-month progress update&lt;/a&gt;, we&#x27;re excited to share a
deeper dive into the a..."/><meta property="og:image" content="https://lmsys.org/images/blog/sgl-diffusion/sgl-diffusion-banner-16-9.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/sgl-diffusion/sgl-diffusion-banner-16-9.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2026-02-16-sglang-diffusion-advanced-optimizations"/><meta name="twitter:url" content="https://lmsys.org/blog/2026-02-16-sglang-diffusion-advanced-optimizations"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d62cc293bc63f5ee.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/QRPdtedtwlzVI-v5jUlfR/_buildManifest.js" defer=""></script><script src="/_next/static/QRPdtedtwlzVI-v5jUlfR/_ssgManifest.js" defer=""></script><script src="/_next/static/QRPdtedtwlzVI-v5jUlfR/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.io" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">SGLang-Diffusion: Advanced Optimizations for Production-Ready Video Generation</h1><p class="text-xl pt-2 pb-2">by: <!-- -->The SGLang-Diffusion Team<!-- -->,<!-- --> <!-- -->Feb 16, 2026<!-- --></p><hr/><div class="pt-2 article"><p>Following our <a href="https://lmsys.org/blog/2026-01-16-sglang-diffusion/">two-month progress update</a>, we're excited to share a
deeper dive into the advanced optimizations that make SGLang-Diffusion a production-ready framework for video
generation. These improvements focus on scalability, efficiency, and stability—essential for deploying diffusion models
at scale.</p>
<p>Here's what we've been working on:</p>
<h2><a id="overview" class="anchor" href="#overview" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Overview</h2>
<p>As video generation models continue to grow in complexity, we've identified and addressed critical bottlenecks across
the entire inference pipeline:</p>
<ul>
<li><strong>Smarter Parallelism</strong>: Token-level sequence sharding and parallel folding for optimal resource utilization</li>
<li><strong>Distributed VAE</strong>: Parallel encoding/decoding to eliminate memory bottlenecks for high-resolution video</li>
<li><strong>Production-Ready Serving</strong>: Fixed Cache-DiT integration bugs for stable multi-request serving</li>
<li><strong>Optimized I/O</strong>: Accelerated video save operations by eliminating unnecessary serialization</li>
<li><strong>Fused Kernels</strong>: Custom JIT kernels for LayerNorm variants, reducing GPU bubbles</li>
</ul>
<p>Let's dive into the technical details.</p>
<h2><a id="key-improvements" class="anchor" href="#key-improvements" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Key Improvements</h2>
<h3><a id="1-sp-sharding-improvement-from-frame-level-to-token-level" class="anchor" href="#1-sp-sharding-improvement-from-frame-level-to-token-level" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>1. SP-Sharding Improvement: From Frame-Level to Token-Level</h3>
<p>For Video DiT models, input tensors typically have shape <code>B, T, H, W, C</code>. For a common configuration with
<code>num_frames=81</code>, this might be: <code>1, 21, 90, 160, 3</code>.</p>
<p>In an 8×H100 setup with Ulysses Sequence Parallel (N=8), the framework needs to shard along the sequence dimension
during non-attention operations, then use all-to-all communication to switch to head dimension sharding for attention.</p>
<h4><a id="previous-approach-frame-level-sharding" class="anchor" href="#previous-approach-frame-level-sharding" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Previous Approach: Frame-Level Sharding</h4>
<p>Our initial implementation sharded directly along the <code>T</code> (temporal) dimension. However, 21 frames cannot be evenly
divided by 8 GPUs, leading to two suboptimal solutions:</p>
<ol>
<li><strong>Adjust-frame</strong>: Modify <code>num_frames</code> during preprocessing to make T divisible by N</li>
<li><strong>Token Padding</strong>: Pad the temporal dimension to the next multiple of N (21 → 24)</li>
</ol>
<p>The frame-level padding approach introduces significant overhead: each padded token requires <code>H × W × C</code> redundant
computations.</p>
<h4><a id="new-approach-token-level-sharding" class="anchor" href="#new-approach-token-level-sharding" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>New Approach: Token-Level Sharding</h4>
<p>To minimize padding overhead, we now <strong>flatten <code>T × H × W</code> into a single sequence dimension</strong> before sharding. This has
two major benefits:</p>
<ul>
<li><strong>Reduced or Zero Padding</strong>: For common resolutions and VAE configurations, <code>H × W</code> is often divisible by 8,
eliminating padding entirely</li>
<li><strong>Lower Communication Volume</strong>: When padding is needed, the overhead is minimal compared to frame-level padding</li>
</ul>
<h3><a id="comparison-shape-and-comm-volume-analysis" class="anchor" href="#comparison-shape-and-comm-volume-analysis" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Comparison: Shape and Comm Volume Analysis</h3>
<table>
<thead>
<tr>
<th>Solution</th>
<th>Padding Overhead</th>
<th>Input Tensor Shape (Per-rank)</th>
<th>All-to-All Comm Volume</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Frame Sharding</strong></td>
<td>3 frames (14.3%)</td>
<td><code>3, 90, 160, C</code> (24/8)</td>
<td><code>1.0 × feature_map</code></td>
</tr>
<tr>
<td><strong>Token Sharding</strong></td>
<td>0 frames</td>
<td><code>2.625, 90, 160, C</code> (21/8)</td>
<td><code>0.875 × feature_map</code></td>
</tr>
</tbody>
</table>
<p>This optimization delivers both faster communication and reduced memory footprint, especially for video models.</p>
<p>See related <a href="https://github.com/sgl-project/sglang/pull/18161">PR</a> for technical details.</p>
<h3><a id="2-parallel-folding-decoupling-text-encoder-and-dit-parallelism" class="anchor" href="#2-parallel-folding-decoupling-text-encoder-and-dit-parallelism" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>2. Parallel Folding: Decoupling Text Encoder and DiT Parallelism</h3>
<p>In our original implementation, the Text Encoder and DiT shared the same Tensor Parallel (TP) group. When DiT used only
Sequence Parallel (SP), this meant the Text Encoder ran with TP=1—each GPU held a complete model copy, wasting memory
and compute.</p>
<p>Since Text Encoder and DiT computations are <strong>completely decoupled</strong>, we introduced <strong>Parallel Folding</strong>: the Text
Encoder now uses the DiT's SP group as its TP group.</p>
<p><strong>What this means in practice:</strong></p>
<ul>
<li><strong>For Text Encoder</strong>: Apply TP across the SP group to maximize speed and reduce memory</li>
<li><strong>For Denoiser</strong>: Apply SP to optimize throughput and memory for sequence processing</li>
</ul>
<p>This approach ensures both components use optimal parallelism strategies without interference, improving overall
efficiency.</p>
<p>See related <a href="https://github.com/sgl-project/sglang/pull/17818">PR</a> for technical details.</p>
<h3><a id="3-parallel-vae-distributed-encodingdecoding" class="anchor" href="#3-parallel-vae-distributed-encodingdecoding" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3. Parallel VAE: Distributed Encoding/Decoding</h3>
<p>VAE encoding/decoding involves heavy 3D convolution operations. For high-resolution video, single-GPU implementations
are slow and prone to OOM.</p>
<p>The two common approaches to alleviate this are:</p>
<ol>
<li><strong>Tiling</strong>: Split feature maps into tiles, process them sequentially—reduces peak memory but increases latency</li>
<li><strong>Parallel</strong>: Distribute tiles across GPUs for concurrent processing—reduces both peak memory and latency</li>
</ol>
<p>We implemented <strong>Parallel VAE</strong> for Wan-VAE with the following strategy:</p>
<ul>
<li><strong>Height-wise Sharding</strong>: Split feature maps along the height dimension across ranks</li>
<li><strong>Conv Operations</strong>: Use <code>halo_exchange</code> to share boundary pixels between neighboring ranks (P2P), ensuring
mathematical
equivalence with global convolution</li>
<li><strong>Attention Operations</strong>: Use <code>all_gather</code> for global context when needed</li>
<li><strong>Result Aggregation</strong>: <code>all_gather</code> to reconstruct full height at the end of encoding/decoding</li>
</ul>
<p>This approach eliminates VAE as a bottleneck for high-resolution video generation, enabling higher resolutions and
longer sequences without OOM.</p>
<h3><a id="4-serving-with-cache-dit-fixing-multi-request-stability" class="anchor" href="#4-serving-with-cache-dit-fixing-multi-request-stability" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>4. Serving with Cache-DiT: Fixing Multi-Request Stability</h3>
<p><a href="https://github.com/vipshop/cache-dit">Cache-DiT</a> in SGLang-Diffusion accelerates inference by caching residuals and
skipping redundant
computations. However, its correct operation depends on proper <code>num_inference_steps</code> configuration, which determines
step counting and the Selective Computation Mask (SCM).</p>
<p><strong>The Problem:</strong></p>
<p>Wan2.2 uses a dual-transformer architecture, where <code>transformer</code> and <code>transformer_2</code> execute <code>num_high_noise_steps</code> and
<code>num_low_noise_steps</code> respectively (summing to <code>num_inference_steps</code>). Our initial implementation had two critical bugs:</p>
<ol>
<li>Both transformers incorrectly used total <code>num_inference_steps</code> to configure their cache contexts</li>
<li>In serving mode, cache contexts persisted across requests, even when different requests used different
<code>num_inference_steps</code></li>
</ol>
<p>These issues caused incorrect step counting and cache buffer contamination. When consecutive requests had different
video shapes, cache buffers would encounter shape mismatches, <strong>crashing the server</strong>.</p>
<p><strong>Our Solution:</strong></p>
<ol>
<li><code>transformer</code> and <code>transformer_2</code> now use <code>num_high_noise_steps</code> and <code>num_low_noise_steps</code> respectively to configure
independent cache contexts</li>
<li>For each new request, we recalculate timestep splits and <strong>refresh</strong> cache contexts using Cache-DiT's API, completely
isolating requests</li>
</ol>
<p>This ensures stable, production-ready serving with Cache-DiT acceleration.</p>
<h3><a id="5-optimize-video-save-eliminating-serialization-overhead" class="anchor" href="#5-optimize-video-save-eliminating-serialization-overhead" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>5. Optimize Video Save: Eliminating Serialization Overhead</h3>
<p>In our serving architecture, <code>scheduler_client</code> and <code>gpu_worker</code> communicate via ZMQ.</p>
<p>Previously, <code>gpu_worker</code> would:</p>
<ol>
<li>Complete inference</li>
<li>Serialize output tensor</li>
<li>Send tensor to <code>scheduler_client</code> via ZMQ</li>
<li><code>scheduler_client</code> deserializes tensor</li>
<li><code>scheduler_client</code> processes tensor and saves video</li>
</ol>
<p>This introduced significant overhead from serialization/deserialization and memory copies.</p>
<p><strong>Our Solution:</strong></p>
<p><code>gpu_worker</code> now directly processes the output tensor and saves the video to disk, returning only the file path to
<code>scheduler_client</code>.</p>
<p>This eliminates serialization/deserialization overhead, while avoiding duplicate tensor copies.</p>
<h3><a id="6-wanvideo-layernorm-fusion-cutedsl-jit-kernels" class="anchor" href="#6-wanvideo-layernorm-fusion-cutedsl-jit-kernels" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>6. WanVideo LayerNorm Fusion: CuTeDSL JIT Kernels</h3>
<p>WanVideo introduces two specialized LayerNorm patterns:</p>
<ol>
<li>
<p><strong>LayerNormScaleShift</strong>: <code>y = LN(x) * (1 + scale) + shift</code></p>
</li>
<li>
<p><strong>ScaleResidualLayerNormScaleShift</strong>:</p>
<ul>
<li><code>residual_out = residual + gate * x</code></li>
<li><code>y = LN(residual_out) * (1 + scale) + shift</code></li>
</ul>
</li>
</ol>
<p>These patterns combine elementwise operations with normalization reductions. Implementing them as separate kernels would
introduce multiple kernel launches and intermediate memory traffic, creating GPU bubbles.</p>
<p><strong>Our Solution:</strong></p>
<p>We implemented <strong>fused JIT kernels</strong> using CuTeDSL (located in <a href="https://github.com/sgl-project/sglang/tree/main/python/sglang/jit_kernel/diffusion/cutedsl"><code>sglang/jit_kernel/diffusion/cutedsl</code></a>) that combine
these operations into single, efficient kernels.</p>
<p><strong>Benefits:</strong></p>
<ul>
<li><strong>Fewer Kernel Launches</strong>: Reduced launch overhead</li>
<li><strong>Lower Memory Traffic</strong>: Eliminates intermediate reads/writes</li>
<li><strong>Better GPU Utilization</strong>: Reduces bubbles and improves throughput</li>
</ul>
<p>These micro-optimizations add up, especially for multi-layer architectures like WanVideo.</p>
<h2><a id="performance-results" class="anchor" href="#performance-results" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Performance Results</h2>
<p>Here's a comparison of SGLang-Diffusion and LightX2V for Wan2.2 T2V under different settings:</p>
<iframe style="display:block; margin: auto;" width="838" height="523" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQRK_j_q8NXZKEqtrTBagxFxvvaxYXXB56HTqqYlD_aAv1v74WKle2HIc7HPK3P0ZVrYlZrjshKYnaV/pubchart?oid=677973346&amp;format=interactive"></iframe>
<h2><a id="whats-next" class="anchor" href="#whats-next" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What's Next</h2>
<p>We continue to push the boundaries of diffusion model serving. Please refer to <a href="https://github.com/sgl-project/sglang/issues/18286"><strong>SGLang-Diffusion's Roadmap for 26Q1</strong></a> for more details.</p>
<p>Stay tuned for more updates as we continue to optimize SGLang-Diffusion for production deployments.</p>
<h2><a id="acknowledgment" class="anchor" href="#acknowledgment" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgment</h2>
<ul>
<li>We would like to thank the following contributors for their work on these optimizations:
<strong>Skywork.ai, <a href="https://github.com/Songrui625">Song Rui</a>, SGLang-Diffusion Team</strong></li>
<li>Special thanks to our compute partners for their continued support.</li>
</ul>
<p>Try diffusion generation, proudly powered by SGLang-Diffusion: <a href="https://www.apifree.ai/home">APIFree</a></p>
<h2><a id="learn-more" class="anchor" href="#learn-more" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Learn More</h2>
<ul>
<li><strong>Slack channel</strong>: <a href="https://sgl-fru7574.slack.com/archives/C09P0HTKE6A">#diffusion</a> (join via slack.sglang.io)</li>
<li><a href="https://cookbook.sglang.io/docs/diffusion"><strong>Cookbook for SGLang-Diffusion</strong></a></li>
<li><a href="https://github.com/sgl-project/sglang/blob/main/python/sglang/multimodal_gen/docs"><strong>Documentation on SGLang-Diffusion</strong></a></li>
<li><a href="https://lmsys.org/blog/2026-01-16-sglang-diffusion/"><strong>Previous Update: Two Months In</strong></a></li>
</ul>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"SGLang-Diffusion: Advanced Optimizations for Production-Ready Video Generation","author":"The SGLang-Diffusion Team","date":"February 16, 2026","previewImg":"/images/blog/sgl-diffusion/sgl-diffusion-banner-16-9.png"},"content":"\nFollowing our [two-month progress update](https://lmsys.org/blog/2026-01-16-sglang-diffusion/), we're excited to share a\ndeeper dive into the advanced optimizations that make SGLang-Diffusion a production-ready framework for video\ngeneration. These improvements focus on scalability, efficiency, and stability—essential for deploying diffusion models\nat scale.\n\nHere's what we've been working on:\n\n## Overview\n\nAs video generation models continue to grow in complexity, we've identified and addressed critical bottlenecks across\nthe entire inference pipeline:\n\n- **Smarter Parallelism**: Token-level sequence sharding and parallel folding for optimal resource utilization\n- **Distributed VAE**: Parallel encoding/decoding to eliminate memory bottlenecks for high-resolution video\n- **Production-Ready Serving**: Fixed Cache-DiT integration bugs for stable multi-request serving\n- **Optimized I/O**: Accelerated video save operations by eliminating unnecessary serialization\n- **Fused Kernels**: Custom JIT kernels for LayerNorm variants, reducing GPU bubbles\n\nLet's dive into the technical details.\n\n## Key Improvements\n\n### 1. SP-Sharding Improvement: From Frame-Level to Token-Level\n\nFor Video DiT models, input tensors typically have shape `B, T, H, W, C`. For a common configuration with\n`num_frames=81`, this might be: `1, 21, 90, 160, 3`.\n\nIn an 8×H100 setup with Ulysses Sequence Parallel (N=8), the framework needs to shard along the sequence dimension\nduring non-attention operations, then use all-to-all communication to switch to head dimension sharding for attention.\n\n#### Previous Approach: Frame-Level Sharding\n\nOur initial implementation sharded directly along the `T` (temporal) dimension. However, 21 frames cannot be evenly\ndivided by 8 GPUs, leading to two suboptimal solutions:\n\n1. **Adjust-frame**: Modify `num_frames` during preprocessing to make T divisible by N\n2. **Token Padding**: Pad the temporal dimension to the next multiple of N (21 → 24)\n\nThe frame-level padding approach introduces significant overhead: each padded token requires `H × W × C` redundant\ncomputations.\n\n#### New Approach: Token-Level Sharding\n\nTo minimize padding overhead, we now **flatten `T × H × W` into a single sequence dimension** before sharding. This has\ntwo major benefits:\n\n- **Reduced or Zero Padding**: For common resolutions and VAE configurations, `H × W` is often divisible by 8,\n  eliminating padding entirely\n- **Lower Communication Volume**: When padding is needed, the overhead is minimal compared to frame-level padding\n\n### Comparison: Shape and Comm Volume Analysis\n\n| Solution           | Padding Overhead | Input Tensor Shape (Per-rank) | All-to-All Comm Volume | \n|--------------------|------------------|-------------------------------|------------------------|\n| **Frame Sharding** | 3 frames (14.3%) | `3, 90, 160, C` (24/8)        | `1.0 × feature_map`    |\n| **Token Sharding** | 0 frames         | `2.625, 90, 160, C` (21/8)    | `0.875 × feature_map`  |\n\nThis optimization delivers both faster communication and reduced memory footprint, especially for video models.\n\nSee related [PR](https://github.com/sgl-project/sglang/pull/18161) for technical details.\n\n### 2. Parallel Folding: Decoupling Text Encoder and DiT Parallelism\n\nIn our original implementation, the Text Encoder and DiT shared the same Tensor Parallel (TP) group. When DiT used only\nSequence Parallel (SP), this meant the Text Encoder ran with TP=1—each GPU held a complete model copy, wasting memory\nand compute.\n\nSince Text Encoder and DiT computations are **completely decoupled**, we introduced **Parallel Folding**: the Text\nEncoder now uses the DiT's SP group as its TP group.\n\n**What this means in practice:**\n\n- **For Text Encoder**: Apply TP across the SP group to maximize speed and reduce memory\n- **For Denoiser**: Apply SP to optimize throughput and memory for sequence processing\n\nThis approach ensures both components use optimal parallelism strategies without interference, improving overall\nefficiency.\n\nSee related [PR](https://github.com/sgl-project/sglang/pull/17818) for technical details.\n\n### 3. Parallel VAE: Distributed Encoding/Decoding\n\nVAE encoding/decoding involves heavy 3D convolution operations. For high-resolution video, single-GPU implementations\nare slow and prone to OOM.\n\nThe two common approaches to alleviate this are:\n\n1. **Tiling**: Split feature maps into tiles, process them sequentially—reduces peak memory but increases latency\n2. **Parallel**: Distribute tiles across GPUs for concurrent processing—reduces both peak memory and latency\n\nWe implemented **Parallel VAE** for Wan-VAE with the following strategy:\n\n- **Height-wise Sharding**: Split feature maps along the height dimension across ranks\n- **Conv Operations**: Use `halo_exchange` to share boundary pixels between neighboring ranks (P2P), ensuring\n  mathematical\n  equivalence with global convolution\n- **Attention Operations**: Use `all_gather` for global context when needed\n- **Result Aggregation**: `all_gather` to reconstruct full height at the end of encoding/decoding\n\nThis approach eliminates VAE as a bottleneck for high-resolution video generation, enabling higher resolutions and\nlonger sequences without OOM.\n\n### 4. Serving with Cache-DiT: Fixing Multi-Request Stability\n\n[Cache-DiT](https://github.com/vipshop/cache-dit) in SGLang-Diffusion accelerates inference by caching residuals and\nskipping redundant\ncomputations. However, its correct operation depends on proper `num_inference_steps` configuration, which determines\nstep counting and the Selective Computation Mask (SCM).\n\n**The Problem:**\n\nWan2.2 uses a dual-transformer architecture, where `transformer` and `transformer_2` execute `num_high_noise_steps` and\n`num_low_noise_steps` respectively (summing to `num_inference_steps`). Our initial implementation had two critical bugs:\n\n1. Both transformers incorrectly used total `num_inference_steps` to configure their cache contexts\n2. In serving mode, cache contexts persisted across requests, even when different requests used different\n   `num_inference_steps`\n\nThese issues caused incorrect step counting and cache buffer contamination. When consecutive requests had different\nvideo shapes, cache buffers would encounter shape mismatches, **crashing the server**.\n\n**Our Solution:**\n\n1. `transformer` and `transformer_2` now use `num_high_noise_steps` and `num_low_noise_steps` respectively to configure\n   independent cache contexts\n2. For each new request, we recalculate timestep splits and **refresh** cache contexts using Cache-DiT's API, completely\n   isolating requests\n\nThis ensures stable, production-ready serving with Cache-DiT acceleration.\n\n### 5. Optimize Video Save: Eliminating Serialization Overhead\n\nIn our serving architecture, `scheduler_client` and `gpu_worker` communicate via ZMQ.\n\nPreviously, `gpu_worker` would:\n\n1. Complete inference\n2. Serialize output tensor\n3. Send tensor to `scheduler_client` via ZMQ\n4. `scheduler_client` deserializes tensor\n5. `scheduler_client` processes tensor and saves video\n\nThis introduced significant overhead from serialization/deserialization and memory copies.\n\n**Our Solution:**\n\n`gpu_worker` now directly processes the output tensor and saves the video to disk, returning only the file path to\n`scheduler_client`.\n\nThis eliminates serialization/deserialization overhead, while avoiding duplicate tensor copies.\n\n### 6. WanVideo LayerNorm Fusion: CuTeDSL JIT Kernels\n\nWanVideo introduces two specialized LayerNorm patterns:\n\n1. **LayerNormScaleShift**: `y = LN(x) * (1 + scale) + shift`\n\n2. **ScaleResidualLayerNormScaleShift**:\n    - `residual_out = residual + gate * x`\n    - `y = LN(residual_out) * (1 + scale) + shift`\n\nThese patterns combine elementwise operations with normalization reductions. Implementing them as separate kernels would\nintroduce multiple kernel launches and intermediate memory traffic, creating GPU bubbles.\n\n**Our Solution:**\n\nWe implemented **fused JIT kernels** using CuTeDSL (located in [`sglang/jit_kernel/diffusion/cutedsl`](https://github.com/sgl-project/sglang/tree/main/python/sglang/jit_kernel/diffusion/cutedsl)) that combine\nthese operations into single, efficient kernels.\n\n**Benefits:**\n\n- **Fewer Kernel Launches**: Reduced launch overhead\n- **Lower Memory Traffic**: Eliminates intermediate reads/writes\n- **Better GPU Utilization**: Reduces bubbles and improves throughput\n\nThese micro-optimizations add up, especially for multi-layer architectures like WanVideo.\n\n## Performance Results\n\nHere's a comparison of SGLang-Diffusion and LightX2V for Wan2.2 T2V under different settings:\n\n\u003ciframe style=\"display:block; margin: auto;\" width=\"838\" height=\"523\" seamless frameborder=\"0\" scrolling=\"no\" src=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQRK_j_q8NXZKEqtrTBagxFxvvaxYXXB56HTqqYlD_aAv1v74WKle2HIc7HPK3P0ZVrYlZrjshKYnaV/pubchart?oid=677973346\u0026amp;format=interactive\"\u003e\u003c/iframe\u003e\n\n## What's Next\n\nWe continue to push the boundaries of diffusion model serving. Please refer to [**SGLang-Diffusion's Roadmap for 26Q1**](https://github.com/sgl-project/sglang/issues/18286) for more details.\n\nStay tuned for more updates as we continue to optimize SGLang-Diffusion for production deployments.\n\n## Acknowledgment\n\n- We would like to thank the following contributors for their work on these optimizations:\n  **Skywork.ai, [Song Rui](https://github.com/Songrui625), SGLang-Diffusion Team**\n- Special thanks to our compute partners for their continued support.\n\nTry diffusion generation, proudly powered by SGLang-Diffusion: [APIFree](https://www.apifree.ai/home)\n\n## Learn More\n\n- **Slack channel**: [#diffusion](https://sgl-fru7574.slack.com/archives/C09P0HTKE6A) (join via slack.sglang.io)\n- [**Cookbook for SGLang-Diffusion**](https://cookbook.sglang.io/docs/diffusion)\n- [**Documentation on SGLang-Diffusion**](https://github.com/sgl-project/sglang/blob/main/python/sglang/multimodal_gen/docs)\n- [**Previous Update: Two Months In**](https://lmsys.org/blog/2026-01-16-sglang-diffusion/)\n","slug":"2026-02-16-sglang-diffusion-advanced-optimizations"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2026-02-16-sglang-diffusion-advanced-optimizations"},"buildId":"QRPdtedtwlzVI-v5jUlfR","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>