<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>NVIDIA DGX Spark In-Depth Review: A New Standard for Local AI Inference | LMSYS Org</title><meta name="title" content="NVIDIA DGX Spark In-Depth Review: A New Standard for Local AI Inference | LMSYS Org"/><meta property="og:title" content="NVIDIA DGX Spark In-Depth Review: A New Standard for Local AI Inference | LMSYS Org"/><meta name="twitter:title" content="NVIDIA DGX Spark In-Depth Review: A New Standard for Local AI Inference | LMSYS Org"/><meta name="description" content="&lt;p&gt;Thanks to NVIDIA’s early access program, we are thrilled to get our hands on the NVIDIA DGX™ Spark. It’s quite an unconventional system, as NVIDIA rarely ..."/><meta property="og:description" content="&lt;p&gt;Thanks to NVIDIA’s early access program, we are thrilled to get our hands on the NVIDIA DGX™ Spark. It’s quite an unconventional system, as NVIDIA rarely ..."/><meta name="twitter:description" content="&lt;p&gt;Thanks to NVIDIA’s early access program, we are thrilled to get our hands on the NVIDIA DGX™ Spark. It’s quite an unconventional system, as NVIDIA rarely ..."/><meta property="og:image" content="https://lmsys.org/images/blog/nvidia_dgx_spark/product_1.jpg"/><meta name="twitter:image" content="https://lmsys.org/images/blog/nvidia_dgx_spark/product_1.jpg"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2025-10-13-nvidia-dgx-spark"/><meta name="twitter:url" content="https://lmsys.org/blog/2025-10-13-nvidia-dgx-spark"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-0bb93d4b49319e30.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/uLkn4zkcA58kiW8X6wEgX/_buildManifest.js" defer=""></script><script src="/_next/static/uLkn4zkcA58kiW8X6wEgX/_ssgManifest.js" defer=""></script><script src="/_next/static/uLkn4zkcA58kiW8X6wEgX/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">NVIDIA DGX Spark In-Depth Review: A New Standard for Local AI Inference</h1><p class="text-xl pt-2 pb-2">by: <!-- -->Jerry Zhou and Richard Chen<!-- -->,<!-- --> <!-- -->Oct 13, 2025<!-- --></p><hr/><div class="pt-2 article"><p>Thanks to NVIDIA’s early access program, we are thrilled to get our hands on the NVIDIA DGX™ Spark. It’s quite an unconventional system, as NVIDIA rarely releases compact, all-in-one machines that bring supercomputing-class performance to a desktop workstation form factor.</p>
<p>Over the past year, SGLang has been rapidly expanding its developer base in the datacenter segment, recognized by the inference community for its great performance. Successfully deploying DeepSeek with Prefill-decode Disaggregation (PD) and Expert Parallelism (EP) at large scale, running on both <a href="https://lmsys.org/blog/2025-05-05-large-scale-ep/" target="_blank"><strong>96 NVIDIA H100 GPU clusters</strong></a> and the latest <a href="https://lmsys.org/blog/2025-09-25-gb200-part-2/" target="_blank"><strong>GB200 NVL72 systems</strong></a>, SGLang has continually pushed the boundaries of large-scale inference performance and developer productivity.</p>
<p>Inspired by the capabilities of the DGX Spark, for the first time, SGLang is now expanding beyond the datacenter and into the consumer market, bringing its proven inference framework directly to developers and researchers everywhere. In this review, we’ll be taking a close look at this beautiful machine, from its exterior aesthetics to its performance and use cases.</p>
<blockquote>
<p>Also check out our video review <a href="https://youtu.be/-3r2woTQjec" target="_blank">here</a>.</p>
</blockquote>
<p><img src="/images/blog/nvidia_dgx_spark/product_1.jpg" alt=""></p>
<h2><a id="exterior" class="anchor" href="#exterior" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Exterior</h2>
<p>The DGX Spark is a gorgeous piece of engineering. It features a full-metal chassis with a sleek champagne-gold finish. Both the front and rear panels are built with metal foam, reminding me of the design of NVIDIA DGX A100 and H100.</p>
<p>Around the back, the DGX Spark offers an impressive array of connectivity options: a power button, four USB-C ports (with the leftmost supporting up to <strong>240 W of power delivery</strong>), an HDMI port, a <strong>10 GbE RJ-45 Ethernet port</strong>, and <strong>two QSFP ports driven by NVIDIA ConnectX-7 NIC capable of up to 200 Gbps</strong>. These interfaces allow two DGX Spark units to be connected together, allowing them to run even larger AI models.</p>
<p>The use of USB Type-C for power delivery is a particularly interesting design choice, one that’s virtually unheard of on other desktop machines. Comparable systems like the Mac Mini or Mac Studio rely on the standard C5/C7 power connector, which is far more secure but also bulkier. NVIDIA likely opted for USB-C to keep the power supply external, freeing up valuable internal space for the cooling system. The trade-off, however, is that you’ll want to be extra careful not to accidentally tug the cable loose.</p>
<p><img src="/images/blog/nvidia_dgx_spark/product_2.jpg" alt=""></p>
<h2><a id="hardware-capabilities" class="anchor" href="#hardware-capabilities" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hardware Capabilities</h2>
<p>On the hardware side, the DGX Spark packs remarkable performance for its size and power envelope. At its core is the NVIDIA GB10 Grace Blackwell Superchip, designed specifically for this device. It integrates 10 Cortex-X925 performance cores and 10 Cortex-A725 efficiency cores, for a total of 20 CPU cores.</p>
<p>On the GPU side, the GB10 delivers up to <strong>1 PFLOP of sparse FP4 tensor performance</strong>, placing its AI capability roughly between that of an RTX 5070 and 5070 Ti. The standout feature is its <strong>128 GB of coherent unified system memory</strong>, shared seamlessly between the CPU and GPU. This unified architecture allows the DGX Spark to load and run large models directly without the overhead of system-to-VRAM data transfers. With the help of its dual QSFP Ethernet ports with an aggregate bandwidth of 200 Gb/s, two DGX Spark units can be connected together to operate as a small cluster, enabling distributed inference of even larger models. According to NVIDIA, two interconnected DGX Sparks can handle models with up to <strong>405 billion parameters in FP4</strong>.</p>
<p>However, the only downside of this machine lies in memory bandwidth, the unified memory is LPDDR5x, offering up to <strong>273 GB/s</strong>, shared across both CPU and GPU. As we’ll see later, this limited bandwidth is expected (and empirically shown) to be the key bottleneck in AI inference performance. Nonetheless, the 128GB of memory enables DGX Spark to run models that are too large for most desktop systems.</p>
<p><img src="/images/blog/nvidia_dgx_spark/product_3.jpg" alt=""></p>
<h2><a id="performance" class="anchor" href="#performance" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Performance</h2>
<p>We benchmarked several open-weight large language models on the DGX Spark using both <strong>SGLang</strong> and <strong>Ollama</strong>. Our findings show that while the DGX Spark can indeed load and run very large models, such as <strong>GPT-OSS 120B</strong> and <strong>Llama 3.1 70B,</strong> these workloads are best suited for <strong>prototyping and experimentation</strong> rather than production. The DGX Spark truly shines when serving <strong>smaller models</strong>, especially when <strong>batching</strong> is utilized to maximize throughput.</p>
<h3><a id="methodology" class="anchor" href="#methodology" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Methodology</h3>
<blockquote>
<p>⚠️ <strong>Note:</strong> Since software support for the DGX Spark is still in its early stages, the benchmark results presented in this section may become outdated as future software updates improve performance and compatibility.</p>
</blockquote>
<h4><a id="test-devices" class="anchor" href="#test-devices" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Test Devices</h4>
<p>We prepared the following systems for benchmarking:</p>
<ul>
<li><strong>NVIDIA DGX Spark</strong></li>
<li><strong>NVIDIA RTX PRO™ 6000 Blackwell Workstation Edition</strong></li>
<li><strong>NVIDIA GeForce RTX 5090 Founders Edition</strong></li>
<li><strong>NVIDIA GeForce RTX 5080 Founders Edition</strong></li>
<li><strong>Apple Mac Studio (M1 Max, 64 GB unified memory)</strong></li>
<li><strong>Apple Mac Mini (M4 Pro, 24 GB unified memory)</strong></li>
</ul>
<h4><a id="benchmark-models" class="anchor" href="#benchmark-models" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Benchmark Models</h4>
<p>We evaluated a variety of open-weight large language models using two frameworks, <strong>SGLang</strong> and <strong>Ollama</strong>, as summarized below:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Framework</th>
<th style="text-align:left">Batch Size</th>
<th style="text-align:left">Models &amp; Quantization</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>SGLang</strong></td>
<td style="text-align:left">1–32</td>
<td style="text-align:left">Llama 3.1 8B (FP8)<br>Llama 3.1 70B (FP8)<br>Gemma 3 12B (FP8)<br>Gemma 3 27B (FP8)<br>DeepSeek-R1 14B (FP8)<br>Qwen 3 32B (FP8)</td>
</tr>
<tr>
<td style="text-align:left"><strong>Ollama</strong></td>
<td style="text-align:left">1</td>
<td style="text-align:left">GPT-OSS 20B (MXFP4)<br>GPT-OSS 120B (MXFP4)<br>Llama 3.1 8B (q4_K_M / q8_0)<br>Llama 3.1 70B (q4_K_M)<br>Gemma 3 12B (q4_K_M / q8_0)<br>Gemma 3 27B (q4_K_M / q8_0)<br>DeepSeek-R1 14B (q4_K_M / q8_0)<br>Qwen 3 32B (q4_K_M / q8_0)</td>
</tr>
</tbody>
</table>
<p>We also tested <strong>speculative decoding (EAGLE3) with SGLang</strong> on some of the models listed above. We excluded models that exceeded the available RAM or VRAM capacity of the target machine.</p>
<h3><a id="results" class="anchor" href="#results" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Results</h3>
<blockquote>
<p>Full benchmark results can be found <a href="https://docs.google.com/spreadsheets/d/1SF1u0J2vJ-ou-R_Ry1JZQ0iscOZL8UKHpdVFr85tNLU/edit?usp=sharing" target="_blank">here</a>.</p>
</blockquote>
<h4><a id="overall-performance" class="anchor" href="#overall-performance" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Overall Performance</h4>
<p>While the DGX Spark demonstrates impressive engineering for its size and power envelope, its raw performance is understandably limited compared to full-sized discrete GPU systems.</p>
<p>For example, running <strong>GPT-OSS 20B (MXFP4)</strong> in <strong>Ollama</strong>, the Spark achieved <strong>2,053 tps prefill / 49.7 tps decode</strong>, whereas the <strong>RTX Pro 6000 Blackwell</strong> reached <strong>10,108 tps / 215 tps,</strong> roughly <strong>4× faster</strong>. Even the <strong>GeForce RTX 5090</strong> delivered <strong>8,519 tps / 205 tps</strong>, confirming that the Spark’s unified LPDDR5x memory bandwidth is the main limiting factor.</p>
<p>However, for smaller models, particularly <strong>Llama 3.1 8B</strong>, the DGX Spark held its own. With <strong>SGLang</strong> at batch 1, it achieved <strong>7,991 tps prefill / 20.5 tps decode</strong>, scaling up linearly to <strong>7,949 tps / 368 tps</strong> at batch 32, demonstrating excellent batching efficiency and strong throughput consistency across runs.</p>
<h4><a id="strength-in-compact-unified-memory-workloads" class="anchor" href="#strength-in-compact-unified-memory-workloads" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Strength in Compact, Unified-Memory Workloads</h4>
<p>One of the DGX Spark’s defining strengths lies in its <strong>128 GB of coherent unified memory</strong>, which allows both CPU and GPU to access the same address space.</p>
<p>This enables large models, such as <strong>Llama 3.1 70B</strong>, <strong>Gemma 3 27B</strong>, or even <strong>GPT-OSS 120B,</strong> to load <strong>directly into memory</strong> without the traditional system-to-VRAM transfer overhead. Despite its compact form factor, the Spark successfully ran <strong>Llama 3.1 70B (FP8)</strong> at <strong>803 tps prefill / 2.7 tps decode</strong>, which is remarkable for a workstation that sits quietly on a desk.</p>
<p>This unified-memory design makes DGX Spark particularly valuable for <strong>prototyping</strong>, <strong>model experimentation</strong>, and <strong>edge-AI research</strong>, where seamless memory access is often more useful than raw TFLOPs.</p>
<h4><a id="speculative-decoding-acceleration" class="anchor" href="#speculative-decoding-acceleration" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Speculative Decoding Acceleration</h4>
<p>To further explore performance optimization on the DGX Spark, we enabled <strong>speculative decoding</strong> using <strong>EAGLE 3</strong> within <strong>SGLang</strong>. This technique allows a smaller “draft” model to propose multiple tokens ahead, while the larger target model verifies them in parallel.</p>
<p>With speculative decoding enabled, we observed up to a <strong>2× speed-up</strong> in end-to-end inference throughput compared to standard decoding across multiple models, such as <strong>Llama 3.1 8B</strong>.</p>
<p>This improvement effectively mitigates part of the unified-memory bandwidth limitation and demonstrates that <strong>software-level innovations</strong> such as speculative decoding can meaningfully enhance inference performance on compact, bandwidth-constrained systems like the DGX Spark.</p>
<h4><a id="efficiency-and-thermal-design" class="anchor" href="#efficiency-and-thermal-design" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Efficiency and Thermal Design</h4>
<p>The DGX Spark maintains sustained throughput across high-intensity tests without thermal throttling. Even under full load, e.g., <strong>SGLang DeepSeek-R1 14B (FP8)</strong> at batch 8 achieving <strong>2,074 tps / 83.5 tps</strong>, fan noise and temperature remained stable, highlighting NVIDIA’s excellent <strong>metal-foam cooling design</strong> and well-optimized <strong>power delivery system</strong>.</p>
<p>Its <strong>USB-C power input</strong> (up to 240 W) and external PSU allow for greater thermal headroom inside the chassis, a clear advantage for long-running workloads compared to compact consumer systems like the Mac Mini or Mac Studio, which showed thermal drop-off in similar tests.</p>
<h4><a id="summary" class="anchor" href="#summary" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Summary</h4>
<p>In short, the DGX Spark is <strong>not built to compete head-to-head</strong> with full-sized Blackwell or Ada-Lovelace GPUs, but rather to bring the DGX experience into a compact, developer-friendly form factor.<br>
It’s an ideal platform for:</p>
<ul>
<li><strong>Model prototyping and experimentation</strong></li>
<li><strong>Lightweight on-device inference</strong></li>
<li><strong>Research on memory-coherent GPU architectures</strong></li>
</ul>
<p>It’s a <strong>gorgeous, well-engineered mini supercomputer</strong> that trades raw power for accessibility, efficiency, and elegance, and in those areas, it absolutely shines.</p>
<p><img src="/images/blog/nvidia_dgx_spark/product_4.jpg" alt=""></p>
<h2><a id="use-cases" class="anchor" href="#use-cases" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Use Cases</h2>
<h3><a id="sglang-model-serving" class="anchor" href="#sglang-model-serving" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>SGLang Model Serving</h3>
<p>The DGX Spark comes with Docker preinstalled, allowing you to serve open-weight models via SGLang with just a single command:</p>
<pre><code class="hljs language-bash">docker run --gpus all \
    --shm-size 32g \
    -p 30000:30000 \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    --<span class="hljs-built_in">env</span> <span class="hljs-string">&quot;HF_TOKEN=&lt;secret&gt;&quot;</span> \
    --ipc=host \
    lmsysorg/sglang:spark \
    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --quantization fp8 --host 0.0.0.0 --port 30000
</code></pre>
<p>Replace <code>&lt;secret&gt;</code> with your own Hugging Face access token.</p>
<h4><a id="enabling-speculative-decoding-eagle3" class="anchor" href="#enabling-speculative-decoding-eagle3" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Enabling Speculative Decoding (EAGLE3)</h4>
<p>To enable <strong>speculative decoding</strong> using <strong>EAGLE3</strong>, simply run the following command:</p>
<pre><code class="hljs language-bash">docker run --gpus all \
    --shm-size 32g \
    -p 30000:30000 \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    --<span class="hljs-built_in">env</span> <span class="hljs-string">&quot;HF_TOKEN=&lt;secret&gt;&quot;</span> \
    --<span class="hljs-built_in">env</span> <span class="hljs-string">&quot;SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1&quot;</span> \
    --ipc=host \
    lmsysorg/sglang:spark \
    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --quantization fp8 --host 0.0.0.0 --port 30000 \
    --speculative-algorithm EAGLE3 \
    --speculative-draft-model-path jamesliu1/sglang-EAGLE3-Llama-3.1-Instruct-8B \
    --speculative-num-steps 5 \
    --speculative-eagle-topk 8 \
    --speculative-num-draft-tokens 32 \
    --mem-fraction 0.6 \
    --cuda-graph-max-bs 2 \
    --dtype float16
</code></pre>
<p>With speculative decoding enabled, SGLang can leverage a smaller draft model to predict multiple tokens ahead, effectively <strong>doubling inference throughput</strong> compared to standard decoding.</p>
<h4><a id="sending-requests-via-the-openai-compatible-api" class="anchor" href="#sending-requests-via-the-openai-compatible-api" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Sending Requests via the OpenAI-Compatible API</h4>
<p>Once SGLang successfully initializes, you can interact with your model through OpenAI-compatible API endpoints:</p>
<pre><code class="hljs language-bash">curl http://localhost:30000/v1/chat/completions \
    -H <span class="hljs-string">&quot;Content-Type: application/json&quot;</span> \
    -d <span class="hljs-string">&#x27;{
        &quot;messages&quot;: [
            {
                &quot;role&quot;: &quot;system&quot;,
                &quot;content&quot;: &quot;You are a helpful assistant.&quot;
            },
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: &quot;How many letters are there in the word SGLang?&quot;
            }
        ]
    }&#x27;</span>
</code></pre>
<p><img src="/images/blog/nvidia_dgx_spark/demo_1.jpg" alt=""></p>
<h3><a id="chatting-with-local-model" class="anchor" href="#chatting-with-local-model" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Chatting with Local Model</h3>
<p>Once you have <strong>SGLang</strong> set up and serving a model, you can easily connect it to <strong>Open WebUI</strong> to chat with any open-weight model you like. Open WebUI provides a sleek, browser-based interface that’s fully compatible with OpenAI-style APIs, meaning it works seamlessly with your local SGLang server. With just a quick configuration pointing to your DGX Spark’s endpoint, you can interact with models such as <strong>Llama 3</strong>, <strong>Gemma 3</strong>, or <strong>DeepSeek-R1</strong> directly from your browser, no cloud dependencies, no latency, and complete control over your data.</p>
<p><img src="/images/blog/nvidia_dgx_spark/demo_2.jpg" alt=""></p>
<h3><a id="coding-with-local-model" class="anchor" href="#coding-with-local-model" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Coding with Local Model</h3>
<p>One of the most practical ways to utilize the DGX Spark is as a <strong>local coding assistant,</strong> completely offline and secure.</p>
<p>By combining <strong>Zed</strong>, a modern AI-integrated code editor, with <strong>Ollama</strong>, you can run <strong>GPT-OSS 20B</strong> locally to power code completion, inline chat, and smart refactoring without relying on the cloud.</p>
<h4><a id="step-1-install-ollama" class="anchor" href="#step-1-install-ollama" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 1. Install Ollama</h4>
<pre><code class="hljs language-bash">curl -fsSL https://ollama.com/install.sh | sh
</code></pre>
<h4><a id="step-2-pull-gpt-oss-20b-for-coding" class="anchor" href="#step-2-pull-gpt-oss-20b-for-coding" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 2. Pull GPT-OSS 20B for Coding</h4>
<pre><code class="hljs language-bash">ollama pull gpt-oss:20b
</code></pre>
<h4><a id="step-3-integrate-zed-with-ollama" class="anchor" href="#step-3-integrate-zed-with-ollama" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 3. Integrate Zed with Ollama</h4>
<p>Install Zed:</p>
<pre><code class="hljs language-bash">curl -f https://zed.dev/install.sh | sh
</code></pre>
<p>Zed automatically detects local models served by Ollama, allowing you to start using the built-in chat assistant immediately after launching the editor.</p>
<p><img src="/images/blog/nvidia_dgx_spark/demo_3.jpg" alt=""></p>
<h2><a id="conclusion" class="anchor" href="#conclusion" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>The <strong>NVIDIA DGX Spark</strong> is a fascinating glimpse into the future of personal AI computing. It takes what was once reserved for data centers: large memory, high-bandwidth Ethernet interconnects, and Blackwell-class performance, and distills it into a compact, beautifully engineered desktop form factor. While it doesn’t rival full-size DGX servers or discrete RTX GPUs in raw throughput, it shines in accessibility, efficiency, and versatility.</p>
<p>From running <strong>SGLang</strong> and <strong>Ollama</strong> for local model serving, to experimenting with <strong>speculative decoding (EAGLE3)</strong>, to exploring distributed inference through <strong>dual-Spark clustering</strong>, the platform proves itself as more than just a miniature supercomputer. It’s a developer’s sandbox for the next era of AI.</p>
<p>The NVIDIA DGX Spark isn’t built to replace cloud-scale infrastructure; it’s built to <strong>bring AI experimentation to your desk</strong>. Whether you’re benchmarking open-weight LLMs, developing inference frameworks, or building your own private coding assistant, the Spark empowers you to do it all locally, quietly, elegantly, and with NVIDIA’s unmistakable engineering polish.</p>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"NVIDIA DGX Spark In-Depth Review: A New Standard for Local AI Inference","author":"Jerry Zhou and Richard Chen","date":"October 13, 2025","previewImg":"/images/blog/nvidia_dgx_spark/product_1.jpg"},"content":"\nThanks to NVIDIA’s early access program, we are thrilled to get our hands on the NVIDIA DGX™ Spark. It’s quite an unconventional system, as NVIDIA rarely releases compact, all-in-one machines that bring supercomputing-class performance to a desktop workstation form factor.\n\nOver the past year, SGLang has been rapidly expanding its developer base in the datacenter segment, recognized by the inference community for its great performance. Successfully deploying DeepSeek with Prefill-decode Disaggregation (PD) and Expert Parallelism (EP) at large scale, running on both \u003ca href=\"https://lmsys.org/blog/2025-05-05-large-scale-ep/\" target=\"_blank\"\u003e**96 NVIDIA H100 GPU clusters**\u003c/a\u003e and the latest \u003ca href=\"https://lmsys.org/blog/2025-09-25-gb200-part-2/\" target=\"_blank\"\u003e**GB200 NVL72 systems**\u003c/a\u003e, SGLang has continually pushed the boundaries of large-scale inference performance and developer productivity.\n\nInspired by the capabilities of the DGX Spark, for the first time, SGLang is now expanding beyond the datacenter and into the consumer market, bringing its proven inference framework directly to developers and researchers everywhere. In this review, we’ll be taking a close look at this beautiful machine, from its exterior aesthetics to its performance and use cases.\n\n\u003e Also check out our video review \u003ca href=\"https://youtu.be/-3r2woTQjec\" target=\"_blank\"\u003ehere\u003c/a\u003e.\n\n![](/images/blog/nvidia_dgx_spark/product_1.jpg)\n\n## Exterior\n\nThe DGX Spark is a gorgeous piece of engineering. It features a full-metal chassis with a sleek champagne-gold finish. Both the front and rear panels are built with metal foam, reminding me of the design of NVIDIA DGX A100 and H100.\n\nAround the back, the DGX Spark offers an impressive array of connectivity options: a power button, four USB-C ports (with the leftmost supporting up to **240 W of power delivery**), an HDMI port, a **10 GbE RJ-45 Ethernet port**, and **two QSFP ports driven by NVIDIA ConnectX-7 NIC capable of up to 200 Gbps**. These interfaces allow two DGX Spark units to be connected together, allowing them to run even larger AI models.\n\nThe use of USB Type-C for power delivery is a particularly interesting design choice, one that’s virtually unheard of on other desktop machines. Comparable systems like the Mac Mini or Mac Studio rely on the standard C5/C7 power connector, which is far more secure but also bulkier. NVIDIA likely opted for USB-C to keep the power supply external, freeing up valuable internal space for the cooling system. The trade-off, however, is that you’ll want to be extra careful not to accidentally tug the cable loose.\n\n![](/images/blog/nvidia_dgx_spark/product_2.jpg)\n\n## Hardware Capabilities\n\nOn the hardware side, the DGX Spark packs remarkable performance for its size and power envelope. At its core is the NVIDIA GB10 Grace Blackwell Superchip, designed specifically for this device. It integrates 10 Cortex-X925 performance cores and 10 Cortex-A725 efficiency cores, for a total of 20 CPU cores.\n\nOn the GPU side, the GB10 delivers up to **1 PFLOP of sparse FP4 tensor performance**, placing its AI capability roughly between that of an RTX 5070 and 5070 Ti. The standout feature is its **128 GB of coherent unified system memory**, shared seamlessly between the CPU and GPU. This unified architecture allows the DGX Spark to load and run large models directly without the overhead of system-to-VRAM data transfers. With the help of its dual QSFP Ethernet ports with an aggregate bandwidth of 200 Gb/s, two DGX Spark units can be connected together to operate as a small cluster, enabling distributed inference of even larger models. According to NVIDIA, two interconnected DGX Sparks can handle models with up to **405 billion parameters in FP4**.\n\nHowever, the only downside of this machine lies in memory bandwidth, the unified memory is LPDDR5x, offering up to **273 GB/s**, shared across both CPU and GPU. As we’ll see later, this limited bandwidth is expected (and empirically shown) to be the key bottleneck in AI inference performance. Nonetheless, the 128GB of memory enables DGX Spark to run models that are too large for most desktop systems.\n\n![](/images/blog/nvidia_dgx_spark/product_3.jpg)\n\n## Performance\n\nWe benchmarked several open-weight large language models on the DGX Spark using both **SGLang** and **Ollama**. Our findings show that while the DGX Spark can indeed load and run very large models, such as **GPT-OSS 120B** and **Llama 3.1 70B,** these workloads are best suited for **prototyping and experimentation** rather than production. The DGX Spark truly shines when serving **smaller models**, especially when **batching** is utilized to maximize throughput.\n\n### Methodology\n\n\u003e ⚠️ **Note:** Since software support for the DGX Spark is still in its early stages, the benchmark results presented in this section may become outdated as future software updates improve performance and compatibility.\n\n#### Test Devices\n\nWe prepared the following systems for benchmarking:\n\n* **NVIDIA DGX Spark**  \n* **NVIDIA RTX PRO™ 6000 Blackwell Workstation Edition**  \n* **NVIDIA GeForce RTX 5090 Founders Edition**  \n* **NVIDIA GeForce RTX 5080 Founders Edition**  \n* **Apple Mac Studio (M1 Max, 64 GB unified memory)**  \n* **Apple Mac Mini (M4 Pro, 24 GB unified memory)**\n\n#### Benchmark Models\n\nWe evaluated a variety of open-weight large language models using two frameworks, **SGLang** and **Ollama**, as summarized below:\n\n| Framework | Batch Size | Models \u0026 Quantization |\n| :---- | :---- | :---- |\n| **SGLang** | 1–32 | Llama 3.1 8B (FP8)\u003cbr\u003eLlama 3.1 70B (FP8)\u003cbr\u003eGemma 3 12B (FP8)\u003cbr\u003eGemma 3 27B (FP8)\u003cbr\u003eDeepSeek-R1 14B (FP8)\u003cbr\u003eQwen 3 32B (FP8) |\n| **Ollama** | 1 | GPT-OSS 20B (MXFP4)\u003cbr\u003eGPT-OSS 120B (MXFP4)\u003cbr\u003eLlama 3.1 8B (q4\\_K\\_M / q8\\_0)\u003cbr\u003eLlama 3.1 70B (q4\\_K\\_M)\u003cbr\u003eGemma 3 12B (q4\\_K\\_M / q8\\_0)\u003cbr\u003eGemma 3 27B (q4\\_K\\_M / q8\\_0)\u003cbr\u003eDeepSeek-R1 14B (q4\\_K\\_M / q8\\_0)\u003cbr\u003eQwen 3 32B (q4\\_K\\_M / q8\\_0) |\n\nWe also tested **speculative decoding (EAGLE3) with SGLang** on some of the models listed above. We excluded models that exceeded the available RAM or VRAM capacity of the target machine.\n\n### Results\n\n\u003e Full benchmark results can be found \u003ca href=\"https://docs.google.com/spreadsheets/d/1SF1u0J2vJ-ou-R_Ry1JZQ0iscOZL8UKHpdVFr85tNLU/edit?usp=sharing\" target=\"_blank\"\u003ehere\u003c/a\u003e.\n\n#### Overall Performance\n\nWhile the DGX Spark demonstrates impressive engineering for its size and power envelope, its raw performance is understandably limited compared to full-sized discrete GPU systems.\n\nFor example, running **GPT-OSS 20B (MXFP4)** in **Ollama**, the Spark achieved **2,053 tps prefill / 49.7 tps decode**, whereas the **RTX Pro 6000 Blackwell** reached **10,108 tps / 215 tps,** roughly **4× faster**. Even the **GeForce RTX 5090** delivered **8,519 tps / 205 tps**, confirming that the Spark’s unified LPDDR5x memory bandwidth is the main limiting factor.\n\nHowever, for smaller models, particularly **Llama 3.1 8B**, the DGX Spark held its own. With **SGLang** at batch 1, it achieved **7,991 tps prefill / 20.5 tps decode**, scaling up linearly to **7,949 tps / 368 tps** at batch 32, demonstrating excellent batching efficiency and strong throughput consistency across runs.\n\n#### Strength in Compact, Unified-Memory Workloads\n\nOne of the DGX Spark’s defining strengths lies in its **128 GB of coherent unified memory**, which allows both CPU and GPU to access the same address space.\n\nThis enables large models, such as **Llama 3.1 70B**, **Gemma 3 27B**, or even **GPT-OSS 120B,** to load **directly into memory** without the traditional system-to-VRAM transfer overhead. Despite its compact form factor, the Spark successfully ran **Llama 3.1 70B (FP8)** at **803 tps prefill / 2.7 tps decode**, which is remarkable for a workstation that sits quietly on a desk.\n\nThis unified-memory design makes DGX Spark particularly valuable for **prototyping**, **model experimentation**, and **edge-AI research**, where seamless memory access is often more useful than raw TFLOPs.\n\n#### Speculative Decoding Acceleration\n\nTo further explore performance optimization on the DGX Spark, we enabled **speculative decoding** using **EAGLE 3** within **SGLang**. This technique allows a smaller “draft” model to propose multiple tokens ahead, while the larger target model verifies them in parallel.\n\nWith speculative decoding enabled, we observed up to a **2× speed-up** in end-to-end inference throughput compared to standard decoding across multiple models, such as **Llama 3.1 8B**.\n\nThis improvement effectively mitigates part of the unified-memory bandwidth limitation and demonstrates that **software-level innovations** such as speculative decoding can meaningfully enhance inference performance on compact, bandwidth-constrained systems like the DGX Spark.\n\n#### Efficiency and Thermal Design\n\nThe DGX Spark maintains sustained throughput across high-intensity tests without thermal throttling. Even under full load, e.g., **SGLang DeepSeek-R1 14B (FP8)** at batch 8 achieving **2,074 tps / 83.5 tps**, fan noise and temperature remained stable, highlighting NVIDIA’s excellent **metal-foam cooling design** and well-optimized **power delivery system**.\n\nIts **USB-C power input** (up to 240 W) and external PSU allow for greater thermal headroom inside the chassis, a clear advantage for long-running workloads compared to compact consumer systems like the Mac Mini or Mac Studio, which showed thermal drop-off in similar tests.\n\n#### Summary\n\nIn short, the DGX Spark is **not built to compete head-to-head** with full-sized Blackwell or Ada-Lovelace GPUs, but rather to bring the DGX experience into a compact, developer-friendly form factor.  \nIt’s an ideal platform for:\n\n* **Model prototyping and experimentation**  \n* **Lightweight on-device inference**  \n* **Research on memory-coherent GPU architectures**\n\nIt’s a **gorgeous, well-engineered mini supercomputer** that trades raw power for accessibility, efficiency, and elegance, and in those areas, it absolutely shines.\n\n![](/images/blog/nvidia_dgx_spark/product_4.jpg)\n\n## Use Cases\n\n### SGLang Model Serving\n\nThe DGX Spark comes with Docker preinstalled, allowing you to serve open-weight models via SGLang with just a single command:\n\n```bash\ndocker run --gpus all \\\n    --shm-size 32g \\\n    -p 30000:30000 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --env \"HF_TOKEN=\u003csecret\u003e\" \\\n    --ipc=host \\\n    lmsysorg/sglang:spark \\\n    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --quantization fp8 --host 0.0.0.0 --port 30000\n```\n\nReplace `\u003csecret\u003e` with your own Hugging Face access token.\n\n#### Enabling Speculative Decoding (EAGLE3)\n\nTo enable **speculative decoding** using **EAGLE3**, simply run the following command:\n\n```bash\ndocker run --gpus all \\\n    --shm-size 32g \\\n    -p 30000:30000 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    --env \"HF_TOKEN=\u003csecret\u003e\" \\\n    --env \"SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1\" \\\n    --ipc=host \\\n    lmsysorg/sglang:spark \\\n    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --quantization fp8 --host 0.0.0.0 --port 30000 \\\n    --speculative-algorithm EAGLE3 \\\n    --speculative-draft-model-path jamesliu1/sglang-EAGLE3-Llama-3.1-Instruct-8B \\\n    --speculative-num-steps 5 \\\n    --speculative-eagle-topk 8 \\\n    --speculative-num-draft-tokens 32 \\\n    --mem-fraction 0.6 \\\n    --cuda-graph-max-bs 2 \\\n    --dtype float16\n```\n\nWith speculative decoding enabled, SGLang can leverage a smaller draft model to predict multiple tokens ahead, effectively **doubling inference throughput** compared to standard decoding.\n\n#### Sending Requests via the OpenAI-Compatible API\n\nOnce SGLang successfully initializes, you can interact with your model through OpenAI-compatible API endpoints:\n\n```bash\ncurl http://localhost:30000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"How many letters are there in the word SGLang?\"\n            }\n        ]\n    }'\n```\n\n![](/images/blog/nvidia_dgx_spark/demo_1.jpg)\n\n### Chatting with Local Model\n\nOnce you have **SGLang** set up and serving a model, you can easily connect it to **Open WebUI** to chat with any open-weight model you like. Open WebUI provides a sleek, browser-based interface that’s fully compatible with OpenAI-style APIs, meaning it works seamlessly with your local SGLang server. With just a quick configuration pointing to your DGX Spark’s endpoint, you can interact with models such as **Llama 3**, **Gemma 3**, or **DeepSeek-R1** directly from your browser, no cloud dependencies, no latency, and complete control over your data.\n\n![](/images/blog/nvidia_dgx_spark/demo_2.jpg)\n\n### Coding with Local Model\n\nOne of the most practical ways to utilize the DGX Spark is as a **local coding assistant,** completely offline and secure.\n\nBy combining **Zed**, a modern AI-integrated code editor, with **Ollama**, you can run **GPT-OSS 20B** locally to power code completion, inline chat, and smart refactoring without relying on the cloud.\n\n#### Step 1\\. Install Ollama\n\n```bash\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n#### Step 2\\. Pull GPT-OSS 20B for Coding\n\n```bash\nollama pull gpt-oss:20b\n```\n\n#### Step 3\\. Integrate Zed with Ollama\n\nInstall Zed:\n\n```bash\ncurl -f https://zed.dev/install.sh | sh\n```\n\nZed automatically detects local models served by Ollama, allowing you to start using the built-in chat assistant immediately after launching the editor.\n\n![](/images/blog/nvidia_dgx_spark/demo_3.jpg)\n\n## Conclusion\n\nThe **NVIDIA DGX Spark** is a fascinating glimpse into the future of personal AI computing. It takes what was once reserved for data centers: large memory, high-bandwidth Ethernet interconnects, and Blackwell-class performance, and distills it into a compact, beautifully engineered desktop form factor. While it doesn’t rival full-size DGX servers or discrete RTX GPUs in raw throughput, it shines in accessibility, efficiency, and versatility.\n\nFrom running **SGLang** and **Ollama** for local model serving, to experimenting with **speculative decoding (EAGLE3)**, to exploring distributed inference through **dual-Spark clustering**, the platform proves itself as more than just a miniature supercomputer. It’s a developer’s sandbox for the next era of AI.\n\nThe NVIDIA DGX Spark isn’t built to replace cloud-scale infrastructure; it’s built to **bring AI experimentation to your desk**. Whether you’re benchmarking open-weight LLMs, developing inference frameworks, or building your own private coding assistant, the Spark empowers you to do it all locally, quietly, elegantly, and with NVIDIA’s unmistakable engineering polish.\n","slug":"2025-10-13-nvidia-dgx-spark"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2025-10-13-nvidia-dgx-spark"},"buildId":"uLkn4zkcA58kiW8X6wEgX","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>