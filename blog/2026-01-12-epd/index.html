<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>EPD Disaggregation: Elastic Encoder Scaling for Vision-Language Models in SGLang | LMSYS Org</title><meta name="title" content="EPD Disaggregation: Elastic Encoder Scaling for Vision-Language Models in SGLang | LMSYS Org"/><meta property="og:title" content="EPD Disaggregation: Elastic Encoder Scaling for Vision-Language Models in SGLang | LMSYS Org"/><meta name="twitter:title" content="EPD Disaggregation: Elastic Encoder Scaling for Vision-Language Models in SGLang | LMSYS Org"/><meta name="description" content="&lt;h2&gt;&lt;a id=&quot;tldr&quot; class=&quot;anchor&quot; href=&quot;#tldr&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 1..."/><meta property="og:description" content="&lt;h2&gt;&lt;a id=&quot;tldr&quot; class=&quot;anchor&quot; href=&quot;#tldr&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 1..."/><meta name="twitter:description" content="&lt;h2&gt;&lt;a id=&quot;tldr&quot; class=&quot;anchor&quot; href=&quot;#tldr&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 1..."/><meta property="og:image" content="https://lmsys.org/images/blog/epd/epd_preview.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/epd/epd_preview.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2026-01-12-epd"/><meta name="twitter:url" content="https://lmsys.org/blog/2026-01-12-epd"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d62cc293bc63f5ee.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/yZXPcMyfUCcN8SYdOM9NB/_buildManifest.js" defer=""></script><script src="/_next/static/yZXPcMyfUCcN8SYdOM9NB/_ssgManifest.js" defer=""></script><script src="/_next/static/yZXPcMyfUCcN8SYdOM9NB/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.io" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">EPD Disaggregation: Elastic Encoder Scaling for Vision-Language Models in SGLang</h1><p class="text-xl pt-2 pb-2">by: <!-- -->rednote hilab, Alibaba Cloud Computing, AntGroup SCT<!-- -->,<!-- --> <!-- -->Jan 12, 2026<!-- --></p><hr/><div class="pt-2 article"><h2><a id="tldr" class="anchor" href="#tldr" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TL;DR</h2>
<blockquote>
<p>We introduce Encoder-Prefill-Decode (EPD) Disaggregation in SGLang, a novel architecture that separates vision encoding from language processing in Vision-Language Models (VLMs).  This can enable:</p>
</blockquote>
<ul>
<li><strong>Independent scaling of vision encoding capacity</strong>: Encoder servers can be scaled horizontally without affecting language model deployment, enabling better resource utilization for vision-heavy workloads.</li>
<li><strong>Compatibility with existing PD disaggregation</strong>: EPD can be combined with Prefill-Decode disaggregation for a complete three-tier architecture.</li>
<li><strong>Flexible transfer backends</strong>: Support for multiple transfer mechanisms (ZMQ, GPU-direct via Mooncake) allows optimization for different deployment scenarios.</li>
<li><strong>Vision embedding caching</strong>: Frequently used images can be cached at encoder servers, eliminating redundant ViT computations and reducing network transfer overhead.</li>
</ul>
<p>EPD is highly effective in <strong>image-heavy scenarios</strong> (e.g., multi-image inputs), where the visual encoding process is the primary computational bottleneck. For instance, in these scenarios, we leverage EPD to significantly reduce request TTFT under load—achieving approximately 6–8× lower latency compared to the colocation approach at 1 QPS. Conversely, for <strong>image-light scenarios</strong> with few images, EPD may be less efficient or even counterproductive. This is because the additional network latency incurred by transmitting embeddings across nodes can outweigh the time saved by offloading the encoding task, potentially resulting in a higher TTFT compared to a colocation approach.</p>
<h2><a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h2>
<p>Vision-Language Models (VLMs) like Qwen2.5-VL and Llama-Vision combine visual understanding with language generation. However, these models face unique scaling challenges:</p>
<ul>
<li>Heterogeneous compute needs: Vision encoding (CNN/ViT) requires different computational patterns than language decoding (Transformer)</li>
<li>Imbalanced resource usage: Vision processing is compute-intensive but only needed during prefill</li>
<li>Limited flexibility: Traditional monolithic deployments can't independently scale vision and language components</li>
<li>Intra-request parallelism: Different images in one request can be encoded independently.</li>
<li>Poor scaling under tensor parallelism: Because the vision encoder has far fewer parameters than the language component, applying tensor parallelism to it is inefficient and generally unnecessary.</li>
</ul>
<p>SGLang's existing Prefill-Decode (PD) disaggregation already separates prefill from decode phases. EPD extends this by further separating vision encoding from language prefill, creating a three-tier architecture.</p>
<h2><a id="the-vit-scaling-problem-why-tensor-parallelism-doesnt-always-help" class="anchor" href="#the-vit-scaling-problem-why-tensor-parallelism-doesnt-always-help" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The ViT Scaling Problem: Why Tensor Parallelism Doesn't Always Help</h2>
<h3><a id="the-counter-intuitive-finding" class="anchor" href="#the-counter-intuitive-finding" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Counter-Intuitive Finding</h3>
<p>One of the key insights from EPD disaggregation is that Vision Transformers (ViT) <strong>do NOT</strong> benefit from increased Tensor Parallelism, and can actually become slower with higher TP:</p>
<p>Benchmark on H20 with Qwen2.5-VL-72B (4 images per request):</p>
<table>
<thead>
<tr>
<th>tp</th>
<th>vit mean time</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>492.13ms</td>
</tr>
<tr>
<td>4</td>
<td>465.80ms</td>
</tr>
<tr>
<td>8</td>
<td>523.80ms</td>
</tr>
</tbody>
</table>
<p>Why Does This Happen?</p>
<ol>
<li>Communication overhead dominates execution time.</li>
<li>The weight parameters of vision models are usually small.</li>
</ol>
<p>EPD sidesteps this by scaling encoders horizontally instead of increasing TP.</p>
<h3><a id="architecture-overview" class="anchor" href="#architecture-overview" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Architecture Overview</h3>
<p>The EPD architecture follows a request flow:</p>
<ol>
<li><strong>Client Request</strong>: A multimodal request arrives at the prefill server (via load balancer or direct connection).</li>
<li><strong>Image Distribution</strong>: The prefill server identifies image inputs and distributes them to one or more encoder servers. Images can be split across multiple encoders for load balancing.</li>
<li><strong>Vision Encoding</strong>: Encoder servers process images through ViT, generating vision embeddings and image grid metadata. Results are cached if enabled.</li>
<li><strong>Embedding Transfer</strong>: Vision embeddings are transferred back to the prefill server using the configured transfer backend (ZMQ, Mooncake, etc.).</li>
<li><strong>LLM Computation</strong>: The prefill server combines vision embeddings with text tokens to form mm_inputs containing pre-computed tensors. The LLM performs Prefill and Decode computation based on these embeddings. If PD disaggregation is enabled, the existing Prefill-Decode transfer logic is reused; otherwise, token generation happens directly on the prefill server.</li>
</ol>
<h3><a id="key-components" class="anchor" href="#key-components" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Key Components</h3>
<p align="center">
  <img src="/images/blog/epd/epd_workflow.png" alt="EPD Workflow" width="50%" />
</p>
<p align="center">
  <img src="/images/blog/epd/epd_architecture.svg" alt="EPD Architecture" width="50%" />
</p>
<p><strong>Encoder Server</strong> (--encoder-only)</p>
<ul>
<li>Vision-only (no language weights); preprocessing + ViT forward to generate vision embeddings</li>
<li>Prefix multimodal cache support</li>
<li>Scale out for load balancing and parallel multi-image split inference</li>
</ul>
<p><strong>Prefill Server</strong> (--language-only)</p>
<ul>
<li>Language model only</li>
<li>Receives embeddings from encoder(s)</li>
<li>With PD: ships KV to Decode; without PD: decodes locally</li>
</ul>
<p><strong>Decode Server</strong></p>
<ul>
<li>Standard decode-only instance</li>
<li>Receives KV cache from prefill</li>
</ul>
<h3><a id="implementation-details" class="anchor" href="#implementation-details" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Implementation Details</h3>
<h4><a id="image-distribution-strategies" class="anchor" href="#image-distribution-strategies" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Image Distribution Strategies</h4>
<p>Unlike tensor parallelism which splits a single model across GPUs, EPD uses data parallelism by running multiple independent encoder instances and distributing images among them.</p>
<p>Example distribution:</p>
<pre><code class="hljs"><span class="hljs-attribute">Request with 7 images</span><span class="hljs-punctuation">:</span> <span class="hljs-string">[img0, img1, img2, img3, img4, img5, img6]</span>
<span class="hljs-attribute">3 encoders available

Distribution (after shuffle)</span><span class="hljs-punctuation">:</span>
<span class="hljs-attribute">├─ Encoder 0</span><span class="hljs-punctuation">:</span> <span class="hljs-string">[img0, img1, img2] (3 images)</span>
<span class="hljs-attribute">├─ Encoder 1</span><span class="hljs-punctuation">:</span> <span class="hljs-string">[img3, img4] (2 images)</span>
<span class="hljs-attribute">└─ Encoder 2</span><span class="hljs-punctuation">:</span> <span class="hljs-string">[img5, img6] (2 images)</span>
</code></pre>
<h4><a id="transfer-backends" class="anchor" href="#transfer-backends" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Transfer Backends</h4>
<p>EPD supports three transfer backends for vision embeddings:</p>
<p><strong>zmq_to_scheduler (Default)</strong></p>
<ul>
<li>Direct ZMQ socket communication</li>
<li>Embeddings sent from encoder to scheduler via RDMA transfer engine.</li>
<li>No blocking</li>
</ul>
<p><strong>zmq_to_tokenizer</strong></p>
<ul>
<li>Embeddings sent to tokenizer manager</li>
<li>Processed during tokenization phase</li>
</ul>
<p><strong>mooncake</strong></p>
<ul>
<li>RDMA-based transfer for multi-node</li>
<li>Registers embeddings in shared memory</li>
<li>High-bandwidth, low-latency transfer</li>
</ul>
<h4><a id="vision-embedding-cache" class="anchor" href="#vision-embedding-cache" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Vision Embedding Cache</h4>
<p>The encoder supports prefix multimodal caching to avoid redundant ViT computations:</p>
<ul>
<li>Eliminates redundant vision encoding</li>
<li>Reduces latency for repeated images</li>
<li>Configurable cache size (default 4GB via SGLANG_VLM_CACHE_SIZE_MB)</li>
</ul>
<h3><a id="usage-examples" class="anchor" href="#usage-examples" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Usage Examples</h3>
<ul>
<li>To launch the encoder instance</li>
</ul>
<pre><code class="hljs"><span class="hljs-attribute">MODEL</span>=Qwen/Qwen2.<span class="hljs-number">5</span>-VL-<span class="hljs-number">7</span>B-Instruct
<span class="hljs-attribute">PORT</span>=<span class="hljs-number">30002</span>

<span class="hljs-attribute">CUDA_VISIBLE_DEVICES</span>=<span class="hljs-number">2</span> taskset -c $<span class="hljs-number">1</span> python -m sglang.launch_server <span class="hljs-punctuation">\
</span>    --model-path $MODEL <span class="hljs-punctuation">\
</span>    --encoder-only <span class="hljs-punctuation">\
</span>    --enable-prefix-mm-cache <span class="hljs-punctuation">\
</span>    --port $PORT
</code></pre>
<ul>
<li>To launch the prefill instance</li>
</ul>
<pre><code class="hljs"><span class="hljs-attribute">MODEL</span>=Qwen/Qwen2.<span class="hljs-number">5</span>-VL-<span class="hljs-number">7</span>B-Instruct
<span class="hljs-attribute">PORT</span>=<span class="hljs-number">30000</span>
<span class="hljs-attribute">TP</span>=<span class="hljs-number">1</span>
<span class="hljs-attribute">MEM_FRACTION</span>=<span class="hljs-number">0</span>.<span class="hljs-number">5</span>
<span class="hljs-attribute">CHUNK_SIZE</span>=<span class="hljs-number">8192</span>

<span class="hljs-attribute">SGLANG_VLM_CACHE_SIZE_MB</span>=<span class="hljs-number">0</span> CUDA_VISIBLE_DEVICES=<span class="hljs-number">0</span> python -m sglang.launch_server <span class="hljs-punctuation">\
</span>    --model-path $MODEL <span class="hljs-punctuation">\
</span>    --disaggregation-mode prefill <span class="hljs-punctuation">\
</span>    --disaggregation-transfer-backend nixl <span class="hljs-punctuation">\
</span>    --tp $TP <span class="hljs-punctuation">\
</span>    --mem-fraction-static $MEM_FRACTION <span class="hljs-punctuation">\
</span>    --disable-radix-cache <span class="hljs-punctuation">\
</span>    --chunked-prefill-size $CHUNK_SIZE <span class="hljs-punctuation">\
</span>    --language-only <span class="hljs-punctuation">\
</span>    --encoder-urls http://<span class="hljs-number">127.0.0.1:30002</span> http://<span class="hljs-number">127.0.0.1:30003</span> http://<span class="hljs-number">127.0.0.1:30004</span> http://<span class="hljs-number">127.0.0.1:30005</span> http://<span class="hljs-number">127.0.0.1:30006</span> http://<span class="hljs-number">127.0.0.1:30007</span> <span class="hljs-punctuation">\
</span>    --port $PORT
</code></pre>
<ul>
<li>To launch the decode instance</li>
</ul>
<pre><code class="hljs"><span class="hljs-attribute">MODEL</span>=Qwen/Qwen2.5-VL-7B-Instruct
<span class="hljs-attribute">PORT</span>=30001
<span class="hljs-attribute">TP</span>=1

<span class="hljs-attribute">CUDA_VISIBLE_DEVICES</span>=1 python -m sglang.launch_server \
    --model-path <span class="hljs-variable">$MODEL</span> \
    --disaggregation-mode decode \
    --disaggregation-transfer-backend nixl \
    --tp <span class="hljs-variable">$TP</span> \
    --port <span class="hljs-variable">$PORT</span>
</code></pre>
<ul>
<li>To launch minlb</li>
</ul>
<pre><code class="hljs language-bash">python -m sglang_router.launch_router \
  --pd-disaggregation \
  --mini-lb \
  --prefill http://127.0.0.1:30000 \
  --decode http://127.0.0.1:30001 \
  --port 8000
</code></pre>
<h2><a id="benchmark" class="anchor" href="#benchmark" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Benchmark</h2>
<p>EPD disaggregation targets vision-heavy workloads (multi-image requests) and improves Time To First Token (TTFT) by scaling encoder servers horizontally.
Launch bench script:</p>
<pre><code class="hljs"><span class="hljs-attribute">python</span> -m sglang.bench_serving <span class="hljs-punctuation">\
</span>    --random-image-count <span class="hljs-punctuation">\
</span>    --model <span class="hljs-variable">${MODEL_PATH}</span> <span class="hljs-punctuation">\
</span>    --num-prompts <span class="hljs-number">64</span> <span class="hljs-punctuation">\
</span>    --dataset-name image <span class="hljs-punctuation">\
</span>    --random-input-len <span class="hljs-number">128</span> <span class="hljs-punctuation">\
</span>    --random-output-len <span class="hljs-number">256</span> <span class="hljs-punctuation">\
</span>    --image-count <span class="hljs-number">8</span> <span class="hljs-punctuation">\
</span>    --image-resolution <span class="hljs-number">1080</span>p <span class="hljs-punctuation">\
</span>    --host $HOST_IP <span class="hljs-punctuation">\
</span>    --port $port <span class="hljs-punctuation">\
</span>    --backend vllm-chat <span class="hljs-punctuation">\
</span>    --request-rate $request_rate 
</code></pre>
<h3><a id="experimental-setup" class="anchor" href="#experimental-setup" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Experimental Setup</h3>
<p><strong>Environment</strong>: 8× H20 96GB GPUs</p>
<p><strong>Model</strong>: Qwen3-VL-235B-A22B-Instruct-FP8</p>
<p><strong>Dataset</strong>: Random multimodal dataset</p>
<ul>
<li>Text tokens: 128 / 256</li>
<li>Images per request: 1-8 images (random count, average ~4 images)</li>
<li>Image resolution: 1080p</li>
<li>QPS range: 0.2-1.0</li>
</ul>
<p><strong>Deployment Configurations</strong>:</p>
<ul>
<li><strong>Colocate</strong>: 1 PD instance with tensor-parallel-size=4, uses 4× H20 GPUs</li>
<li><strong>1E1P</strong> (1 Encoder + 1 PD instance): 1 Encoder with tensor-parallel-size=1 + 1 PD instance with tensor-parallel-size=4, uses 5× H20 GPUs</li>
<li><strong>2E1P</strong> (2 Encoders + 1 PD instance): 2 Encoders with tensor-parallel-size=1 each + 1 PD instance with tensor-parallel-size=4, uses 6× H20 GPUs</li>
</ul>
<h3><a id="bench-results" class="anchor" href="#bench-results" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Bench Results</h3>
<p>Mean TTFT (EPD vs colocate)</p>
<p align="center">
  <img src="/images/blog/epd/epd_vs_colocate_ttft.png" alt="TTFT Results" width="50%" />
</p>
<p>Mean TPOT (EPD vs colocate)</p>
<p align="center">
  <img src="/images/blog/epd/epd_vs_colocate_tpot.png" alt="TPOT Results" width="50%" />
</p>
<blockquote>
<p>The higher TPOT observed in 2e1p is attributed to its larger batch size during decoding.</p>
</blockquote>
<p>Request Throughput (EPD vs colocate)</p>
<p align="center">
  <img src="/images/blog/epd/epd_vs_colocate_throughput.png" alt="Throughput Results" width="50%" />
</p>
<p>Key Findings (vs. colocate)</p>
<ul>
<li>Encoder/prefill keeps TTFT much lower under load (≈6–8x lower than colocate at 1 qps).</li>
<li>TPOT stays far below colocate (≈8–10x lower), indicating much tighter latency.</li>
<li>Throughput roughly doubles at higher QPS (≈2x at 0.8–1.0 qps vs. colocate).</li>
<li>The dramatic reduction in TTFT is achieved by allocating dedicated GPU resources to the encoder. Despite using 50% additional GPUs (2E1P uses 6× H20 vs. colocate's 4× H20), EPD disaggregation achieves a much higher return on investment (ROI) compared to simply scaling the traditional integrated architecture, with performance gains exceeding the additional resource cost.</li>
<li>While highly effective for image-heavy tasks, dedicated encoder GPUs may experience lower utilization (idle time) in image-light scenarios, suggesting that EPD is most resource-efficient when visual processing is the primary bottleneck.</li>
</ul>
<h2><a id="acknowledgment" class="anchor" href="#acknowledgment" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgment</h2>
<p><strong>rednote hilab</strong>: <a href="https://github.com/gty111">Tianyu Guo</a>, a'du, Tianming Xu</p>
<p><strong>Alibaba Cloud Computing</strong>: <a href="https://github.com/liusy58">Siyu Liu</a>, <a href="https://github.com/ShangmingCai">Shangming Cai</a></p>
<p><strong>AntGroup SCT</strong>: <a href="https://github.com/ZhengWG">Wengang Zheng</a></p>
<h2><a id="learn-more" class="anchor" href="#learn-more" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Learn more</h2>
<ul>
<li>Roadmap: <a href="https://github.com/sgl-project/sglang/issues/15118">Encoder Disaggregation (2025 Q4)</a></li>
</ul>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"EPD Disaggregation: Elastic Encoder Scaling for Vision-Language Models in SGLang","author":"rednote hilab, Alibaba Cloud Computing, AntGroup SCT","date":"January 12, 2026","previewImg":"/images/blog/epd/epd_preview.png"},"content":"\n## TL;DR\n\n\u003e We introduce Encoder-Prefill-Decode (EPD) Disaggregation in SGLang, a novel architecture that separates vision encoding from language processing in Vision-Language Models (VLMs).  This can enable:\n\n- **Independent scaling of vision encoding capacity**: Encoder servers can be scaled horizontally without affecting language model deployment, enabling better resource utilization for vision-heavy workloads.\n- **Compatibility with existing PD disaggregation**: EPD can be combined with Prefill-Decode disaggregation for a complete three-tier architecture.\n- **Flexible transfer backends**: Support for multiple transfer mechanisms (ZMQ, GPU-direct via Mooncake) allows optimization for different deployment scenarios.\n- **Vision embedding caching**: Frequently used images can be cached at encoder servers, eliminating redundant ViT computations and reducing network transfer overhead.\n\nEPD is highly effective in **image-heavy scenarios** (e.g., multi-image inputs), where the visual encoding process is the primary computational bottleneck. For instance, in these scenarios, we leverage EPD to significantly reduce request TTFT under load—achieving approximately 6–8× lower latency compared to the colocation approach at 1 QPS. Conversely, for **image-light scenarios** with few images, EPD may be less efficient or even counterproductive. This is because the additional network latency incurred by transmitting embeddings across nodes can outweigh the time saved by offloading the encoding task, potentially resulting in a higher TTFT compared to a colocation approach.\n\n## Introduction\nVision-Language Models (VLMs) like Qwen2.5-VL and Llama-Vision combine visual understanding with language generation. However, these models face unique scaling challenges:\n\n- Heterogeneous compute needs: Vision encoding (CNN/ViT) requires different computational patterns than language decoding (Transformer)\n- Imbalanced resource usage: Vision processing is compute-intensive but only needed during prefill\n- Limited flexibility: Traditional monolithic deployments can't independently scale vision and language components\n- Intra-request parallelism: Different images in one request can be encoded independently. \n- Poor scaling under tensor parallelism: Because the vision encoder has far fewer parameters than the language component, applying tensor parallelism to it is inefficient and generally unnecessary.\n\nSGLang's existing Prefill-Decode (PD) disaggregation already separates prefill from decode phases. EPD extends this by further separating vision encoding from language prefill, creating a three-tier architecture.\n\n## The ViT Scaling Problem: Why Tensor Parallelism Doesn't Always Help\n\n### The Counter-Intuitive Finding\n\nOne of the key insights from EPD disaggregation is that Vision Transformers (ViT) **do NOT** benefit from increased Tensor Parallelism, and can actually become slower with higher TP:\n\nBenchmark on H20 with Qwen2.5-VL-72B (4 images per request):\n\n| tp  | vit mean time |\n|-----|---------------|\n| 2   | 492.13ms      |\n| 4   | 465.80ms      |\n| 8   | 523.80ms      |\n\nWhy Does This Happen?\n\n1. Communication overhead dominates execution time.\n2. The weight parameters of vision models are usually small.\n\nEPD sidesteps this by scaling encoders horizontally instead of increasing TP.\n\n### Architecture Overview\nThe EPD architecture follows a request flow:\n\n1. **Client Request**: A multimodal request arrives at the prefill server (via load balancer or direct connection).\n2. **Image Distribution**: The prefill server identifies image inputs and distributes them to one or more encoder servers. Images can be split across multiple encoders for load balancing.\n3. **Vision Encoding**: Encoder servers process images through ViT, generating vision embeddings and image grid metadata. Results are cached if enabled.\n4. **Embedding Transfer**: Vision embeddings are transferred back to the prefill server using the configured transfer backend (ZMQ, Mooncake, etc.).\n5. **LLM Computation**: The prefill server combines vision embeddings with text tokens to form mm_inputs containing pre-computed tensors. The LLM performs Prefill and Decode computation based on these embeddings. If PD disaggregation is enabled, the existing Prefill-Decode transfer logic is reused; otherwise, token generation happens directly on the prefill server.\n\n### Key Components\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/epd/epd_workflow.png\" alt=\"EPD Workflow\" width=\"50%\" /\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/epd/epd_architecture.svg\" alt=\"EPD Architecture\" width=\"50%\" /\u003e\n\u003c/p\u003e\n\n\n\n\n**Encoder Server** (--encoder-only)\n- Vision-only (no language weights); preprocessing + ViT forward to generate vision embeddings\n- Prefix multimodal cache support\n- Scale out for load balancing and parallel multi-image split inference\n\n**Prefill Server** (--language-only)\n- Language model only\n- Receives embeddings from encoder(s)\n- With PD: ships KV to Decode; without PD: decodes locally  \n\n**Decode Server**\n- Standard decode-only instance\n- Receives KV cache from prefill\n\n### Implementation Details\n\n#### Image Distribution Strategies\nUnlike tensor parallelism which splits a single model across GPUs, EPD uses data parallelism by running multiple independent encoder instances and distributing images among them.\n\nExample distribution:\n\n```\nRequest with 7 images: [img0, img1, img2, img3, img4, img5, img6]\n3 encoders available\n\nDistribution (after shuffle):\n├─ Encoder 0: [img0, img1, img2] (3 images)\n├─ Encoder 1: [img3, img4] (2 images)\n└─ Encoder 2: [img5, img6] (2 images)\n```\n\n#### Transfer Backends\n\nEPD supports three transfer backends for vision embeddings:\n\n**zmq_to_scheduler (Default)**\n- Direct ZMQ socket communication\n- Embeddings sent from encoder to scheduler via RDMA transfer engine.\n- No blocking \n\n**zmq_to_tokenizer**\n- Embeddings sent to tokenizer manager\n- Processed during tokenization phase\n\n**mooncake**\n- RDMA-based transfer for multi-node\n- Registers embeddings in shared memory\n- High-bandwidth, low-latency transfer\n\n\n#### Vision Embedding Cache\nThe encoder supports prefix multimodal caching to avoid redundant ViT computations:\n- Eliminates redundant vision encoding\n- Reduces latency for repeated images\n- Configurable cache size (default 4GB via SGLANG_VLM_CACHE_SIZE_MB)\n\n### Usage Examples\n- To launch the encoder instance\n```\nMODEL=Qwen/Qwen2.5-VL-7B-Instruct\nPORT=30002\n\nCUDA_VISIBLE_DEVICES=2 taskset -c $1 python -m sglang.launch_server \\\n    --model-path $MODEL \\\n    --encoder-only \\\n    --enable-prefix-mm-cache \\\n    --port $PORT\n```\n- To launch the prefill instance\n```\nMODEL=Qwen/Qwen2.5-VL-7B-Instruct\nPORT=30000\nTP=1\nMEM_FRACTION=0.5\nCHUNK_SIZE=8192\n\nSGLANG_VLM_CACHE_SIZE_MB=0 CUDA_VISIBLE_DEVICES=0 python -m sglang.launch_server \\\n    --model-path $MODEL \\\n    --disaggregation-mode prefill \\\n    --disaggregation-transfer-backend nixl \\\n    --tp $TP \\\n    --mem-fraction-static $MEM_FRACTION \\\n    --disable-radix-cache \\\n    --chunked-prefill-size $CHUNK_SIZE \\\n    --language-only \\\n    --encoder-urls http://127.0.0.1:30002 http://127.0.0.1:30003 http://127.0.0.1:30004 http://127.0.0.1:30005 http://127.0.0.1:30006 http://127.0.0.1:30007 \\\n    --port $PORT\n```\n- To launch the decode instance\n```\nMODEL=Qwen/Qwen2.5-VL-7B-Instruct\nPORT=30001\nTP=1\n\nCUDA_VISIBLE_DEVICES=1 python -m sglang.launch_server \\\n    --model-path $MODEL \\\n    --disaggregation-mode decode \\\n    --disaggregation-transfer-backend nixl \\\n    --tp $TP \\\n    --port $PORT\n```\n\n- To launch minlb\n```bash\npython -m sglang_router.launch_router \\\n  --pd-disaggregation \\\n  --mini-lb \\\n  --prefill http://127.0.0.1:30000 \\\n  --decode http://127.0.0.1:30001 \\\n  --port 8000\n```\n## Benchmark\nEPD disaggregation targets vision-heavy workloads (multi-image requests) and improves Time To First Token (TTFT) by scaling encoder servers horizontally.\nLaunch bench script:\n```\npython -m sglang.bench_serving \\\n    --random-image-count \\\n    --model ${MODEL_PATH} \\\n    --num-prompts 64 \\\n    --dataset-name image \\\n    --random-input-len 128 \\\n    --random-output-len 256 \\\n    --image-count 8 \\\n    --image-resolution 1080p \\\n    --host $HOST_IP \\\n    --port $port \\\n    --backend vllm-chat \\\n    --request-rate $request_rate \n```\n\n### Experimental Setup\n\n**Environment**: 8× H20 96GB GPUs\n\n**Model**: Qwen3-VL-235B-A22B-Instruct-FP8\n\n**Dataset**: Random multimodal dataset\n- Text tokens: 128 / 256\n- Images per request: 1-8 images (random count, average ~4 images)\n- Image resolution: 1080p\n- QPS range: 0.2-1.0\n\n**Deployment Configurations**:\n- **Colocate**: 1 PD instance with tensor-parallel-size=4, uses 4× H20 GPUs\n- **1E1P** (1 Encoder + 1 PD instance): 1 Encoder with tensor-parallel-size=1 + 1 PD instance with tensor-parallel-size=4, uses 5× H20 GPUs\n- **2E1P** (2 Encoders + 1 PD instance): 2 Encoders with tensor-parallel-size=1 each + 1 PD instance with tensor-parallel-size=4, uses 6× H20 GPUs\n\n\n### Bench Results\n\nMean TTFT (EPD vs colocate)\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/epd/epd_vs_colocate_ttft.png\" alt=\"TTFT Results\" width=\"50%\" /\u003e\n\u003c/p\u003e\n\nMean TPOT (EPD vs colocate)\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/epd/epd_vs_colocate_tpot.png\" alt=\"TPOT Results\" width=\"50%\" /\u003e\n\u003c/p\u003e\n\n\u003e The higher TPOT observed in 2e1p is attributed to its larger batch size during decoding.\n\nRequest Throughput (EPD vs colocate)\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/epd/epd_vs_colocate_throughput.png\" alt=\"Throughput Results\" width=\"50%\" /\u003e\n\u003c/p\u003e\n\nKey Findings (vs. colocate)\n- Encoder/prefill keeps TTFT much lower under load (≈6–8x lower than colocate at 1 qps).\n- TPOT stays far below colocate (≈8–10x lower), indicating much tighter latency.\n- Throughput roughly doubles at higher QPS (≈2x at 0.8–1.0 qps vs. colocate).\n- The dramatic reduction in TTFT is achieved by allocating dedicated GPU resources to the encoder. Despite using 50% additional GPUs (2E1P uses 6× H20 vs. colocate's 4× H20), EPD disaggregation achieves a much higher return on investment (ROI) compared to simply scaling the traditional integrated architecture, with performance gains exceeding the additional resource cost.\n- While highly effective for image-heavy tasks, dedicated encoder GPUs may experience lower utilization (idle time) in image-light scenarios, suggesting that EPD is most resource-efficient when visual processing is the primary bottleneck.\n\n\n\n## Acknowledgment\n\n**rednote hilab**: [Tianyu Guo](https://github.com/gty111), a'du, Tianming Xu\n\n**Alibaba Cloud Computing**: [Siyu Liu](https://github.com/liusy58), [Shangming Cai](https://github.com/ShangmingCai)\n\n**AntGroup SCT**: [Wengang Zheng](https://github.com/ZhengWG)\n\n## Learn more\n\n- Roadmap: [Encoder Disaggregation (2025 Q4)](https://github.com/sgl-project/sglang/issues/15118)\n","slug":"2026-01-12-epd"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2026-01-12-epd"},"buildId":"yZXPcMyfUCcN8SYdOM9NB","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>