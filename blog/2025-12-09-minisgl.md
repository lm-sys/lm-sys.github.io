---
title: "Mini-SGLang, Efficient Inference Engine in a Nutshell with 5k LoCs"
author: "SGLang Team"
date: "December 9, 2025"
previewImg: /images/blog/sglang_jax/cover.jpg
---

We introduce **Mini-SGLang**, a lightweight yet high-performance inference framework for Large Language Models (LLMs). Derived from the [SGLang](https://github.com/sgl-project/sglang) project, Mini-SGLang is designed to demystify the complexities of modern serving systems. Despite its compact codebase, it retains the advanced features that define state-of-the-art performance, including **Radix Attention** for efficient KV cache reuse, **Chunked Prefill** for controlled memory footprint, and **Tensor Parallelism** for scalable distributed serving. With an OpenAI-compatible API and out-of-the-box support for models like Llama-3 and Qwen-3, Mini-SGLang serves as both a capable inference engine and a transparent reference implementation for researchers and developers.

The source code is available at [https://github.com/DarkSharpness/mini-sglang](https://github.com/DarkSharpness/mini-sglang).

## Motivation: Why Mini-SGLang?

Although SGLang has achieved state-of-the-art inference performance with a comprehensive feature set, its codebase has grown massive, reaching nearly 300k lines of Python code. We built Mini-SGLang to address the complexity barrier, focusing on two main objectives:

### Educational Purposes

Mini-SGLang features a clean, highly modular codebase of only **5k lines of code**, making it significantly easier for beginners to understand the internals of a modern LLM engine.

Despite its simplicity, it supports both online and offline inference and implements essential modern optimizations, including **Tensor Parallelism**, **Overlap Scheduling**, **Chunked Prefill**, **Radix Cache**, and **JIT CUDA kernels**. This makes it a comprehensive learning resource. For kernel developers, Mini-SGLang also provides fine-grained **NVTX annotations**, which are invaluable for kernel debugging and performance profiling.

### Quick Research Prototype

Many ML and systems researchers struggle to integrate their optimizations into existing systems. On one hand, injecting new logic into complex frameworks like SGLang is risky; it can break invariants and lead to subtle bugs. On the other hand, building an inference engine from scratch is tedious, requiring significant effort to handle infrastructure details (e.g., frontend servers, tokenization, NCCL communication) just to match state-of-the-art baselines.

Mini-SGLang strikes a balance. It offers an out-of-the-box, high-performance framework that is easy to extend and optimize. It handles the heavy lifting of infrastructure while remaining flexible enough for rapid prototyping. Additionally, Mini-SGLang provides **OpenAI-compatible benchmark utilities**, facilitating end-to-end performance analysis and comparison against various serving engines, such as [SGLang](https://github.com/sgl-project/sglang), [vLLM](https://github.com/vllm-project/vllm) and [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM).

## Features

Mini-SGLang shares the same high-level system architecture as SGLang, consisting of a frontend API server, a tokenizer server, and a backend scheduler for each GPU.

![system-design](/images/blog/minisgl/design.png)

### Overlap Scheduling

LLM inference is not just about GPU computation; a significant amount of work is handled by the CPU, including batch scheduling, memory management, and token processing. Without optimization, this CPU overhead can lead to GPU idling, hurting overall performance.

Mini-SGLang implements an **overlap scheduling** mechanism, similar to the one in SGLang, to mitigate this. By preparing the next batch of requests on the CPU while the GPU is busy with the current batch, it effectively hides the CPU overhead. As the Nsight profile below shows, this keeps the GPU consistently utilized, eliminating idle gaps ("bubbles") and maximizing throughput. More technical details are available in our [previous blog post](https://lmsys.org/blog/2024-12-04-sglang-v0-4/).

![overlap](/images/blog/minisgl/overlap.png)

> An example of overlapped execution. There's no bubble in GPU stream, which means the CPU execution latency is overlapped.

![no-overlap](/images/blog/minisgl/no-overlap.png)

> An example of non-overlapped execution. There're many bubbles in GPU stream due to CPU and GPU synchronization.

### High-Performance Attention Kernels

Mini-SGLang integrates state-of-the-art attention kernels to ensure top performance. It leverages [FlashAttention-3](https://github.com/Dao-AILab/flash-attention) for efficient prefill and [FlashInfer](https://github.com/flashinfer-ai/flashinfer) for accelerated decode on NVIDIA Hopper architecture.

### Interactive Shell Mode

For direct interaction and testing, Mini-SGLang includes a simple shell mode. This allows users to chat with LLMs directly from the command line, providing a convenient way to test models and observe their behavior without needing a separate client.

![Shell Example](/images/blog/minisgl/shell.png)

## Performance Benchmark

To evaluate the performance of Mini-SGLang, we conducted comprehensive experiments covering both offline throughput and online serving latency.

### Offline Inference Throughput

We evaluated Mini-SGLang's offline throughput against nano-vllm and vLLM on a single NVIDIA H200 GPU. Following the methodology from [nano-vllm](https://github.com/GeeeekExplorer/nano-vllm/), we used the [Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B/) model and also tested the larger [Qwen3-14B](https://huggingface.co/Qwen/Qwen3-14B/) model to assess performance at scale. We focused on Qwen3 models due to the current limitations of the nano-vllm baseline.

The throughput results (in tokens per second) are shown below:

| Model      | Output Tokens | Mini-SGLang          | nano-vllm | vLLM   | Mini-SGLang w/o overlap |
| ---------- | ------------- | ---------------------- | --------- | ------ | ----------------------- |
| Qwen3-0.6B | 133,966       | **20,711**             | 19,638    | 18,014 | 15,014                  |
| Qwen3-14B  | 133,966       | **5,811**              | 5,560     | 5,541  | 5,314                   |

The results show that Mini-SGLang consistently outperforms both nano-vllm and vLLM. On the smaller Qwen3-0.6B model, the performance gain is particularly noticeable, thanks to our **overlap scheduling** mechanism that effectively hides CPU overhead. While the gap narrows on the larger Qwen3-14B model as GPU computation becomes the primary bottleneck, Mini-SGLang maintains a clear advantage.

**Reproducibility**: The offline benchmark script is available at [this link](https://github.com/DarkSharpness/mini-sglang/blob/main/benchmark/offline/bench_nanovllm.py). To run an ablation study without overlap scheduling, set the environment variable `MINISGL_DISABLE_OVERLAP_SCHEDULING=1`.

### Online Serving Latency

To assess real-world serving performance, we benchmarked Mini-SGLang against SGLang using a realistic workload from the [Qwen trace](https://github.com/alibaba-edu/qwen-bailian-usagetraces-anon/blob/main/qwen_traceA_blksz_16.jsonl). We replayed 1,000 requests to a [Llama3.1-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct) model deployed with 8-way tensor parallelism on 8 H200 GPUs. We measured throughput, 90th percentile (P90) Time To First Token (TTFT), and Time Between Tokens (TBT).

| System      | Throughput (token/s) | P90 TTFT (ms) | P90 TBT (ms) |
| ----------- | -------------------- | ------------- | ------------ |
| Mini-SGLang | 2165                 | 725           | 43.88        |
| SGLang      | 2149                 | 744           | 45.11        |

The results demonstrate that Mini-SGLang achieves nearly identical performance to SGLang, confirming that its lightweight design does not compromise on throughput or latency.

**Reproducibility**: Use the following commands to launch each system:

```bash
# Mini-SGLang
python -m minisgl --model "meta-llama/Llama-3.1-70B-Instruct" --tp 8 --cache naive

# SGLang
python3 -m sglang.launch_server --model "meta-llama/Llama-3.1-70B-Instruct" --tp 8 --disable-radix --port 1919
```

The online benchmark script is available at [this link](https://github.com/DarkSharpness/mini-sglang/blob/main/benchmark/online/bench_qwen.py).

## Conclusion

Mini-SGLang successfully distills the power of a state-of-the-art inference engine into a compact and understandable codebase. By retaining key optimizations like overlap scheduling and high-performance attention kernels, it delivers impressive performance while serving as an invaluable educational tool and a flexible platform for research.

We invite you to explore the [source code](https://github.com/DarkSharpness/mini-sglang), run the benchmarks, and see for yourself how Mini-SGLang makes high-performance LLM inference more accessible than ever.
