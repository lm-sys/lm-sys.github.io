---
title: "Mini-SGLang, Efficient Inference Engine in a Nutshell with 5k LoCs"
author: "SGLang Team"
date: "December 9, 2025"
previewImg: /images/blog/sglang_jax/cover.jpg
---

We introduce **Mini-SGLang**, a lightweight yet high-performance inference framework for Large Language Models (LLMs). Derived from the [SGLang](https://github.com/sgl-project/sglang) project, Mini-SGLang is designed to demystify the complexities of modern serving systems. Despite its compact codebase, it retains the advanced features that define state-of-the-art performance, including **Radix Attention** for efficient KV cache reuse, **Chunked Prefill** for controlled memory footprint, and **Tensor Parallelism** for scalable distributed serving. With an OpenAI-compatible API and out-of-the-box support for models like Llama-3 and Qwen-3, Mini-SGLang serves as both a capable inference engine and a transparent reference implementation for researchers and developers.

The source code is available at [https://github.com/DarkSharpness/mini-sglang](https://github.com/DarkSharpness/mini-sglang).

## Motivation: Why Mini-SGLang?

Although SGLang has achieved state-of-the-art inference performance with a comprehensive feature set, its codebase has grown massive, reaching nearly 300k lines of Python code. We built Mini-SGLang to address the complexity barrier, focusing on two main objectives:

### Educational Purposes

Mini-SGLang features a clean, highly modular codebase of only **5k lines of code**, making it significantly easier for beginners to understand the internals of a modern LLM engine.

Despite its simplicity, it supports both online and offline inference and implements essential modern optimizations, including **Tensor Parallelism**, **Overlap Scheduling**, **Chunked Prefill**, **Radix Cache**, and **JIT CUDA kernels**. This makes it a comprehensive learning resource. For kernel developers, Mini-SGLang also provides fine-grained **NVTX annotations**, which are invaluable for kernel debugging and performance profiling.

### Quick Research Prototype

Many ML and systems researchers struggle to integrate their optimizations into existing systems. On one hand, injecting new logic into complex frameworks like SGLang is risky; it can break invariants and lead to subtle bugs. On the other hand, building an inference engine from scratch is tedious, requiring significant effort to handle infrastructure details (e.g., frontend servers, tokenization, NCCL communication) just to match state-of-the-art baselines.

Mini-SGLang strikes a balance. It offers an out-of-the-box, high-performance framework that is easy to extend and optimize. It handles the heavy lifting of infrastructure while remaining flexible enough for rapid prototyping. Additionally, Mini-SGLang provides **OpenAI-compatible benchmark utilities**, facilitating end-to-end performance analysis and comparison against various serving engines, such as [SGLang](https://github.com/sgl-project/sglang), [vLLM](https://github.com/vllm-project/vllm) and [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM).

## Features

Mini-SGLang has a similar system design to SGLang: frontend API server, tokenizer server, and one backend scheduler on each GPU.

![system-desing](/images/blog/minisgl/design.png)

### Overlap Scheduling

Similar to SGLang, we introduce overlap scheduling to hide CPU execution overhead. More details can be found in our previous [blog](https://lmsys.org/blog/2024-12-04-sglang-v0-4/). As illustrated in the profiling results, overlap scheduling can significantly reduce GPU bubbles and thereby improve the overall throughput.

![overlap](/images/blog/minisgl/overlap.png)
![no-overlap](/images/blog/minisgl/no-overlap.png)

### Attention Integration

Mini-SGLang has integrated [FlashAttention3](https://github.com/Dao-AILab/flash-attention) and [FlashInfer](https://github.com/flashinfer-ai/flashinfer) for our attention backend. By default, Mini-SGLang will use FlashAttention3 for prefill attention, and Flashinfer for decode attention on Hopper GPUs.

### Shell mode

Mini-SGLang also provide as simple shell mode so that users can interact with LLMs directly through a shell intereface.

![alt text](/images/blog/minisgl/shell.png)

## Benchmark

To evaluate the performance of Mini-SGLang, we conducted comprehensive experiments covering both offline throughput and online serving latency.

### Offline Inference

Following the methodology of [nano-vllm](https://github.com/GeeeekExplorer/nano-vllm/), we benchmarked offline inference throughput using the [Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B/) model. We also extended the evaluation to the larger [Qwen3-14B](https://huggingface.co/Qwen/Qwen3-14B/) model to assess scalability. We compared Mini-SGLang against nano-vllm and vLLM on a single NVIDIA H200 GPU. Note that we focused on Qwen3 models as the nano-vllm baseline currently supports only this series.

The throughput results (in tokens per second) are shown below:

| Model | Output Tokens | Mini-SGLang | nano-vllm | vLLM  | Mini-SGLang w/o overlap |
| ----- | ------------- | ---------------------- | -------------------- | --- | - |
| Qwen3-0.6B | 133,966 | **20,711** | 19,638 | 18,014 | 15,014 |
| Qwen3-14B  | 133,966 | **5,811** | 5,560 | 5,541 | 5,314 |

Consistent with nano-vllm's findings, we observed that nano-vllm holds a slight edge over vLLM on smaller models, likely due to vLLM's higher CPU scheduling overhead. This gap narrows as model size increases and GPU execution becomes the bottleneck. Mini-SGLang consistently outperforms all baselines. By leveraging **overlap scheduling** alongside high-performance attention kernels, Mini-SGLang effectively hides CPU overhead, delivering the highest throughput across both model sizes.

Reproduce: The offline benchmark script is available through [this link](https://github.com/DarkSharpness/mini-sglang/blob/main/benchmark/offline/bench_nanovllm.py). Overlap scheduling can be disabled for ablation studies by setting `export MINISGL_DISABLE_OVERLAP_SCHEDULING=1`.

### Online Serving

To assess real-world serving performance, we benchmarked Mini-SGLang against SGLang using the [Qwen trace](https://github.com/alibaba-edu/qwen-bailian-usagetraces-anon/blob/main/qwen_traceA_blksz_16.jsonl). We replayed 1,000 requests and measured the 90th percentile (P90) of Time Between Tokens (TBT) and Time To First Token (TTFT). We use [Llama3.1-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct) as our base model for benchmark. We deploy each engine with 8-way Tensor Parallelism on 8 H200 GPUs.

| System | Throughput (token/s)  | P90 TTFT (ms) | P90 TBT (ms) |
| -----  | ------------- | ---------------------- | -------------------- |
| Mini-SGLang | 2165 | 725 | 43.88 |
| SGLang      | 2149 | 744 | 45.11 |

Reproducability: Here's the launch command for both systems:

```bash
# mini-sglang
python -m minisgl --model "meta-llama/Llama-3.1-70B-Instruct" --tp 8 --cache naive
# sglang
python3 -m sglang.launch_server --model "meta-llama/Llama-3.1-70B-Instruct" --tp 8 --disable-radix --port 1919
```

## Conclusion
