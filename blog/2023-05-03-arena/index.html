<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings | LMSYS Org</title><meta name="title" content="Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings | LMSYS Org"/><meta property="og:title" content="Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings | LMSYS Org"/><meta name="twitter:title" content="Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings | LMSYS Org"/><meta name="description" content="&lt;p&gt;We present Chatbot Arena, a benchmark platform for large language models (LLMs) that features anonymous, randomized battles in a crowdsourced manner. In t..."/><meta property="og:description" content="&lt;p&gt;We present Chatbot Arena, a benchmark platform for large language models (LLMs) that features anonymous, randomized battles in a crowdsourced manner. In t..."/><meta name="twitter:description" content="&lt;p&gt;We present Chatbot Arena, a benchmark platform for large language models (LLMs) that features anonymous, randomized battles in a crowdsourced manner. In t..."/><meta property="og:image" content="https://lmsys.org/images/blog/arena/cover.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/arena/cover.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2023-05-03-arena"/><meta name="twitter:url" content="https://lmsys.org/blog/2023-05-03-arena"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-5289d222aec181f4.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/JBIEFQ8z002ecfiaSkF33/_buildManifest.js" defer=""></script><script src="/_next/static/JBIEFQ8z002ecfiaSkF33/_ssgManifest.js" defer=""></script><script src="/_next/static/JBIEFQ8z002ecfiaSkF33/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings</h1><p class="text-xl pt-2 pb-2">by: <!-- -->Lianmin Zheng*, Ying Sheng*, Wei-Lin Chiang, Hao Zhang, Joseph E. Gonzalez, Ion Stoica<!-- -->,<!-- --> <!-- -->May 03, 2023<!-- --></p><hr/><div class="pt-2 article"><p>We present Chatbot Arena, a benchmark platform for large language models (LLMs) that features anonymous, randomized battles in a crowdsourced manner. In this blog post, we are releasing our initial results and a leaderboard based on the Elo rating system, which is a widely-used rating system in chess and other competitive games. We invite the entire community to join this effort by contributing new models and evaluating them by asking questions and voting for your favorite answer.</p>
<style>
th {text-align: left}
td {text-align: left}
</style>
<br>
<p style="color:gray; text-align: center;">Table 1. LLM Leaderboard (Timeframe: April 24 - May 1, 2023). The latest and detailed version <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard" target="_blank">here</a>.</p>
<table style="display: flex; justify-content: center;" align="left" >
<tbody>
<tr>
<th>Rank</th> <th>Model</th> <th>Elo Rating</th> <th>Description</th>
</tr>
<tr>
<td>1</td> <td>ðŸ¥‡ <a href="https://lmsys.org/blog/2023-03-30-vicuna/" target="_blank">vicuna-13b</a></td> <td>1169</td> <td>a chat assistant fine-tuned from LLaMA on user-shared conversations by LMSYS</td>
</tr>
<tr>
<td>2</td> <td>ðŸ¥ˆ <a href="https://bair.berkeley.edu/blog/2023/04/03/koala" target="_blank">koala-13b</a></td> <td>1082</td> <td>a dialogue model for academic research by BAIR</td>
</tr>
<tr>
<td>3</td> <td>ðŸ¥‰ <a href="https://open-assistant.io" target="_blank">oasst-pythia-12b</a></td> <td>1065</td> <td>an Open Assistant for everyone by LAION</td>
</tr>
<tr>
<td>4</td> <td><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" target="_blank">alpaca-13b</a></td> <td>1008</td> <td>a model fine-tuned from LLaMA on instruction-following demonstrations by Stanford</td>
</tr>
<tr>
<td>5</td> <td><a href="https://chatglm.cn/blog" target="_blank">chatglm-6b</a></td> <td>985</td> <td>an open bilingual dialogue language model by Tsinghua University</td>
</tr>
<tr>
<td>6</td> <td><a href="https://huggingface.co/lmsys/fastchat-t5-3b-v1.0" target="_blank">fastchat-t5-3b</a></td> <td>951</td> <td>a chat assistant fine-tuned from FLAN-T5 by LMSYS</td>
</tr>
<tr>
<td>7</td> <td><a href="https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm" target="_blank">dolly-v2-12b</a></td> <td>944</td> <td>an instruction-tuned open large language model by Databricks</td>
</tr>
<tr>
<td>8</td> <td><a href="https://arxiv.org/abs/2302.13971" target="_blank">llama-13b</a></td> <td>932</td> <td>open and efficient foundation language models by Meta</td>
</tr>
<tr>
<td>9</td> <td><a href="https://github.com/stability-AI/stableLM" target="_blank">stablelm-tuned-alpha-7b</a></td> <td>858</td> <td>Stability AI language models</td>
</tr>
</tbody>
</table>
<p>Â­</p>
<p>Table 1 displays the Elo ratings of nine popular models, which are based on the 4.7K voting data and calculations shared in this <a href="https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing">notebook</a>. You can also try the voting <a href="https://lmarena.ai">demo</a>.</p>
<p><img src="/images/blog/arena/chat_demo.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;"></img></p>
<p style="color:gray; text-align: center;">Figure 1. The side-by-side chatting and voting interface.</p>
<p>Please note that we periodically release blog posts to update the leaderboard. Feel free to check the following updates:</p>
<ul>
<li><a href="https://lmsys.org/blog/2023-05-10-leaderboard/">May 10 Updates</a></li>
<li><a href="https://lmsys.org/blog/2023-05-25-leaderboard/">May 25 Updates</a></li>
<li><a href="https://lmsys.org/blog/2023-06-22-leaderboard/">June 22 Updates</a></li>
<li><a href="https://lmsys.org/blog/2023-07-20-dataset/">Dataset Release (July 20)</a></li>
<li><a href="https://lmsys.org/blog/2023-12-07-leaderboard/">Dec. 7 Updates</a></li>
<li><a href="https://lmsys.org/blog/2024-03-01-policy/">Policy Updates (March 1, 2024)</a></li>
</ul>
<h2><a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h2>
<p>Following the great success of ChatGPT, there has been a proliferation of open-source large language models that are finetuned to follow instructions. These models are capable of providing valuable assistance in response to usersâ€™ questions/prompts. Notable examples include Alpaca and Vicuna, based on LLaMA, and OpenAssistant and Dolly, based on Pythia.</p>
<p>Despite the constant release of new models every week, the community faces a challenge in benchmarking these models effectively. Benchmarking LLM assistants is extremely challenging because the problems can be open-ended, and it is very difficult to write a program to automatically evaluate the response quality.
In this case, we typically have to resort to human evaluation based on pairwise comparison.</p>
<p>There are some desired properties for a good benchmark system based on pairwise comparison.</p>
<ul>
<li><strong>Scalability</strong>. The system should scale to a large number of models when it is not feasible to collect sufficient data for all possible model pairs.</li>
<li><strong>Incrementality</strong>. The system should be able to evaluate a new model using a relatively small number of trials.</li>
<li><strong>Unique order</strong>. The system should provide a unique order for all models. Given any two models, we should be able to tell which ranks higher or whether they are tied.</li>
</ul>
<p>Existing LLM benchmark systems rarely satisfy all of these properties. Classical LLM benchmark frameworks, such as <a href="https://crfm.stanford.edu/helm/latest/">HELM</a> and <a href="https://github.com/EleutherAI/lm-evaluation-harness">lm-evaluation-harness</a>, provide multi-metric measurements for tasks commonly used in academic research. However, they are not based on pairwise comparison and are not effective at evaluating open-ended questions. OpenAI also launched the <a href="https://github.com/openai/evals">evals</a> project to collect better questions, but this project does not provide ranking mechanisms for all participating models. When we launched our <a href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a> model, we utilized a GPT-4-based evaluation pipeline, but it does not provide a solution for scalable and incremental ratings.</p>
<p>In this blog post, we introduce Chatbot Arena, an LLM benchmark platform featuring anonymous randomized battles in a crowdsourced manner. Chatbot Arena adopts the <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo rating system</a>, which is a widely-used rating system in chess and other competitive games. The Elo rating system is promising to provide the desired property mentioned above. We noticed that the <a href="https://arxiv.org/pdf/2204.05862.pdf">Anthropic LLM paper</a> also adopted the Elo rating system.</p>
<p>To collect data, we launched the arena with several popular open-source LLMs one week ago. In the arena, a user can chat with two anonymous models side-by-side and vote for which one is better. This crowdsourcing way of data collection represents some use cases of LLMs in the wild. A comparison between several evaluation methods is shown in Table 2.</p>
<br>
<p style="color:gray; text-align: center;">Table 2: Comparison between different evaluation methods.</p>
<div style="display: flex; justify-content: center; min-width: 700px;">
<table>
<tbody>
<tr>
<th></th> <th>HELM / lm-evaluation-harness</th> <th>OpenAI/eval</th> <th>Alpaca Evaluation</th> <th>Vicuna Evaluation</th> <th>Chatbot Arena</th>
</tr>
<tr>
<td><strong>Question Source</strong></td> <td>Academic datasets</td> <td>Mixed</td> <td>Self-instruct evaluation set</td> <td>GPT-4 generated</td> <td>User prompts</td>
</tr>
<tr>
<td><strong>Evaluator</strong></td> <td>Program</td> <td>Program/Model</td> <td>Human</td> <td>GPT-4</td> <td>User</td>
</tr>
<tr>
<td><strong>Metrics</strong></td> <td>Basic metrics </td> <td>Basic metrics</td> <td>Win rate</td> <td>Win rate</td> <td>Elo ratings</td>
</tr>
</tbody>
</table>
</div>
<h2><a id="data-collection" class="anchor" href="#data-collection" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data Collection</h2>
<p>We hosted the arena at <a href="https://lmarena.ai">https://lmarena.ai</a> with our multi-model serving system, <a href="https://github.com/lm-sys/FastChat">FastChat</a>. When a user enters the arena, they can chat with two anonymous models side-by-side, as shown in Figure 1.
After getting responses from the two models, users can continue chatting or vote for the model they think is better. Once a vote is submitted, the model names will be revealed. Users can continue chatting or restart a new battle with two new randomly chosen anonymous models. The platform logs all user interactions. In our analysis, we only use the votes when the model names are hidden.</p>
<p>The arena was launched about one week ago and we have collected 4.7k valid anonymous votes since then.  We share some exploratory analysis in this <a href="https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing">notebook</a> and present a short summary here.</p>
<p><img src="/images/blog/arena/battle_counts.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 60%"></img></p>
<p style="color:gray; text-align: center;">Figure 2: Battle count of each combination of models</p>
<p>Figure 2 shows the battles count of each combination of models. When we initially launched the tournament, we had prior information on the likely ranking based on our benchmarks and chose to pair models according to this ranking. We gave preference to what we believed would be strong pairings based on this ranking. However, we later switched to uniform sampling to get better overall coverage of the rankings. Towards the end of the tournament, we also introduced a new model <code>fastchat-t5-3b</code>. All of these result in non-uniform model frequency.</p>
<p><img src="/images/blog/arena/lang_counts.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 80%"></img></p>
<p style="color:gray; text-align: center;">Figure 3: Battle counts for the top-15 languages.</p>
<p>Figure 3 plots the language distribution and shows most user prompts are in English.</p>
<h2><a id="elo-rating-system" class="anchor" href="#elo-rating-system" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Elo Rating System</h2>
<p>The <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo rating system</a> is a method for calculating the relative skill levels of players, which has been widely adopted in competitive games and sports. The difference in the ratings between two players serves as a predictor of the outcome of a match. The Elo rating system works well for our case because we have multiple models and we run pairwise battles between them.</p>
<p>If player A has a rating of <code>Ra</code> and player B a rating of <code>Rb</code>, the exact formula (using the logistic curve with base 10) for the probability of player A winning is</p>
<p><img src=" https://wikimedia.org/api/rest_v1/media/math/render/svg/7c80282e9c95e92d6b210467aab48a8c4c81ef10" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;"></img></p>
<p>The ratings of players can be linearly updated after each battle. Suppose player A (with Rating <code>Ra</code>) was expected to score <code>Ea</code> points but actucally scored <code>Sa</code> points. The formula for updating that player's rating is</p>
<p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1cad9fb1cfc6a8e845493ac9a40eb98541a4641a" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;"></img></p>
<p>Using the collected data, we compute the Elo ratings of the models in this <a href="https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing">notebook</a> and put the main results in Table 1. You are welcome to try the notebook and play with the voting data by yourself. The data only contains voting results without conversation histories because releasing the conversation history will raise concerns such as privacy and toxicity.</p>
<h2><a id="pairwise-win-rates" class="anchor" href="#pairwise-win-rates" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Pairwise Win Rates</h2>
<p>As a basis for calibration, we also present here the pairwise win rates for each model in the tournament (Figure 4) as well as the predicted pairwise win rate estimated using Elo ratings (Figure 5).
By comparing the figures, we find the elo ratings can predict win rates relatively well.</p>
<p><img src="/images/blog/arena/win_fraction.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;"></img></p>
<p style="color:gray; text-align: center;">Figure 4: Fraction of Model A wins for all non-tied A vs. B battles.</p>
<p><img src="/images/blog/arena/predicted_win_fraction.png" style="display:block; margin-left: auto; margin-right: auto; margin-bottom: auto;"></img></p>
<p style="color:gray; text-align: center;">Figure 5: Predicted win rate using Elo ratings for Model A in an A vs. B battle</p>
<h2><a id="future-plans" class="anchor" href="#future-plans" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Future Plans</h2>
<p>We plan to work on the following items:</p>
<ul>
<li>Add more closed-source models (ChatGPT-3.5, ChatGPT-4, and Claude-v1 are avaiable now in the anonymous Arena)</li>
<li>Add more open-source models</li>
<li>Release periodically updated leaderboards (e.g., monthly)</li>
<li>Implement better sampling algorithms, tournament mechanisms, and serving systems to support a much larger number of models</li>
<li>Provide fine-grained rankings on different task types.</li>
</ul>
<p>We appreciate any feedback from you to make the arena better.</p>
<h2><a id="join-us" class="anchor" href="#join-us" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Join Us</h2>
<p>We invite the entire community to join this benchmarking effort by contributing your models and votes for the anonymous models you think provide better answers. You can visit <a href="https://lmarena.ai">https://lmarena.ai</a> to vote for better models. If you want to see a specific model in the arena, you can follow this <a href="https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model">guide</a> to help us add it.</p>
<h2><a id="acknowledgment" class="anchor" href="#acknowledgment" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgment</h2>
<p>We thank other members of the Vicuna team for valuable feedback and MBZUAI for donating compute resources. Additionally, we extend our thanks to Tianjun Zhang and Eric Wallace for their insightful discussions.</p>
<h2><a id="links" class="anchor" href="#links" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Links</h2>
<ul>
<li>Demo: <a href="https://lmarena.ai">https://lmarena.ai</a></li>
<li>Leaderboard: <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard</a></li>
<li>GitHub: <a href="https://github.com/lm-sys/FastChat">https://github.com/lm-sys/FastChat</a></li>
<li>Colab notebook: <a href="https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing">https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing</a></li>
</ul>
<h2><a id="citation" class="anchor" href="#citation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Citation</h2>
<p>Please cite the following <a href="https://arxiv.org/abs/2403.04132">papers</a> if you find our work useful.</p>
<pre><code class="hljs"><span class="hljs-comment">@misc{chiang2024chatbot,</span>
    title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},
    author={Wei-Lin Chiang <span class="hljs-keyword">and</span> Lianmin Zheng <span class="hljs-keyword">and</span> Ying Sheng <span class="hljs-keyword">and</span> Anastasios Nikolas Angelopoulos <span class="hljs-keyword">and</span> Tianle Li <span class="hljs-keyword">and</span> Dacheng Li <span class="hljs-keyword">and</span> Hao Zhang <span class="hljs-keyword">and</span> Banghua Zhu <span class="hljs-keyword">and</span> Michael Jordan <span class="hljs-keyword">and</span> Joseph E. Gonzalez <span class="hljs-keyword">and</span> Ion Stoica},
    year={<span class="hljs-number">2024</span>},
    eprint={<span class="hljs-number">2403</span>.<span class="hljs-number">04132</span>},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

<span class="hljs-comment">@inproceedings{zheng2023judging,</span>
    title={Judging LLM-as-a-Judge with MT-Bench <span class="hljs-keyword">and</span> Chatbot Arena},
    author={Lianmin Zheng <span class="hljs-keyword">and</span> Wei-Lin Chiang <span class="hljs-keyword">and</span> Ying Sheng <span class="hljs-keyword">and</span> Siyuan Zhuang <span class="hljs-keyword">and</span> Zhanghao Wu <span class="hljs-keyword">and</span> Yonghao Zhuang <span class="hljs-keyword">and</span> Zi Lin <span class="hljs-keyword">and</span> Zhuohan Li <span class="hljs-keyword">and</span> Dacheng Li <span class="hljs-keyword">and</span> Eric Xing <span class="hljs-keyword">and</span> Hao Zhang <span class="hljs-keyword">and</span> Joseph E. Gonzalez <span class="hljs-keyword">and</span> Ion Stoica},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets <span class="hljs-keyword">and</span> Benchmarks Track},
    year={<span class="hljs-number">2023</span>},
    url={https:<span class="hljs-comment">//openreview.net/forum?id=uccHPGDlao}</span>
}

<span class="hljs-comment">@inproceedings{zheng2024lmsyschatm,</span>
    title={LMSYS-Chat-<span class="hljs-number">1</span>M: A Large-Scale Real-World LLM Conversation Dataset},
    author={Lianmin Zheng <span class="hljs-keyword">and</span> Wei-Lin Chiang <span class="hljs-keyword">and</span> Ying Sheng <span class="hljs-keyword">and</span> Tianle Li <span class="hljs-keyword">and</span> Siyuan Zhuang <span class="hljs-keyword">and</span> Zhanghao Wu <span class="hljs-keyword">and</span> Yonghao Zhuang <span class="hljs-keyword">and</span> Zhuohan Li <span class="hljs-keyword">and</span> Zi Lin <span class="hljs-keyword">and</span> Eric Xing <span class="hljs-keyword">and</span> Joseph E. Gonzalez <span class="hljs-keyword">and</span> Ion Stoica <span class="hljs-keyword">and</span> Hao Zhang},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={<span class="hljs-number">2024</span>},
    url={https:<span class="hljs-comment">//openreview.net/forum?id=BOfDKxfwt0}</span>
}
</code></pre>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings","author":"Lianmin Zheng*, Ying Sheng*, Wei-Lin Chiang, Hao Zhang, Joseph E. Gonzalez, Ion Stoica","date":"May 3, 2023","previewImg":"/images/blog/arena/cover.png"},"content":"\r\nWe present Chatbot Arena, a benchmark platform for large language models (LLMs) that features anonymous, randomized battles in a crowdsourced manner. In this blog post, we are releasing our initial results and a leaderboard based on the Elo rating system, which is a widely-used rating system in chess and other competitive games. We invite the entire community to join this effort by contributing new models and evaluating them by asking questions and voting for your favorite answer.\r\n\r\n\u003cstyle\u003e\r\nth {text-align: left}\r\ntd {text-align: left}\r\n\u003c/style\u003e\r\n\r\n\u003cbr\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 1. LLM Leaderboard (Timeframe: April 24 - May 1, 2023). The latest and detailed version \u003ca href=\"https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\r\n\u003ctable style=\"display: flex; justify-content: center;\" align=\"left\" \u003e\r\n\u003ctbody\u003e\r\n\u003ctr\u003e\r\n\u003cth\u003eRank\u003c/th\u003e \u003cth\u003eModel\u003c/th\u003e \u003cth\u003eElo Rating\u003c/th\u003e \u003cth\u003eDescription\u003c/th\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e1\u003c/td\u003e \u003ctd\u003eðŸ¥‡ \u003ca href=\"https://lmsys.org/blog/2023-03-30-vicuna/\" target=\"_blank\"\u003evicuna-13b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1169\u003c/td\u003e \u003ctd\u003ea chat assistant fine-tuned from LLaMA on user-shared conversations by LMSYS\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e2\u003c/td\u003e \u003ctd\u003eðŸ¥ˆ \u003ca href=\"https://bair.berkeley.edu/blog/2023/04/03/koala\" target=\"_blank\"\u003ekoala-13b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1082\u003c/td\u003e \u003ctd\u003ea dialogue model for academic research by BAIR\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e3\u003c/td\u003e \u003ctd\u003eðŸ¥‰ \u003ca href=\"https://open-assistant.io\" target=\"_blank\"\u003eoasst-pythia-12b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1065\u003c/td\u003e \u003ctd\u003ean Open Assistant for everyone by LAION\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e4\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\" target=\"_blank\"\u003ealpaca-13b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1008\u003c/td\u003e \u003ctd\u003ea model fine-tuned from LLaMA on instruction-following demonstrations by Stanford\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e5\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://chatglm.cn/blog\" target=\"_blank\"\u003echatglm-6b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e985\u003c/td\u003e \u003ctd\u003ean open bilingual dialogue language model by Tsinghua University\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e6\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://huggingface.co/lmsys/fastchat-t5-3b-v1.0\" target=\"_blank\"\u003efastchat-t5-3b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e951\u003c/td\u003e \u003ctd\u003ea chat assistant fine-tuned from FLAN-T5 by LMSYS\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e7\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\" target=\"_blank\"\u003edolly-v2-12b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e944\u003c/td\u003e \u003ctd\u003ean instruction-tuned open large language model by Databricks\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e8\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://arxiv.org/abs/2302.13971\" target=\"_blank\"\u003ellama-13b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e932\u003c/td\u003e \u003ctd\u003eopen and efficient foundation language models by Meta\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e9\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://github.com/stability-AI/stableLM\" target=\"_blank\"\u003establelm-tuned-alpha-7b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e858\u003c/td\u003e \u003ctd\u003eStability AI language models\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\r\n\u0026shy;\r\n\r\nTable 1 displays the Elo ratings of nine popular models, which are based on the 4.7K voting data and calculations shared in this [notebook](https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing). You can also try the voting [demo](https://lmarena.ai).\r\n\r\n\u003cimg src=\"/images/blog/arena/chat_demo.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 1. The side-by-side chatting and voting interface.\u003c/p\u003e\r\n\r\nPlease note that we periodically release blog posts to update the leaderboard. Feel free to check the following updates:\r\n- [May 10 Updates](https://lmsys.org/blog/2023-05-10-leaderboard/)\r\n- [May 25 Updates](https://lmsys.org/blog/2023-05-25-leaderboard/)\r\n- [June 22 Updates](https://lmsys.org/blog/2023-06-22-leaderboard/)\r\n- [Dataset Release (July 20)](https://lmsys.org/blog/2023-07-20-dataset/)\r\n- [Dec. 7 Updates](https://lmsys.org/blog/2023-12-07-leaderboard/)\r\n- [Policy Updates (March 1, 2024)](https://lmsys.org/blog/2024-03-01-policy/)\r\n\r\n## Introduction\r\nFollowing the great success of ChatGPT, there has been a proliferation of open-source large language models that are finetuned to follow instructions. These models are capable of providing valuable assistance in response to usersâ€™ questions/prompts. Notable examples include Alpaca and Vicuna, based on LLaMA, and OpenAssistant and Dolly, based on Pythia.\r\n\r\nDespite the constant release of new models every week, the community faces a challenge in benchmarking these models effectively. Benchmarking LLM assistants is extremely challenging because the problems can be open-ended, and it is very difficult to write a program to automatically evaluate the response quality.\r\nIn this case, we typically have to resort to human evaluation based on pairwise comparison.\r\n\r\nThere are some desired properties for a good benchmark system based on pairwise comparison.\r\n- **Scalability**. The system should scale to a large number of models when it is not feasible to collect sufficient data for all possible model pairs.\r\n- **Incrementality**. The system should be able to evaluate a new model using a relatively small number of trials.\r\n- **Unique order**. The system should provide a unique order for all models. Given any two models, we should be able to tell which ranks higher or whether they are tied.\r\n\r\nExisting LLM benchmark systems rarely satisfy all of these properties. Classical LLM benchmark frameworks, such as [HELM](https://crfm.stanford.edu/helm/latest/) and [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), provide multi-metric measurements for tasks commonly used in academic research. However, they are not based on pairwise comparison and are not effective at evaluating open-ended questions. OpenAI also launched the [evals](https://github.com/openai/evals) project to collect better questions, but this project does not provide ranking mechanisms for all participating models. When we launched our [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) model, we utilized a GPT-4-based evaluation pipeline, but it does not provide a solution for scalable and incremental ratings.\r\n\r\nIn this blog post, we introduce Chatbot Arena, an LLM benchmark platform featuring anonymous randomized battles in a crowdsourced manner. Chatbot Arena adopts the [Elo rating system](https://en.wikipedia.org/wiki/Elo_rating_system), which is a widely-used rating system in chess and other competitive games. The Elo rating system is promising to provide the desired property mentioned above. We noticed that the [Anthropic LLM paper](https://arxiv.org/pdf/2204.05862.pdf) also adopted the Elo rating system.\r\n\r\nTo collect data, we launched the arena with several popular open-source LLMs one week ago. In the arena, a user can chat with two anonymous models side-by-side and vote for which one is better. This crowdsourcing way of data collection represents some use cases of LLMs in the wild. A comparison between several evaluation methods is shown in Table 2.\r\n\r\n\u003cbr\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 2: Comparison between different evaluation methods.\u003c/p\u003e\r\n\u003cdiv style=\"display: flex; justify-content: center; min-width: 700px;\"\u003e\r\n\u003ctable\u003e\r\n\u003ctbody\u003e\r\n\u003ctr\u003e\r\n\u003cth\u003e\u003c/th\u003e \u003cth\u003eHELM / lm-evaluation-harness\u003c/th\u003e \u003cth\u003eOpenAI/eval\u003c/th\u003e \u003cth\u003eAlpaca Evaluation\u003c/th\u003e \u003cth\u003eVicuna Evaluation\u003c/th\u003e \u003cth\u003eChatbot Arena\u003c/th\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e\u003cstrong\u003eQuestion Source\u003c/strong\u003e\u003c/td\u003e \u003ctd\u003eAcademic datasets\u003c/td\u003e \u003ctd\u003eMixed\u003c/td\u003e \u003ctd\u003eSelf-instruct evaluation set\u003c/td\u003e \u003ctd\u003eGPT-4 generated\u003c/td\u003e \u003ctd\u003eUser prompts\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e\u003cstrong\u003eEvaluator\u003c/strong\u003e\u003c/td\u003e \u003ctd\u003eProgram\u003c/td\u003e \u003ctd\u003eProgram/Model\u003c/td\u003e \u003ctd\u003eHuman\u003c/td\u003e \u003ctd\u003eGPT-4\u003c/td\u003e \u003ctd\u003eUser\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e\u003cstrong\u003eMetrics\u003c/strong\u003e\u003c/td\u003e \u003ctd\u003eBasic metrics \u003c/td\u003e \u003ctd\u003eBasic metrics\u003c/td\u003e \u003ctd\u003eWin rate\u003c/td\u003e \u003ctd\u003eWin rate\u003c/td\u003e \u003ctd\u003eElo ratings\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\u003c/div\u003e\r\n\r\n## Data Collection\r\nWe hosted the arena at [https://lmarena.ai](https://lmarena.ai) with our multi-model serving system, [FastChat](https://github.com/lm-sys/FastChat). When a user enters the arena, they can chat with two anonymous models side-by-side, as shown in Figure 1.\r\nAfter getting responses from the two models, users can continue chatting or vote for the model they think is better. Once a vote is submitted, the model names will be revealed. Users can continue chatting or restart a new battle with two new randomly chosen anonymous models. The platform logs all user interactions. In our analysis, we only use the votes when the model names are hidden.\r\n\r\nThe arena was launched about one week ago and we have collected 4.7k valid anonymous votes since then.  We share some exploratory analysis in this [notebook](https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing) and present a short summary here.\r\n\r\n\u003cimg src=\"/images/blog/arena/battle_counts.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 60%\"\u003e\u003c/img\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 2: Battle count of each combination of models\u003c/p\u003e\r\n\r\nFigure 2 shows the battles count of each combination of models. When we initially launched the tournament, we had prior information on the likely ranking based on our benchmarks and chose to pair models according to this ranking. We gave preference to what we believed would be strong pairings based on this ranking. However, we later switched to uniform sampling to get better overall coverage of the rankings. Towards the end of the tournament, we also introduced a new model `fastchat-t5-3b`. All of these result in non-uniform model frequency.\r\n\r\n\u003cimg src=\"/images/blog/arena/lang_counts.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 80%\"\u003e\u003c/img\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 3: Battle counts for the top-15 languages.\u003c/p\u003e\r\n\r\nFigure 3 plots the language distribution and shows most user prompts are in English.\r\n\r\n## Elo Rating System\r\nThe [Elo rating system](https://en.wikipedia.org/wiki/Elo_rating_system) is a method for calculating the relative skill levels of players, which has been widely adopted in competitive games and sports. The difference in the ratings between two players serves as a predictor of the outcome of a match. The Elo rating system works well for our case because we have multiple models and we run pairwise battles between them.\r\n\r\nIf player A has a rating of `Ra` and player B a rating of `Rb`, the exact formula (using the logistic curve with base 10) for the probability of player A winning is\r\n\r\n\u003cimg src=\" https://wikimedia.org/api/rest_v1/media/math/render/svg/7c80282e9c95e92d6b210467aab48a8c4c81ef10\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\r\n\r\nThe ratings of players can be linearly updated after each battle. Suppose player A (with Rating `Ra`) was expected to score `Ea` points but actucally scored `Sa` points. The formula for updating that player's rating is \r\n\r\n\u003cimg src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1cad9fb1cfc6a8e845493ac9a40eb98541a4641a\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\r\n\r\nUsing the collected data, we compute the Elo ratings of the models in this [notebook](https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing) and put the main results in Table 1. You are welcome to try the notebook and play with the voting data by yourself. The data only contains voting results without conversation histories because releasing the conversation history will raise concerns such as privacy and toxicity.\r\n\r\n## Pairwise Win Rates\r\nAs a basis for calibration, we also present here the pairwise win rates for each model in the tournament (Figure 4) as well as the predicted pairwise win rate estimated using Elo ratings (Figure 5).\r\nBy comparing the figures, we find the elo ratings can predict win rates relatively well.\r\n\r\n\u003cimg src=\"/images/blog/arena/win_fraction.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 4: Fraction of Model A wins for all non-tied A vs. B battles.\u003c/p\u003e\r\n\r\n\u003cimg src=\"/images/blog/arena/predicted_win_fraction.png\" style=\"display:block; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 5: Predicted win rate using Elo ratings for Model A in an A vs. B battle\u003c/p\u003e\r\n\r\n## Future Plans\r\nWe plan to work on the following items:\r\n- Add more closed-source models (ChatGPT-3.5, ChatGPT-4, and Claude-v1 are avaiable now in the anonymous Arena)\r\n- Add more open-source models\r\n- Release periodically updated leaderboards (e.g., monthly)\r\n- Implement better sampling algorithms, tournament mechanisms, and serving systems to support a much larger number of models\r\n- Provide fine-grained rankings on different task types.\r\n\r\nWe appreciate any feedback from you to make the arena better.\r\n\r\n## Join Us\r\nWe invite the entire community to join this benchmarking effort by contributing your models and votes for the anonymous models you think provide better answers. You can visit [https://lmarena.ai](https://lmarena.ai) to vote for better models. If you want to see a specific model in the arena, you can follow this [guide](https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model) to help us add it.\r\n\r\n## Acknowledgment\r\nWe thank other members of the Vicuna team for valuable feedback and MBZUAI for donating compute resources. Additionally, we extend our thanks to Tianjun Zhang and Eric Wallace for their insightful discussions.\r\n\r\n## Links\r\n- Demo: [https://lmarena.ai](https://lmarena.ai)\r\n- Leaderboard: [https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)\r\n- GitHub: [https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat)\r\n- Colab notebook: [https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing](https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing)\r\n\r\n## Citation\r\nPlease cite the following [papers](https://arxiv.org/abs/2403.04132) if you find our work useful.\r\n\r\n```\r\n@misc{chiang2024chatbot,\r\n    title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},\r\n    author={Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica},\r\n    year={2024},\r\n    eprint={2403.04132},\r\n    archivePrefix={arXiv},\r\n    primaryClass={cs.AI}\r\n}\r\n\r\n@inproceedings{zheng2023judging,\r\n    title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena},\r\n    author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},\r\n    booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\r\n    year={2023},\r\n    url={https://openreview.net/forum?id=uccHPGDlao}\r\n}\r\n\r\n@inproceedings{zheng2024lmsyschatm,\r\n    title={LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset},\r\n    author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Tianle Li and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zhuohan Li and Zi Lin and Eric Xing and Joseph E. Gonzalez and Ion Stoica and Hao Zhang},\r\n    booktitle={The Twelfth International Conference on Learning Representations},\r\n    year={2024},\r\n    url={https://openreview.net/forum?id=BOfDKxfwt0}\r\n}\r\n```\r\n","slug":"2023-05-03-arena"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2023-05-03-arena"},"buildId":"JBIEFQ8z002ecfiaSkF33","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>