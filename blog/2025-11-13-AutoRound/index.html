<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>ðŸš€ AutoRound Meets SGLang: Enabling Quantized Model Inference with AutoRound | LMSYS Org</title><meta name="title" content="ðŸš€ AutoRound Meets SGLang: Enabling Quantized Model Inference with AutoRound | LMSYS Org"/><meta property="og:title" content="ðŸš€ AutoRound Meets SGLang: Enabling Quantized Model Inference with AutoRound | LMSYS Org"/><meta name="twitter:title" content="ðŸš€ AutoRound Meets SGLang: Enabling Quantized Model Inference with AutoRound | LMSYS Org"/><meta name="description" content="&lt;h2&gt;&lt;a id=&quot;overview&quot; class=&quot;anchor&quot; href=&quot;#overview&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewbo..."/><meta property="og:description" content="&lt;h2&gt;&lt;a id=&quot;overview&quot; class=&quot;anchor&quot; href=&quot;#overview&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewbo..."/><meta name="twitter:description" content="&lt;h2&gt;&lt;a id=&quot;overview&quot; class=&quot;anchor&quot; href=&quot;#overview&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewbo..."/><meta property="og:image" content="https://lmsys.org/images/blog/AutoRound/preview.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/AutoRound/preview.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2025-11-13-AutoRound"/><meta name="twitter:url" content="https://lmsys.org/blog/2025-11-13-AutoRound"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-eef2afd147d8eda9.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/433Rw7afKHCar-Gdg6TNW/_buildManifest.js" defer=""></script><script src="/_next/static/433Rw7afKHCar-Gdg6TNW/_ssgManifest.js" defer=""></script><script src="/_next/static/433Rw7afKHCar-Gdg6TNW/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">ðŸš€ AutoRound Meets SGLang: Enabling Quantized Model Inference with AutoRound</h1><p class="text-xl pt-2 pb-2">by: <!-- -->By Intel Neural Compressor Team<!-- -->,<!-- --> <!-- -->Nov 14, 2025<!-- --></p><hr/><div class="pt-2 article"><h2><a id="overview" class="anchor" href="#overview" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Overview</h2>
<p>We are thrilled to announce an official collaboration between <a href="https://github.com/sgl-project/sglang"><strong>SGLang</strong></a> and <a href="https://github.com/intel/auto-round"><strong>AutoRound</strong></a>, enabling low-bit quantization for efficient LLM inference.</p>
<p>Through this integration, developers can now quantize large models with AutoRoundâ€™s signed-gradient optimization and directly deploy them in SGLangâ€™s efficient runtime, achieving low-bit model inference with minimal accuracy loss and significant latency reduction.</p>
<h2><a id="what-is-autoround" class="anchor" href="#what-is-autoround" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What Is AutoRound?</h2>
<p>AutoRound is an advanced post-training quantization (PTQ) toolkit designed for Large Language Models (<strong>LLMs</strong>) and Vision-Language Models (<strong>VLMs</strong>).  It uses signed gradient descent to jointly optimize weight rounding and clipping ranges, enabling accurate low-bit quantization (e.g., INT2 - INT8) with minimal accuracy loss in most scenarios. For example, at INT2 precision, it outperforms popular baselines by up to 2.1x higher in relative accuracy. At INT4 precision, AutoRound continues to hold a competitive edge in most cases. The image below provides an overview of the core algorithm in AutoRound.</p>
<p>Full technical details are presented in the AutoRound paper:</p>
<p>ðŸ‘‰ <a href="https://arxiv.org/abs/2309.05516">Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs</a></p>
<p align="center">
  <img src="/images/blog/AutoRound/autoround_overview.png" width="80%">
</p>
<p align="center" style="color:gray; text-align: center;"><em>AutoRound algorithm overview</em></p>
<p>Despite its robust performance, AutoRound remains fast and lightweightâ€”quantizing a 72B model takes only 37 minutes on a single GPU under light mode.</p>
<p>It further supports mixed-bit tuning, lm-head quantization, GPTQ/AWQ/GGUF format exports, and customizable tuning recipes.</p>
<h2><a id="autoround-highlights" class="anchor" href="#autoround-highlights" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>AutoRound Highlights</h2>
<p>AutoRound is not only focused on algorithmic innovation and exploration, but also widely recognized for its completeness in quantization engineering.</p>
<ul>
<li><strong>Accuracy:</strong> deliver superior accuracy at low-bit precision</li>
</ul>
<p align="center">
  <img src="/images/blog/AutoRound/int4_accs.png" width="80%">
</p>
<p align="center" style="color:gray; text-align: center;"><em>Average accuracy of 10+ tasks at INT4 weight</em></p>
<ul>
<li><strong>Schemes:</strong> support weight-only quantization, weight &amp; activation quantization, dynamic and static for activation quantization</li>
<li><strong>Mixed-bits:</strong> propose an effective algorithm to generate mixed-bits / other data types schemes in minutes</li>
<li><strong>Broad Compatibility:</strong>
<ul>
<li>Support nearly all popular LLM architectures and over 10 vision-language models (VLMs)</li>
<li>Support Devices: CPU, Intel GPU, CUDA</li>
<li>Support Data Types: INT2 - INT8, MXFP4, NVFP4, FP8, and MXFP8</li>
</ul>
</li>
<li><strong>Efficiency:</strong> Enables block-wise tuning to lower VRAM usage without sacrificing throughput yet fast</li>
</ul>
<p align="center">
  <img src="/images/blog/AutoRound/timecost.png" width="80%"> 
</p>
<p align="center" style="color:gray; text-align: center;"><em>Quantization time cost comparison</em></p>
<ul>
<li><strong>Community adoption:</strong>
<ul>
<li>Work seamlessly with SGLang, TorchAO, Transformers, and vLLM</li>
<li>Widely used by HuggingFace model hubs such as <a href="https://huggingface.co/Intel">Intel</a>, <a href="https://huggingface.co/OPEA">OPEA</a>,  <a href="https://huggingface.co/kaitchup">Kaitchup</a>, and <a href="https://huggingface.co/fbaldassarri">fbaldassarri</a> with approximately two million downloads</li>
</ul>
</li>
<li><strong>Export Formats:</strong>
<ul>
<li>AutoRound</li>
<li>GPTQ</li>
<li>AWQ</li>
<li>GGUF</li>
<li>Compressed-tensor (initial support)</li>
</ul>
</li>
</ul>
<h2><a id="integration-overview" class="anchor" href="#integration-overview" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Integration Overview</h2>
<p>SGLang provides a next-generation inference runtime built for scalable, low-latency LLM deployment. Its multi-modal, multi-GPU, and streaming execution model enables both chat and agentic reasoning workloads with exceptional efficiency.</p>
<p>SGLangâ€™s flexible architecture now offers native hooks for quantized model loading, unlocking AutoRoundâ€™s full potential for deployment.</p>
<h3><a id="1-quantize-with-autoround" class="anchor" href="#1-quantize-with-autoround" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>1. Quantize with AutoRound</strong></h3>
<p>AutoRound automatically optimizes weight rounding and exports quantized weights that compatible with SGLang.</p>
<h4><a id="11-api-usage" class="anchor" href="#11-api-usage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>1.1 API Usage</strong></h4>
<pre><code class="hljs language-python"><span class="hljs-comment"># for LLM</span>
<span class="hljs-keyword">from</span> auto_round <span class="hljs-keyword">import</span> AutoRound
model_id = <span class="hljs-string">&quot;meta-llama/Llama-3.2-1B-Instruct&quot;</span>
quant_path = <span class="hljs-string">&quot;Llama-3.2-1B-Instruct-autoround-4bit&quot;</span>
<span class="hljs-comment"># Scheme examples: &quot;W2A16&quot;, &quot;W3A16&quot;, &quot;W4A16&quot;, &quot;W8A16&quot;, &quot;NVFP4&quot;, &quot;MXFP4&quot; (no real kernels), &quot;GGUF:Q4_K_M&quot;, etc.</span>
scheme = <span class="hljs-string">&quot;W4A16&quot;</span>
<span class="hljs-built_in">format</span> = <span class="hljs-string">&quot;auto_round&quot;</span>
autoround = AutoRound(model_id, scheme=scheme)
autoround.quantize_and_save(quant_path, <span class="hljs-built_in">format</span>=<span class="hljs-built_in">format</span>) <span class="hljs-comment"># quantize and save</span>
</code></pre>
<h4><a id="12-cmd-usage" class="anchor" href="#12-cmd-usage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>1.2 CMD Usage</strong></h4>
<pre><code class="hljs language-bash">auto-round \
    --model Qwen/Qwen2-VL-2B-Instruct \
    --bits 4 \
    --group_size 128 \
    --format <span class="hljs-string">&quot;auto_round&quot;</span> \
    --output_dir ./tmp_autoround
</code></pre>
<h3><a id="2-deploying-with-sglang" class="anchor" href="#2-deploying-with-sglang" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>2. Deploying with SGLang</strong></h3>
<p>SGLang supports AutoRound-quantized models directly (Version&gt;=v0.5.4.post2). It is compatible with SGLang-supported modeling architectures, including common LLM, VLM, and MoE models, and also supports inference and evaluation of AutoRound mixed-bit quantized models.</p>
<h4><a id="21-openai-compatible-inference-usage" class="anchor" href="#21-openai-compatible-inference-usage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>2.1 OpenAI-Compatible Inference Usage</strong></h4>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> sglang.test.doc_patch <span class="hljs-keyword">import</span> launch_server_cmd
<span class="hljs-keyword">from</span> sglang.utils <span class="hljs-keyword">import</span> wait_for_server, print_highlight, terminate_process

<span class="hljs-comment"># This is equivalent to running the following command in your terminal</span>
<span class="hljs-comment"># python3 -m sglang.launch_server --model-path Intel/DeepSeek-R1-0528-Qwen3-8B-int4-AutoRound --host 0.0.0.0</span>

server_process, port = launch_server_cmd(
    <span class="hljs-string">&quot;&quot;&quot;
python3 -m sglang.launch_server --model-path Intel/DeepSeek-R1-0528-Qwen3-8B-int4-AutoRound \
 --host 0.0.0.0 --log-level warning
&quot;&quot;&quot;</span>
)
wait_for_server(<span class="hljs-string">f&quot;http://localhost:<span class="hljs-subst">{port}</span>&quot;</span>)
</code></pre>
<h4><a id="22-offline-engine-api-inference-usage" class="anchor" href="#22-offline-engine-api-inference-usage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>2.2 Offline Engine API Inference Usage</strong></h4>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> sglang <span class="hljs-keyword">as</span> sgl

llm = sgl.Engine(model_path=<span class="hljs-string">&quot;Intel/DeepSeek-R1-0528-Qwen3-8B-int4-AutoRound&quot;</span>)

prompts = [<span class="hljs-string">&quot;Hello, my name is&quot;</span>]
sampling_params = {<span class="hljs-string">&quot;temperature&quot;</span>: <span class="hljs-number">0.6</span>, <span class="hljs-string">&quot;top_p&quot;</span>: <span class="hljs-number">0.95</span>}

outputs = llm.generate(prompts, sampling_params)
<span class="hljs-keyword">for</span> prompt, output <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prompts, outputs):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Prompt: <span class="hljs-subst">{prompt}</span>\nGenerated text: <span class="hljs-subst">{output[<span class="hljs-string">&#x27;text&#x27;</span>]}</span>&quot;</span>)
</code></pre>
<p>More flexible configurations and deployment options are waiting for you to explore!</p>
<h2><a id="quantization-roadmap" class="anchor" href="#quantization-roadmap" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Quantization Roadmap</h2>
<p>AutoRoundâ€™s quantization benchmark results demonstrate robust accuracy retention at low precision. The results below highlight AutoRoundâ€™s strong advantages and potential in MXFP4, NVFP4, and mixed-bits model quantization. Note that the accuracy result is measured by average accuracy across <em>lambada_openai</em>, <em>hellaswag</em>, <em>piqa</em>, <em>winogrande</em>, and <em>mmlu</em> task.</p>
<p>As part of AutoRound roadmap, we plan to continue enhancing MXFP4 &amp; NVFP4 accuracy for common models and auto mixed-bits quantization in the upcoming releases.</p>
<ul>
<li>
<p>MXFP4 &amp; NVFP4 Quantization. RTN (Round-to-nearest) algorithm is baseline, and <em>'alg_ext'</em> option indicates experimental optimization algorithms enabled.</p>
<table>
<thead>
<tr>
<th style="text-align:left">mxfp4</th>
<th style="text-align:center">llama3.1-8B-Instruct</th>
<th style="text-align:center">Qwen2-7.5-Instruct</th>
<th style="text-align:center">Phi4</th>
<th style="text-align:center">Qwen3-32B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">RTN</td>
<td style="text-align:center">0.6212</td>
<td style="text-align:center">0.6550</td>
<td style="text-align:center">0.7167</td>
<td style="text-align:center">0.6901</td>
</tr>
<tr>
<td style="text-align:left">AutoRound</td>
<td style="text-align:center">0.6686</td>
<td style="text-align:center">0.6758</td>
<td style="text-align:center">0.7247</td>
<td style="text-align:center">0.7211</td>
</tr>
<tr>
<td style="text-align:left">AutoRound+alg_ext</td>
<td style="text-align:center">0.6732</td>
<td style="text-align:center">0.6809</td>
<td style="text-align:center">0.7225</td>
<td style="text-align:center">0.7201</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:left">nvfp4</th>
<th style="text-align:center">llama3.1-8B-Instruct</th>
<th style="text-align:center">Qwen2-7.5-Instruct</th>
<th style="text-align:center">Phi4</th>
<th style="text-align:center">Qwen3-32B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">RTN</td>
<td style="text-align:center">0.6876</td>
<td style="text-align:center">0.6906</td>
<td style="text-align:center">0.7296</td>
<td style="text-align:center">0.7164</td>
</tr>
<tr>
<td style="text-align:left">AutoRound</td>
<td style="text-align:center">0.6918</td>
<td style="text-align:center">0.6973</td>
<td style="text-align:center">0.7306</td>
<td style="text-align:center">0.7306</td>
</tr>
<tr>
<td style="text-align:left">AutoRound+alg_ext</td>
<td style="text-align:center">0.6965</td>
<td style="text-align:center">0.6989</td>
<td style="text-align:center">0.7318</td>
<td style="text-align:center">0.7295</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Auto MXFP4 &amp; MXFP8 Mixed-Bits Quantization</p>
<table>
<thead>
<tr>
<th style="text-align:left">Average bits</th>
<th style="text-align:center">Llama3.1-8B-I</th>
<th style="text-align:center">Qwen2.5-7B-I</th>
<th style="text-align:center">Qwen3-8B</th>
<th style="text-align:center">Qwen3-32B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>BF16</strong></td>
<td style="text-align:center">0.7076 (100%)</td>
<td style="text-align:center">0.7075 (100%)</td>
<td style="text-align:center">0.6764 (100%)</td>
<td style="text-align:center">0.7321 (100%)</td>
</tr>
<tr>
<td style="text-align:left"><strong>4-bit</strong></td>
<td style="text-align:center">0.6626 (93.6%)</td>
<td style="text-align:center">0.6550 (92.6%)</td>
<td style="text-align:center">0.6316 (93.4%)</td>
<td style="text-align:center">0.6901 (94.3%)</td>
</tr>
<tr>
<td style="text-align:left"><strong>4.5-bit</strong></td>
<td style="text-align:center">0.6808 (96.2%)</td>
<td style="text-align:center">0.6776 (95.8%)</td>
<td style="text-align:center">0.6550 (96.8%)</td>
<td style="text-align:center">0.7176 (98.0%)</td>
</tr>
<tr>
<td style="text-align:left"><strong>5-bit</strong></td>
<td style="text-align:center">0.6857 (96.9%)</td>
<td style="text-align:center">0.6823 (96.4%)</td>
<td style="text-align:center">0.6594 (97.5%)</td>
<td style="text-align:center">0.7201 (98.3%)</td>
</tr>
<tr>
<td style="text-align:left"><strong>6-bit</strong></td>
<td style="text-align:center">0.6975 (98.6%)</td>
<td style="text-align:center">0.6970 (98.5%)</td>
<td style="text-align:center">0.6716 (99.3%)</td>
<td style="text-align:center">0.7303 (99.8%)</td>
</tr>
</tbody>
</table>
</li>
</ul>
<h2><a id="conclusion" class="anchor" href="#conclusion" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>The integration of AutoRound and SGLang marks a major milestone in efficient AI model deployment. This collaboration bridges precision optimization and runtime scalability, allowing developers to move seamlessly from quantization to real-time inference with minimal friction. AutoRoundâ€™s signed-gradient quantization ensures high fidelity even at extreme compression ratios, while SGLangâ€™s high-throughput inference engine unlocks the full potential of low-bit execution across CPUs, GPUs, and multi-node clusters.</p>
<p>Looking forward, we aim to expand support for advanced quantization formats, optimize kernel efficiency, and bring AutoRound quantization into** **broader multimodal and agentic workloads. Together, AutoRound and SGLang are setting a new standard for intelligent, efficient, and scalable LLM deployment.</p>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"ðŸš€ AutoRound Meets SGLang: Enabling Quantized Model Inference with AutoRound","author":"By Intel Neural Compressor Team","date":"November 14, 2025","previewImg":"/images/blog/AutoRound/preview.png"},"content":"\n## Overview\n\nWe are thrilled to announce an official collaboration between [**SGLang**](https://github.com/sgl-project/sglang) and [**AutoRound**](https://github.com/intel/auto-round), enabling low-bit quantization for efficient LLM inference.\n\nThrough this integration, developers can now quantize large models with AutoRoundâ€™s signed-gradient optimization and directly deploy them in SGLangâ€™s efficient runtime, achieving low-bit model inference with minimal accuracy loss and significant latency reduction.\n\n\n## What Is AutoRound?\n\nAutoRound is an advanced post-training quantization (PTQ) toolkit designed for Large Language Models (**LLMs**) and Vision-Language Models (**VLMs**).  It uses signed gradient descent to jointly optimize weight rounding and clipping ranges, enabling accurate low-bit quantization (e.g., INT2 - INT8) with minimal accuracy loss in most scenarios. For example, at INT2 precision, it outperforms popular baselines by up to 2.1x higher in relative accuracy. At INT4 precision, AutoRound continues to hold a competitive edge in most cases. The image below provides an overview of the core algorithm in AutoRound.\n\nFull technical details are presented in the AutoRound paper:\n\nðŸ‘‰ [Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs](https://arxiv.org/abs/2309.05516)\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/AutoRound/autoround_overview.png\" width=\"80%\"\u003e\n\u003c/p\u003e\n\u003cp align=\"center\" style=\"color:gray; text-align: center;\"\u003e\u003cem\u003eAutoRound algorithm overview\u003c/em\u003e\u003c/p\u003e\n\nDespite its robust performance, AutoRound remains fast and lightweightâ€”quantizing a 72B model takes only 37 minutes on a single GPU under light mode.\n\nIt further supports mixed-bit tuning, lm-head quantization, GPTQ/AWQ/GGUF format exports, and customizable tuning recipes.\n\n\n\n## AutoRound Highlights\n\nAutoRound is not only focused on algorithmic innovation and exploration, but also widely recognized for its completeness in quantization engineering.\n\n- **Accuracy:** deliver superior accuracy at low-bit precision\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/AutoRound/int4_accs.png\" width=\"80%\"\u003e\n\u003c/p\u003e\n\u003cp align=\"center\" style=\"color:gray; text-align: center;\"\u003e\u003cem\u003eAverage accuracy of 10+ tasks at INT4 weight\u003c/em\u003e\u003c/p\u003e\n\n- **Schemes:** support weight-only quantization, weight \u0026 activation quantization, dynamic and static for activation quantization\n- **Mixed-bits:** propose an effective algorithm to generate mixed-bits / other data types schemes in minutes\n- **Broad Compatibility:**\n  - Support nearly all popular LLM architectures and over 10 vision-language models (VLMs)\n  - Support Devices: CPU, Intel GPU, CUDA\n  - Support Data Types: INT2 - INT8, MXFP4, NVFP4, FP8, and MXFP8\n- **Efficiency:** Enables block-wise tuning to lower VRAM usage without sacrificing throughput yet fast\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/AutoRound/timecost.png\" width=\"80%\"\u003e \n\u003c/p\u003e\n\u003cp align=\"center\" style=\"color:gray; text-align: center;\"\u003e\u003cem\u003eQuantization time cost comparison\u003c/em\u003e\u003c/p\u003e\n\n- **Community adoption:** \n  - Work seamlessly with SGLang, TorchAO, Transformers, and vLLM\n  - Widely used by HuggingFace model hubs such as [Intel](https://huggingface.co/Intel), [OPEA](https://huggingface.co/OPEA),  [Kaitchup](https://huggingface.co/kaitchup), and [fbaldassarri](https://huggingface.co/fbaldassarri) with approximately two million downloads\n- **Export Formats:**\n  - AutoRound\n  - GPTQ\n  - AWQ\n  - GGUF\n  - Compressed-tensor (initial support)\n\n\n## Integration Overview\n\nSGLang provides a next-generation inference runtime built for scalable, low-latency LLM deployment. Its multi-modal, multi-GPU, and streaming execution model enables both chat and agentic reasoning workloads with exceptional efficiency.\n\nSGLangâ€™s flexible architecture now offers native hooks for quantized model loading, unlocking AutoRoundâ€™s full potential for deployment.\n\n### **1. Quantize with AutoRound**\n\nAutoRound automatically optimizes weight rounding and exports quantized weights that compatible with SGLang.\n\n#### **1.1 API Usage**\n\n```python\n# for LLM\nfrom auto_round import AutoRound\nmodel_id = \"meta-llama/Llama-3.2-1B-Instruct\"\nquant_path = \"Llama-3.2-1B-Instruct-autoround-4bit\"\n# Scheme examples: \"W2A16\", \"W3A16\", \"W4A16\", \"W8A16\", \"NVFP4\", \"MXFP4\" (no real kernels), \"GGUF:Q4_K_M\", etc.\nscheme = \"W4A16\"\nformat = \"auto_round\"\nautoround = AutoRound(model_id, scheme=scheme)\nautoround.quantize_and_save(quant_path, format=format) # quantize and save\n```\n\n#### **1.2 CMD Usage**\n```bash\nauto-round \\\n    --model Qwen/Qwen2-VL-2B-Instruct \\\n    --bits 4 \\\n    --group_size 128 \\\n    --format \"auto_round\" \\\n    --output_dir ./tmp_autoround\n```\n\n### **2. Deploying with SGLang**\n\nSGLang supports AutoRound-quantized models directly (Version\u003e=v0.5.4.post2). It is compatible with SGLang-supported modeling architectures, including common LLM, VLM, and MoE models, and also supports inference and evaluation of AutoRound mixed-bit quantized models.\n\n#### **2.1 OpenAI-Compatible Inference Usage**\n\n```python\nfrom sglang.test.doc_patch import launch_server_cmd\nfrom sglang.utils import wait_for_server, print_highlight, terminate_process\n\n# This is equivalent to running the following command in your terminal\n# python3 -m sglang.launch_server --model-path Intel/DeepSeek-R1-0528-Qwen3-8B-int4-AutoRound --host 0.0.0.0\n\nserver_process, port = launch_server_cmd(\n    \"\"\"\npython3 -m sglang.launch_server --model-path Intel/DeepSeek-R1-0528-Qwen3-8B-int4-AutoRound \\\n --host 0.0.0.0 --log-level warning\n\"\"\"\n)\nwait_for_server(f\"http://localhost:{port}\")\n```\n\n#### **2.2 Offline Engine API Inference Usage**\n\n\n```python\nimport sglang as sgl\n\nllm = sgl.Engine(model_path=\"Intel/DeepSeek-R1-0528-Qwen3-8B-int4-AutoRound\")\n\nprompts = [\"Hello, my name is\"]\nsampling_params = {\"temperature\": 0.6, \"top_p\": 0.95}\n\noutputs = llm.generate(prompts, sampling_params)\nfor prompt, output in zip(prompts, outputs):\n    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")\n```\n\nMore flexible configurations and deployment options are waiting for you to explore!\n\n\n## Quantization Roadmap\n\nAutoRoundâ€™s quantization benchmark results demonstrate robust accuracy retention at low precision. The results below highlight AutoRoundâ€™s strong advantages and potential in MXFP4, NVFP4, and mixed-bits model quantization. Note that the accuracy result is measured by average accuracy across *lambada_openai*, *hellaswag*, *piqa*, *winogrande*, and *mmlu* task.\n\nAs part of AutoRound roadmap, we plan to continue enhancing MXFP4 \u0026 NVFP4 accuracy for common models and auto mixed-bits quantization in the upcoming releases.\n\n- MXFP4 \u0026 NVFP4 Quantization. RTN (Round-to-nearest) algorithm is baseline, and _'alg_ext'_ option indicates experimental optimization algorithms enabled.\n\n    | mxfp4    | llama3.1-8B-Instruct | Qwen2-7.5-Instruct | Phi4    | Qwen3-32B |\n    |:-------------------|:----------------------:|:--------------------:|:---------:|:-----------:|\n    | RTN               | 0.6212               | 0.6550            | 0.7167 | 0.6901   |\n    | AutoRound         | 0.6686               | 0.6758            | 0.7247 | 0.7211   |\n    | AutoRound+alg_ext | 0.6732               | 0.6809            | 0.7225 | 0.7201   |\n\n\n    | nvfp4   | llama3.1-8B-Instruct | Qwen2-7.5-Instruct | Phi4    | Qwen3-32B |\n    |:-------------------|:----------------------:|:--------------------:|:---------:|:-----------:|\n    | RTN               | 0.6876              | 0.6906             | 0.7296 | 0.7164      |\n    | AutoRound         | 0.6918              | 0.6973             | 0.7306 | 0.7306      |\n    | AutoRound+alg_ext | 0.6965              | 0.6989             | 0.7318  | 0.7295     |\n\n\n-  Auto MXFP4 \u0026 MXFP8 Mixed-Bits Quantization\n\n    | Average bits     | Llama3.1-8B-I  | Qwen2.5-7B-I   | Qwen3-8B       | Qwen3-32B      |\n    |:------------------|:----------------:|:----------------:|:----------------:|:----------------:|\n    | **BF16**         | 0.7076 (100%)  | 0.7075 (100%)  | 0.6764 (100%)  | 0.7321 (100%)  |\n    | **4-bit**   | 0.6626 (93.6%) | 0.6550 (92.6%) | 0.6316 (93.4%) | 0.6901 (94.3%) |\n    | **4.5-bit** | 0.6808 (96.2%) | 0.6776 (95.8%) | 0.6550 (96.8%) | 0.7176 (98.0%) |\n    | **5-bit**   | 0.6857 (96.9%) | 0.6823 (96.4%) | 0.6594 (97.5%) | 0.7201 (98.3%) |\n    | **6-bit**   | 0.6975 (98.6%) | 0.6970 (98.5%) | 0.6716 (99.3%) | 0.7303 (99.8%) |\n\n\n\n## Conclusion\n\nThe integration of AutoRound and SGLang marks a major milestone in efficient AI model deployment. This collaboration bridges precision optimization and runtime scalability, allowing developers to move seamlessly from quantization to real-time inference with minimal friction. AutoRoundâ€™s signed-gradient quantization ensures high fidelity even at extreme compression ratios, while SGLangâ€™s high-throughput inference engine unlocks the full potential of low-bit execution across CPUs, GPUs, and multi-node clusters.\n\nLooking forward, we aim to expand support for advanced quantization formats, optimize kernel efficiency, and bring AutoRound quantization into** **broader multimodal and agentic workloads. Together, AutoRound and SGLang are setting a new standard for intelligent, efficient, and scalable LLM deployment.\n","slug":"2025-11-13-AutoRound"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2025-11-13-AutoRound"},"buildId":"433Rw7afKHCar-Gdg6TNW","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>