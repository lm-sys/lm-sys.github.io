<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>Let Tensors Fly ‚Äî Accelerating Large Model Weight Loading with R-Fork | LMSYS Org</title><meta name="title" content="Let Tensors Fly ‚Äî Accelerating Large Model Weight Loading with R-Fork | LMSYS Org"/><meta property="og:title" content="Let Tensors Fly ‚Äî Accelerating Large Model Weight Loading with R-Fork | LMSYS Org"/><meta name="twitter:title" content="Let Tensors Fly ‚Äî Accelerating Large Model Weight Loading with R-Fork | LMSYS Org"/><meta name="description" content="&lt;h2&gt;&lt;a id=&quot;tldr&quot; class=&quot;anchor&quot; href=&quot;#tldr&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 1..."/><meta property="og:description" content="&lt;h2&gt;&lt;a id=&quot;tldr&quot; class=&quot;anchor&quot; href=&quot;#tldr&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 1..."/><meta name="twitter:description" content="&lt;h2&gt;&lt;a id=&quot;tldr&quot; class=&quot;anchor&quot; href=&quot;#tldr&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 1..."/><meta property="og:image" content="https://lmsys.org/images/blog/rfork/preview.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/rfork/preview.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2025-12-10-rfork"/><meta name="twitter:url" content="https://lmsys.org/blog/2025-12-10-rfork"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d62cc293bc63f5ee.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/rTodAa4cLK7cUPkwXDz07/_buildManifest.js" defer=""></script><script src="/_next/static/rTodAa4cLK7cUPkwXDz07/_ssgManifest.js" defer=""></script><script src="/_next/static/rTodAa4cLK7cUPkwXDz07/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.io" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">Let Tensors Fly ‚Äî Accelerating Large Model Weight Loading with R-Fork</h1><p class="text-xl pt-2 pb-2">by: <!-- -->Ant Group DeepXPU Team, SGLang Team<!-- -->,<!-- --> <!-- -->Dec 10, 2025<!-- --></p><hr/><div class="pt-2 article"><h2><a id="tldr" class="anchor" href="#tldr" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TL;DR</h2>
<blockquote>
<p>We introduce <strong>Tensor R-Fork</strong> (stands for Tensor Remote Fork), a novel weight loading methodology that leverages <strong>efficient inter-node GPU-to-GPU data transfer path</strong> to load tensors from a running SGLang instance to a new instance with <strong>zero-copy</strong>.</p>
</blockquote>
<p>Our approach provides three key advantages:</p>
<ol>
<li>Significantly accelerates weight-loading performance;</li>
<li>Eliminates redundant model weight storage on local disk and/or DRAM;</li>
<li>Ensures non-disturbing operation for inference services.</li>
</ol>
<p>For instance, when applied to Deepseek-R1 model, the loading time is reduced <strong>from several minutes to mere seconds</strong>, while local disk and/or DRAM storage usage is <strong>reduced by ~600GB</strong> and inference service quality maintains during model transfers.</p>
<h2><a id="background" class="anchor" href="#background" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Background</h2>
<p>As the scale LLM services and the size of model weights continue to expand, the cold-start time of SGLang instances has become a critical bottleneck for production efficiency. Among the cold-start phases, weight loading remains the most time-consuming task.</p>
<p>Taking Deepseek-R1 as an example, loading weights from local disk typically takes several minutes, while loading from remote storage systems can take up to tens of minutes. As model sizes continue to grow exponentially, the time required for initialization and data transfer will likely worsen.</p>
<p>How can we optimize weight loading performance? The most straightforward approach is to maximize the bottleneck bandwidth in the weight data flow. The data flow of commonly-used model loading approaches in the industry and their associated bottleneck bandwidths are as follows:</p>
<table>
<thead>
<tr>
<th>Load weights from</th>
<th>Data Flow</th>
<th>Bottleneck</th>
</tr>
</thead>
<tbody>
<tr>
<td>remote storage center</td>
<td>remote storage -&gt; remote Ethernet NIC -&gt; Ethernet -&gt; local Ethernet NIC -&gt; local DRAM -&gt; local GPU memory</td>
<td>NVMe/Ethernet NIC</td>
</tr>
<tr>
<td>local disk</td>
<td>disk -&gt; DRAM -&gt; GPU memory</td>
<td>NVMe</td>
</tr>
<tr>
<td>local DRAM</td>
<td>DRAM -&gt; GPU memory</td>
<td>PCIe</td>
</tr>
</tbody>
</table>
<p>Can we exploit higher-bandwidth data flows for transferring tensors? The answer is <strong>YES</strong> ‚Äî InfiniBand offers hundreds of gigabytes per second of throughput. However, the critical question remains: How can we fully leverage InfiniBand's bandwidth for efficient weight loading in SGLang?</p>
<p>To address this challenge, we have developed <strong>a novel weight-loading framework called Tensor R-Fork</strong> (stands for Tensor Remote Fork), which reduces Deepseek-R1 model loading time to mere seconds and is already production-ready.</p>
<h2><a id="design" class="anchor" href="#design" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Design</h2>
<p>The core concept of <a href=https://github.com/sgl-project/sglang/blob/main/docs/advanced_features/rfork.md>Tensor R-Fork</a>[0] is to <strong>leverage GPU-Direct RDMA for constructing a peer-to-peer (P2P) weight storage architecture.</strong></p>
<p>The performance of data transfer using traditional method is low, because there is always bottleneck in the entire path, whose bandwidth is much smaller than InfiniBand.
From the data flow analysis, we observe that weight tensors are stored on each GPU and can be transmitted directly between nodes via GPU-direct RDMA.</p>
<p>To maximize the utilization of InfiniBand NIC's bandwidth, we design a per GPU-pair data transfer strategy: a local GPU directly transfers data to/from its paired remote GPU. This design effectively bypasses the PCIe bottleneck between GPU and CPU, enabling high-throughput communication without relying on CPU or host memory.
The data flow of loading weights from remote SGLang instance is the following:</p>
<p><img src="/images/blog/rfork/design.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 100%; image-orientation: none;"></img></p>
<h2><a id="implementation" class="anchor" href="#implementation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Implementation</h2>
<p>To make every running instances act as the source of model weights for any new instance requiring the same model‚Äîwhile minimizing (or ideally eliminating) disruption to the inference services of running instances‚Äîwe implemented the framework with two backend options: NCCL and TransferEngine. Consider a running instance A (referred to as the source instance) and a new instance B to be booted (destination instance). Below, we will explain the implementation of weight transfer mechanisms using these two backends in detail.</p>
<h3><a id="nccl-backend" class="anchor" href="#nccl-backend" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>NCCL backend</h3>
<p>When using <a href=https://github.com/sgl-project/sglang/pull/8215>NCCL</a> as the backend[1], the process involves two stages:</p>
<ol>
<li>Establishing communication groups between source and destination instances.</li>
<li>Transferring weights from the source instance to the destination instance via these groups.</li>
</ol>
<p>During destination instance initialization, it sends an HTTP request to the designated source instance to initiate communication group creation. Each TPWorker of destination instance establishes a NCCL communication group with its corresponding TPWorker of source instance (i.e.source rank 0 pairs with destination rank 0, etc.). Each communication group consists of exactly two members: the source TPWorker and destination TPWorker.</p>
<p>Once communication groups are established, each source TPWorker broadcasts its weights tensor located on GPU memory through the group using NCCL broadcast. The destination TPWorker receives the weights directly into its GPU memory without any intermediate memory copies.</p>
<p>While NCCL serves as Tensor R-Fork backend by leveraging GPU-Direct RDMA, it does have a critical limitation: weight transfer disrupts the source instance's inference service, due to two key factors:</p>
<ol>
<li><strong>Communication group establishment</strong>: The source instance must actively participate in creating communication groups.</li>
<li><strong>CUDA kernel interference</strong>: The NCCL broadcast mechanism triggers CUDA kernel execution, which competes for GPU resources and introduces latency spikes during generation tasks.</li>
</ol>
<h3><a id="transferengine-backend" class="anchor" href="#transferengine-backend" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TransferEngine backend</h3>
<p>To achieve non-disturbing weight transfer, we introduce an alternative backend: <a href=https://github.com/sgl-project/sglang/pull/14997>TransferEngine</a>, which leverages GPU-Direct RDMA for efficient data movement[2]. TransferEngine (TE) is a lightweight RDMA-based transfer runtime that runs alongside each TPWorker on the source instance and exposes GPU-resident weight tensors to remote readers without invoking CUDA kernels on the source.</p>
<p>During source SGLang instance initialization:</p>
<ol>
<li>Each TPWorker (tensor parallel worker) spawns a TransferEngine instance.</li>
<li>TransferEngine registers the GPU memory addresses of its weights with the RDMA channel.</li>
</ol>
<p>When initializing the destination instance:</p>
<ol>
<li>It sends an HTTP request to retrieve the source instance's TransferEngine metadata, including RDMA keys mapped to the corresponding GPU memory addresses.</li>
<li>Using these RDMA keys, the destination instance directly loads weights from the source's GPU memory without interrupting the source instance's ongoing services.</li>
</ol>
<p>*Want to learn more about TransferEngine? You are more than welcome to check <strong>TransferEngine</strong> in Appendix session üöÄ</p>
<h3><a id="nccl-vs-transferengine" class="anchor" href="#nccl-vs-transferengine" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>NCCL vs. TransferEngine</h3>
<table>
<thead>
<tr>
<th></th>
<th>NCCL</th>
<th>TransferEngine</th>
</tr>
</thead>
<tbody>
<tr>
<td>Deployment Complexity</td>
<td>‚úÖ No additional dependency.</td>
<td>‚ùå Additional library <code>mooncake</code> is needed.</td>
</tr>
<tr>
<td>Overhead of Transfer Setup</td>
<td>‚úÖ Building communication groups takes hundreds of milliseconds</td>
<td>‚ûñ Registering memory regions to RDMA channel may take several seconds, but can be overlapped with other initialization phases.</td>
</tr>
<tr>
<td>Non-disturbing to GPU workload</td>
<td>‚ùå Tensor transfer will launch CUDA kernels.</td>
<td>‚úÖ No CUDA kernels launched for transferring weights.</td>
</tr>
</tbody>
</table>
<h2><a id="how-to-use" class="anchor" href="#how-to-use" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How to Use</h2>
<p>Detailed usage please refer to <a href=https://github.com/sgl-project/sglang/blob/main/docs/advanced_features/rfork.md>R-Fork document</a></p>
<h3><a id="use-nccl-as-backend" class="anchor" href="#use-nccl-as-backend" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Use NCCL as backend</h3>
<p>seed instance:</p>
<pre><code class="hljs language-shell">python -m sglang.launch_server [args]
</code></pre>
<p>client instance:</p>
<pre><code class="hljs language-shell">python -m sglang.launch_server [args] \
  --load-format remote_instance	\
  --remote-instance-weight-loader-seed-instance-ip [seed_instance_ip] \
  --remote-instance-weight-loader-seed-instance-service-port [seed_instance_service_port] \
  --remote-instance-weight-loader-send-weights-group-ports [send_weights_nccl_group_ports_list]  \
  --remote-instance-weight-loader-backend nccl
</code></pre>
<h3><a id="use-transferengine-as-backend" class="anchor" href="#use-transferengine-as-backend" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Use TransferEngine as backend</h3>
<p>seed instance:</p>
<pre><code class="hljs language-shell">python -m sglang.launch_server [args] \
  --remote-instance-weight-loader-start-seed-via-transfer-engine
</code></pre>
<p>client instance:</p>
<pre><code class="hljs language-shell">python -m sglang.launch_server [args] \
  --load-format remote_instance	\
  --remote-instance-weight-loader-seed-instance-ip [seed_instance_ip] \
  --remote-instance-weight-loader-seed-instance-service-port [seed_instance_service_port] \
  --remote-instance-weight-loader-backend transfer_engine
</code></pre>
<h2><a id="performance" class="anchor" href="#performance" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Performance</h2>
<p>We evaluated the performance of launching a new SGLang instance equipped with eight NVIDIA H20 GPUs, while loading the DeepSeek-R1 model from different sources.</p>
<p><img src="/images/blog/rfork/performance.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 100%; image-orientation: none;"></img></p>
<p>Registering the memory region can be overlapped with other initialization phases to further optimize total boot-up time.</p>
<h2><a id="industrial-practice" class="anchor" href="#industrial-practice" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Industrial Practice</h2>
<p>In the previous sections, we demonstrated how to manually configure seed instances for Tensor R-Fork within SGLang server arguments. However, this manual approach is impractical in real-world industrial deployment, where identifying available seed instances requires significant operational overhead.</p>
<p>To address this challenge, we propose <strong><a href=https://github.com/sgl-project/sglang/issues/12910>Tensor R-Fork Planner</a></strong>[4], a cluster scheduler designed to orchestrate source instance metadata. The Planner tracks critical information, including:</p>
<ol>
<li><strong>Model compatibility</strong>: Which model is currently running on the instance.</li>
<li><strong>Parallelism configuration</strong>: The parallel strategy (e.g., tensor parallelism, pipeline parallelism) employed.</li>
<li><strong>Service health status</strong>: Whether the instance is healthy and suitable as a seed instance.</li>
</ol>
<p>Upon completion of its initialization, each instance registers itself with the Planner, providing its model metadata and parallelism configuration. When a new instance boots up, it first queries the Planner to identify an eligible seed instance that matches both its model and parallelism strategy. If a compatible seed instance is found, the new instance loads weights directly from the seed; otherwise, it falls back to the default load format.</p>
<h2><a id="future-work" class="anchor" href="#future-work" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Future Work</h2>
<p>The practice of R-Fork opens up more imaginative possibilities: the key concept of R-Fork is that it enables all SGLang instances to act as data storage centers for other instances. Starting from weight tensors, we will manage additional tensors through Tensor R-Fork mechanism in the future, allowing GPU clusters to function not only as computing centers but also as storage centers.</p>
<h2><a id="acknowledgements" class="anchor" href="#acknowledgements" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgements</h2>
<p><strong>Ant Group DeepXPU Team</strong>: Anqi Shen, Tianyu Zhou, Zehuan Li, Tiwei Bie, Mingliang Gong, Jianfeng Tan</p>
<p><strong>SGLang Team</strong>: Chenyang Zhao, Liangsheng Yin, Lianmin Zheng</p>
<p><strong>TransferEngine Team</strong>: Teng Ma, Feng Ren, Shangming Cai</p>
<h2><a id="reference" class="anchor" href="#reference" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Reference</h2>
<p>[0] Tensor R-Fork Documentation: <a href=https://github.com/sgl-project/sglang/blob/main/docs/advanced_features/rfork.md>Documentation</a><br>
[1] Tensor R-Fork with NCCL backend: <a href=https://github.com/sgl-project/sglang/pull/8215>PR#8215</a><br>
[2] Tensor R-Fork with TransferEngine backend: <a href=https://github.com/sgl-project/sglang/pull/14997>PR#14997</a><br>
[3] Concurrent weights loading from disk: <a href=https://github.com/sgl-project/sglang/pull/7943>PR#7943</a><br>
[4] Tensor R-Fork Planner SGLang RFC: <a href=https://github.com/sgl-project/sglang/issues/12910>Issue#12910</a><br>
[5] TransferEngine: <a href=https://kvcache-ai.github.io/Mooncake/design/transfer-engine.html>https://kvcache-ai.github.io/Mooncake/design/transfer-engine.html</a>,<br>
TransferEngine APIs: <a href=https://kvcache-ai.github.io/Mooncake/python-api-reference/transfer-engine.html>https://kvcache-ai.github.io/Mooncake/python-api-reference/transfer-engine.html</a></p>
<h2><a id="appendix" class="anchor" href="#appendix" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Appendix</h2>
<h3><a id="transferengine" class="anchor" href="#transferengine" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TransferEngine</h3>
<p>Key advantages provided by TransferEngine[5]:</p>
<ul>
<li><strong>Multi-backend support</strong>: TE supports multiple backends, including RDMA (with GPUDirect), NVLink, GDS, and TCP. It can intelligently identify the best backend per request, so that the highest performance could be reached.</li>
<li><strong>Direct RDMA reads</strong>: using the published addresses and rkeys, the destination performs RDMA operations (typically RDMA READ) directly into its own pre-allocated GPU buffers, leveraging GPU-Direct RDMA so that no host-device or device-host intermediate copies are required.</li>
<li><strong>Non-disturbing</strong>: TE performs pure NIC-driven transfers that avoid launching CUDA kernels on the source GPU.</li>
<li><strong>Lifecycle &amp; housekeeping</strong>: TE maintains the lifetime of registrations until tensors are evicted or the process exits.</li>
<li><strong>Concurrency &amp; flow control</strong>: TE coordinates concurrent reads (from one or many destinations) and can apply throttling or rate limits to avoid saturating instance‚Äôs NIC or impacting inference latency.</li>
</ul>
<p>Known limitation in the current TransferEngine implementation:</p>
<ul>
<li><strong>Memory registration (register_mr) is slow</strong>: <u>This is due to the RDMA driver</u>. If you have any insights or solutions to this issue, we would be truly grateful to hear from you. We value diverse perspectives and are keen to explore innovative approaches together.</li>
</ul>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Let Tensors Fly ‚Äî Accelerating Large Model Weight Loading with R-Fork","author":"Ant Group DeepXPU Team, SGLang Team","date":"December 10, 2025","previewImg":"/images/blog/rfork/preview.png"},"content":"\r\n## TL;DR\r\n\r\n\u003e We introduce **Tensor R-Fork** (stands for Tensor Remote Fork), a novel weight loading methodology that leverages **efficient inter-node GPU-to-GPU data transfer path** to load tensors from a running SGLang instance to a new instance with **zero-copy**.\r\n\r\nOur approach provides three key advantages:\r\n\r\n1. Significantly accelerates weight-loading performance;\r\n2. Eliminates redundant model weight storage on local disk and/or DRAM;\r\n3. Ensures non-disturbing operation for inference services.\r\n\r\nFor instance, when applied to Deepseek-R1 model, the loading time is reduced **from several minutes to mere seconds**, while local disk and/or DRAM storage usage is **reduced by ~600GB** and inference service quality maintains during model transfers.\r\n\r\n## Background\r\nAs the scale LLM services and the size of model weights continue to expand, the cold-start time of SGLang instances has become a critical bottleneck for production efficiency. Among the cold-start phases, weight loading remains the most time-consuming task.\r\n\r\nTaking Deepseek-R1 as an example, loading weights from local disk typically takes several minutes, while loading from remote storage systems can take up to tens of minutes. As model sizes continue to grow exponentially, the time required for initialization and data transfer will likely worsen.\r\n\r\nHow can we optimize weight loading performance? The most straightforward approach is to maximize the bottleneck bandwidth in the weight data flow. The data flow of commonly-used model loading approaches in the industry and their associated bottleneck bandwidths are as follows:\r\n\r\n| Load weights from     | Data Flow                                                | Bottleneck |\r\n|-----------------------|----------------------------------------------------------|------------|\r\n| remote storage center | remote storage -\u003e remote Ethernet NIC -\u003e Ethernet -\u003e local Ethernet NIC -\u003e local DRAM -\u003e local GPU memory   |            NVMe/Ethernet NIC |\r\n| local disk            | disk -\u003e DRAM -\u003e GPU memory            | NVMe             |\r\n| local DRAM            | DRAM -\u003e GPU memory                    | PCIe             |\r\n\r\n\r\nCan we exploit higher-bandwidth data flows for transferring tensors? The answer is **YES** ‚Äî InfiniBand offers hundreds of gigabytes per second of throughput. However, the critical question remains: How can we fully leverage InfiniBand's bandwidth for efficient weight loading in SGLang?\r\n\r\nTo address this challenge, we have developed **a novel weight-loading framework called Tensor R-Fork** (stands for Tensor Remote Fork), which reduces Deepseek-R1 model loading time to mere seconds and is already production-ready.\r\n\r\n## Design\r\n\r\nThe core concept of \u003ca href=https://github.com/sgl-project/sglang/blob/main/docs/advanced_features/rfork.md\u003eTensor R-Fork\u003c/a\u003e[0] is to **leverage GPU-Direct RDMA for constructing a peer-to-peer (P2P) weight storage architecture.**\r\n\r\nThe performance of data transfer using traditional method is low, because there is always bottleneck in the entire path, whose bandwidth is much smaller than InfiniBand. \r\nFrom the data flow analysis, we observe that weight tensors are stored on each GPU and can be transmitted directly between nodes via GPU-direct RDMA.\r\n\r\nTo maximize the utilization of InfiniBand NIC's bandwidth, we design a per GPU-pair data transfer strategy: a local GPU directly transfers data to/from its paired remote GPU. This design effectively bypasses the PCIe bottleneck between GPU and CPU, enabling high-throughput communication without relying on CPU or host memory.\r\nThe data flow of loading weights from remote SGLang instance is the following:  \r\n\r\n\u003cimg src=\"/images/blog/rfork/design.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 100%; image-orientation: none;\"\u003e\u003c/img\u003e  \r\n\r\n\r\n## Implementation\r\n\r\nTo make every running instances act as the source of model weights for any new instance requiring the same model‚Äîwhile minimizing (or ideally eliminating) disruption to the inference services of running instances‚Äîwe implemented the framework with two backend options: NCCL and TransferEngine. Consider a running instance A (referred to as the source instance) and a new instance B to be booted (destination instance). Below, we will explain the implementation of weight transfer mechanisms using these two backends in detail.\r\n\r\n### NCCL backend\r\n\r\nWhen using \u003ca href=https://github.com/sgl-project/sglang/pull/8215\u003eNCCL\u003c/a\u003e as the backend[1], the process involves two stages:\r\n1. Establishing communication groups between source and destination instances.\r\n2. Transferring weights from the source instance to the destination instance via these groups.\r\n\r\nDuring destination instance initialization, it sends an HTTP request to the designated source instance to initiate communication group creation. Each TPWorker of destination instance establishes a NCCL communication group with its corresponding TPWorker of source instance (i.e.source rank 0 pairs with destination rank 0, etc.). Each communication group consists of exactly two members: the source TPWorker and destination TPWorker.\r\n\r\nOnce communication groups are established, each source TPWorker broadcasts its weights tensor located on GPU memory through the group using NCCL broadcast. The destination TPWorker receives the weights directly into its GPU memory without any intermediate memory copies.\r\n\r\nWhile NCCL serves as Tensor R-Fork backend by leveraging GPU-Direct RDMA, it does have a critical limitation: weight transfer disrupts the source instance's inference service, due to two key factors:\r\n1. **Communication group establishment**: The source instance must actively participate in creating communication groups.\r\n2. **CUDA kernel interference**: The NCCL broadcast mechanism triggers CUDA kernel execution, which competes for GPU resources and introduces latency spikes during generation tasks.\r\n\r\n### TransferEngine backend\r\n\r\nTo achieve non-disturbing weight transfer, we introduce an alternative backend: \u003ca href=https://github.com/sgl-project/sglang/pull/14997\u003eTransferEngine\u003c/a\u003e, which leverages GPU-Direct RDMA for efficient data movement[2]. TransferEngine (TE) is a lightweight RDMA-based transfer runtime that runs alongside each TPWorker on the source instance and exposes GPU-resident weight tensors to remote readers without invoking CUDA kernels on the source.\r\n\r\nDuring source SGLang instance initialization:\r\n1. Each TPWorker (tensor parallel worker) spawns a TransferEngine instance.\r\n2. TransferEngine registers the GPU memory addresses of its weights with the RDMA channel.\r\n\r\nWhen initializing the destination instance:\r\n1. It sends an HTTP request to retrieve the source instance's TransferEngine metadata, including RDMA keys mapped to the corresponding GPU memory addresses.\r\n2. Using these RDMA keys, the destination instance directly loads weights from the source's GPU memory without interrupting the source instance's ongoing services.\r\n\r\n*Want to learn more about TransferEngine? You are more than welcome to check **TransferEngine** in Appendix session üöÄ\r\n\r\n### NCCL vs. TransferEngine\r\n\r\n|                      | NCCL                           | TransferEngine |\r\n|----------------------|--------------------------------|----------------|\r\n| Deployment Complexity| ‚úÖ No additional dependency.   |‚ùå Additional library `mooncake` is needed. |\r\n|Overhead of Transfer Setup | ‚úÖ Building communication groups takes hundreds of milliseconds | ‚ûñ Registering memory regions to RDMA channel may take several seconds, but can be overlapped with other initialization phases.|\r\n|Non-disturbing to GPU workload | ‚ùå Tensor transfer will launch CUDA kernels. | ‚úÖ No CUDA kernels launched for transferring weights. | \r\n\r\n## How to Use\r\n\r\nDetailed usage please refer to \u003ca href=https://github.com/sgl-project/sglang/blob/main/docs/advanced_features/rfork.md\u003eR-Fork document\u003c/a\u003e\r\n\r\n### Use NCCL as backend\r\n\r\nseed instance:\r\n```shell\r\npython -m sglang.launch_server [args]\r\n```\r\n\r\nclient instance:\r\n```shell\r\npython -m sglang.launch_server [args] \\\r\n  --load-format remote_instance\t\\\r\n  --remote-instance-weight-loader-seed-instance-ip [seed_instance_ip] \\\r\n  --remote-instance-weight-loader-seed-instance-service-port [seed_instance_service_port] \\\r\n  --remote-instance-weight-loader-send-weights-group-ports [send_weights_nccl_group_ports_list]  \\\r\n  --remote-instance-weight-loader-backend nccl\r\n```\r\n\r\n### Use TransferEngine as backend\r\n\r\nseed instance:\r\n```shell\r\npython -m sglang.launch_server [args] \\\r\n  --remote-instance-weight-loader-start-seed-via-transfer-engine\r\n```\r\n\r\nclient instance:\r\n```shell\r\npython -m sglang.launch_server [args] \\\r\n  --load-format remote_instance\t\\\r\n  --remote-instance-weight-loader-seed-instance-ip [seed_instance_ip] \\\r\n  --remote-instance-weight-loader-seed-instance-service-port [seed_instance_service_port] \\\r\n  --remote-instance-weight-loader-backend transfer_engine\r\n```\r\n\r\n## Performance\r\n\r\nWe evaluated the performance of launching a new SGLang instance equipped with eight NVIDIA H20 GPUs, while loading the DeepSeek-R1 model from different sources.  \r\n\r\n\u003cimg src=\"/images/blog/rfork/performance.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 100%; image-orientation: none;\"\u003e\u003c/img\u003e  \r\n\r\nRegistering the memory region can be overlapped with other initialization phases to further optimize total boot-up time.\r\n\r\n## Industrial Practice\r\n\r\nIn the previous sections, we demonstrated how to manually configure seed instances for Tensor R-Fork within SGLang server arguments. However, this manual approach is impractical in real-world industrial deployment, where identifying available seed instances requires significant operational overhead.\r\n\r\nTo address this challenge, we propose **\u003ca href=https://github.com/sgl-project/sglang/issues/12910\u003eTensor R-Fork Planner\u003c/a\u003e**[4], a cluster scheduler designed to orchestrate source instance metadata. The Planner tracks critical information, including:\r\n1. **Model compatibility**: Which model is currently running on the instance.\r\n2. **Parallelism configuration**: The parallel strategy (e.g., tensor parallelism, pipeline parallelism) employed.\r\n3. **Service health status**: Whether the instance is healthy and suitable as a seed instance.\r\n\r\nUpon completion of its initialization, each instance registers itself with the Planner, providing its model metadata and parallelism configuration. When a new instance boots up, it first queries the Planner to identify an eligible seed instance that matches both its model and parallelism strategy. If a compatible seed instance is found, the new instance loads weights directly from the seed; otherwise, it falls back to the default load format.\r\n\r\n## Future Work\r\n\r\nThe practice of R-Fork opens up more imaginative possibilities: the key concept of R-Fork is that it enables all SGLang instances to act as data storage centers for other instances. Starting from weight tensors, we will manage additional tensors through Tensor R-Fork mechanism in the future, allowing GPU clusters to function not only as computing centers but also as storage centers.\r\n\r\n## Acknowledgements\r\n\r\n**Ant Group DeepXPU Team**: Anqi Shen, Tianyu Zhou, Zehuan Li, Tiwei Bie, Mingliang Gong, Jianfeng Tan\r\n\r\n**SGLang Team**: Chenyang Zhao, Liangsheng Yin, Lianmin Zheng  \r\n\r\n**TransferEngine Team**: Teng Ma, Feng Ren, Shangming Cai\r\n\r\n## Reference\r\n\r\n[0] Tensor R-Fork Documentation: \u003ca href=https://github.com/sgl-project/sglang/blob/main/docs/advanced_features/rfork.md\u003eDocumentation\u003c/a\u003e  \r\n[1] Tensor R-Fork with NCCL backend: \u003ca href=https://github.com/sgl-project/sglang/pull/8215\u003ePR#8215\u003c/a\u003e  \r\n[2] Tensor R-Fork with TransferEngine backend: \u003ca href=https://github.com/sgl-project/sglang/pull/14997\u003ePR#14997\u003c/a\u003e  \r\n[3] Concurrent weights loading from disk: \u003ca href=https://github.com/sgl-project/sglang/pull/7943\u003ePR#7943\u003c/a\u003e  \r\n[4] Tensor R-Fork Planner SGLang RFC: \u003ca href=https://github.com/sgl-project/sglang/issues/12910\u003eIssue#12910\u003c/a\u003e  \r\n[5] TransferEngine: \u003ca href=https://kvcache-ai.github.io/Mooncake/design/transfer-engine.html\u003ehttps://kvcache-ai.github.io/Mooncake/design/transfer-engine.html\u003c/a\u003e,  \r\n     TransferEngine APIs: \u003ca href=https://kvcache-ai.github.io/Mooncake/python-api-reference/transfer-engine.html\u003ehttps://kvcache-ai.github.io/Mooncake/python-api-reference/transfer-engine.html\u003c/a\u003e\r\n\r\n## Appendix\r\n\r\n### TransferEngine\r\n\r\nKey advantages provided by TransferEngine[5]:\r\n* **Multi-backend support**: TE supports multiple backends, including RDMA (with GPUDirect), NVLink, GDS, and TCP. It can intelligently identify the best backend per request, so that the highest performance could be reached.\r\n* **Direct RDMA reads**: using the published addresses and rkeys, the destination performs RDMA operations (typically RDMA READ) directly into its own pre-allocated GPU buffers, leveraging GPU-Direct RDMA so that no host-device or device-host intermediate copies are required.\r\n* **Non-disturbing**: TE performs pure NIC-driven transfers that avoid launching CUDA kernels on the source GPU.\r\n* **Lifecycle \u0026 housekeeping**: TE maintains the lifetime of registrations until tensors are evicted or the process exits.\r\n* **Concurrency \u0026 flow control**: TE coordinates concurrent reads (from one or many destinations) and can apply throttling or rate limits to avoid saturating instance‚Äôs NIC or impacting inference latency.\r\n\r\nKnown limitation in the current TransferEngine implementation:\r\n* **Memory registration (register_mr) is slow**: \u003cu\u003eThis is due to the RDMA driver\u003c/u\u003e. If you have any insights or solutions to this issue, we would be truly grateful to hear from you. We value diverse perspectives and are keen to explore innovative approaches together.\r\n\r\n\r\n","slug":"2025-12-10-rfork"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2025-12-10-rfork"},"buildId":"rTodAa4cLK7cUPkwXDz07","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>