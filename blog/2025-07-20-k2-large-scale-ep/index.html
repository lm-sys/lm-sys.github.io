<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>Deploying Kimi K2 with PD Disaggregation and Large-Scale Expert Parallelism on 128 H200 GPUs | LMSYS Org</title><meta name="title" content="Deploying Kimi K2 with PD Disaggregation and Large-Scale Expert Parallelism on 128 H200 GPUs | LMSYS Org"/><meta property="og:title" content="Deploying Kimi K2 with PD Disaggregation and Large-Scale Expert Parallelism on 128 H200 GPUs | LMSYS Org"/><meta name="twitter:title" content="Deploying Kimi K2 with PD Disaggregation and Large-Scale Expert Parallelism on 128 H200 GPUs | LMSYS Org"/><meta name="description" content="&lt;h2&gt;&lt;a id=&quot;1️⃣-introduction-deploying-the-most-advanced-open-source-moe-model&quot; class=&quot;anchor&quot; href=&quot;#1️⃣-introduction-deploying-the-most-advanced-open-source..."/><meta property="og:description" content="&lt;h2&gt;&lt;a id=&quot;1️⃣-introduction-deploying-the-most-advanced-open-source-moe-model&quot; class=&quot;anchor&quot; href=&quot;#1️⃣-introduction-deploying-the-most-advanced-open-source..."/><meta name="twitter:description" content="&lt;h2&gt;&lt;a id=&quot;1️⃣-introduction-deploying-the-most-advanced-open-source-moe-model&quot; class=&quot;anchor&quot; href=&quot;#1️⃣-introduction-deploying-the-most-advanced-open-source..."/><meta property="og:image" content="https://lmsys.org/images/blog/k2_large_scale/preview.jpg"/><meta name="twitter:image" content="https://lmsys.org/images/blog/k2_large_scale/preview.jpg"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2025-07-20-k2-large-scale-ep"/><meta name="twitter:url" content="https://lmsys.org/blog/2025-07-20-k2-large-scale-ep"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-eef2afd147d8eda9.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/WV1Nu7VeIDsdF7Dm-Pj_Z/_buildManifest.js" defer=""></script><script src="/_next/static/WV1Nu7VeIDsdF7Dm-Pj_Z/_ssgManifest.js" defer=""></script><script src="/_next/static/WV1Nu7VeIDsdF7Dm-Pj_Z/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">Deploying Kimi K2 with PD Disaggregation and Large-Scale Expert Parallelism on 128 H200 GPUs</h1><p class="text-xl pt-2 pb-2">by: <!-- -->The Mooncake Team<!-- -->,<!-- --> <!-- -->Jul 20, 2025<!-- --></p><hr/><div class="pt-2 article"><h2><a id="1️⃣-introduction-deploying-the-most-advanced-open-source-moe-model" class="anchor" href="#1️⃣-introduction-deploying-the-most-advanced-open-source-moe-model" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>1️⃣ Introduction: Deploying the Most Advanced Open-Source MoE Model</h2>
<p><strong>Kimi K2 is currently the most advanced open-source Mixture-of-Experts (MoE) model available.</strong></p>
<p>Released by Moonshot AI in 2025, it features:</p>
<ul>
<li><strong>1 trillion total parameters</strong></li>
<li><strong>32 billion activated parameters per token</strong></li>
<li><strong>384 experts with dynamic routing</strong></li>
<li><strong>Multi-head Latent Attention (MLA)</strong> for long context support</li>
</ul>
<p>Kimi K2 achieves strong performance in <strong>frontier knowledge, math, and coding</strong>, and is optimized for <strong>agentic tasks</strong>—not just answering questions but taking multi-step actions.</p>
<p>Moonshot AI open-sourced two versions:</p>
<ul>
<li><strong>Kimi-K2-Base</strong>: The foundation model for research and fine-tuning</li>
<li><strong>Kimi-K2-Instruct</strong>: A post-trained model for general-purpose chat and agentic applications</li>
</ul>
<p>For more details, please refer to the <a href="https://moonshotai.github.io/Kimi-K2/">official Kimi K2 release</a>.</p>
<hr>
<h3><a id="why-large-scale-deployment-matters" class="anchor" href="#why-large-scale-deployment-matters" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why Large-Scale Deployment Matters</h3>
<p>Large-scale deployment fully leverages hardware capabilities and reduces costs given the model’s architecture.</p>
<ul>
<li><strong>Serve More Requests, Faster:</strong> Higher throughput, lower latency, more concurrent sessions, and shorter queues.</li>
<li><strong>Lower $/Token:</strong> Saturate hardware and amortize model load; efficiency improves at scale.</li>
</ul>
<p>However, the large-scale deployment of trillion-scale MoE models present unique challenges:</p>
<ul>
<li><strong>Computational sparsity in MoE layers</strong> necessitates large batch sizes to make matrix operations compute-intensive. Large-scale Expert Parallelism (EP) scales parallelism strategies across more GPUs, aggregates requests from multiple devices, reduces per-GPU memory pressure, and frees up VRAM for larger KV caches—effectively increasing batch size.</li>
<li><strong>Cross-node</strong> communication takes a large amount of time and requires optimizations</li>
<li><strong>Sparse expert activation</strong> leads to load imbalance</li>
</ul>
<p>Efficient deployment of Kimi K2 on <strong>128 H200 GPUs</strong> requires rethinking both system design and deployment workflows.</p>
<p>In this blog, we explain how we solved this problem using <strong>OME</strong> and <strong>SGLang</strong>.</p>
<hr>
<h2><a id="2️⃣-background-from-deepseek-r1-to-kimi-k2" class="anchor" href="#2️⃣-background-from-deepseek-r1-to-kimi-k2" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>2️⃣ Background: From DeepSeek R1 to Kimi K2</h2>
<p>In May 2025, we published <a href="https://lmsys.org/blog/2025-05-05-large-scale-ep/">Deploying DeepSeek R1 with PD Disaggregation and Large-Scale EP</a>, where we demonstrated:</p>
<ul>
<li><strong>Prefill-Decode (PD) Disaggregation</strong> to separate compute-heavy and latency-sensitive tasks</li>
<li><strong>Large-Scale Expert Parallelism (EP)</strong> to handle MoE routing across 96 GPUs</li>
<li><strong>5× throughput improvement</strong> compared to vanilla tensor parallelism on H100s</li>
</ul>
<p>At the same time, our <a href="https://lmsys.org/blog/2025-07-08-ome/">OME blog</a> introduced <strong>model-driven deployment</strong>, solving the operational gap between:</p>
<ul>
<li><strong>ML Engineers</strong>, who design complex serving strategies</li>
<li><strong>Production Engineers</strong>, who need simple and reliable deployments</li>
</ul>
<p>The OME insight—the model should drive deployment, not vice-versa—proved productive for scaling to Kimi K2’s 1T-parameter architecture. This transition required adapting DeepSeek’s PD Disaggregation and EP to Kimi K2’s 384 experts while maintaining high performance.</p>
<hr>
<h2><a id="3️⃣-our-solution-ome--sglang-pd-disaggregation--large-scale-expert-parallelism" class="anchor" href="#3️⃣-our-solution-ome--sglang-pd-disaggregation--large-scale-expert-parallelism" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3️⃣ Our Solution: OME + SGLang PD Disaggregation + Large-Scale Expert Parallelism</h2>
<p>For Kimi K2, we combined the strengths of <strong>OME</strong> and <strong>SGLang</strong> to create an optimized, scalable deployment pipeline.</p>
<h3><a id="model-driven-deployment-with-ome" class="anchor" href="#model-driven-deployment-with-ome" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model-Driven Deployment with OME</h3>
<p>OME (Open Model Engine) simplifies the deployment of advanced models like Kimi K2 by abstracting away the complexity of parallelism, sharding, scaling, and runtime configuration. With a declarative configuration model, OME enables production teams to deploy and manage large models without manual tuning or custom scripting.</p>
<p><strong>OME Installation</strong></p>
<p>Install OME directly from the OCI registry using the following commands:</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># Step 1: Install OME CRDs</span>
helm upgrade --install ome-crd oci://ghcr.io/moirai-internal/charts/ome-crd --namespace ome --create-namespace

<span class="hljs-comment"># Step 2: Install OME core resources</span>
helm upgrade --install ome oci://ghcr.io/moirai-internal/charts/ome-resources --namespace ome
</code></pre>
<p>For detailed setup instructions, refer to the official <a href="https://docs.sglang.ai/ome/docs/installation/">OME installation guide</a>.</p>
<p><strong>Registering the Kimi K2 Model</strong>
To enable OME to manage the Kimi K2 model family, apply the following ClusterBaseModel resource:</p>
<pre><code class="hljs language-bash">kubectl apply -f https://raw.githubusercontent.com/sgl-project/ome/refs/heads/main/config/models/moonshotai/Kimi-K2-Instruct.yaml
</code></pre>
<p>Note: You may download the YAML file and customize the path field to specify where the model should be stored locally. OME will download the model directly from Hugging Face with optimized parallelism and automatically verify the artifact checksum to ensure integrity.</p>
<p><strong>Installing the Kimi K2 latest SGLang Serving Runtime</strong></p>
<pre><code class="hljs language-bash">kubectl apply -f https://raw.githubusercontent.com/sgl-project/ome/refs/heads/main/config/runtimes/srt/kimi-k2-pd-rt.yaml
</code></pre>
<p><strong>Deploying the Model</strong></p>
<p>Once the model and runtime are registered, deploy the inference endpoint using:</p>
<pre><code class="hljs language-bash">kubectl apply -f https://raw.githubusercontent.com/sgl-project/ome/refs/heads/main/config/samples/isvc/moonshotai/kimi-k2-pd.yaml
</code></pre>
<p>With these declarative resources in place, OME will automatically handle model downloading, runtime orchestration, and endpoint provisioning—enabling scalable, production-grade inference for the Kimi K2 model family.</p>
<p><strong>Interacting with the Model</strong></p>
<p>This command forwards local port 8080 to model on port 80:</p>
<pre><code class="hljs language-bash">kubectl port-forward -n kimi-k2-instruct service/kimi-k2-instruct 8080:80
</code></pre>
<p>Leave this running in one terminal. It will route your local http://localhost:8080 to the SGlang router. After the port-forward is active, run this in a second terminal:</p>
<pre><code class="hljs language-bash">curl -s -X POST http://localhost:8080/generate \
  -H <span class="hljs-string">&#x27;Content-Type: application/json&#x27;</span> \
  -H <span class="hljs-string">&#x27;Authorization: Bearer None&#x27;</span> \
  -d <span class="hljs-string">&#x27;{
    &quot;text&quot;: &quot;The future of AI is&quot;,
    &quot;max_new_tokens&quot;: 50,
    &quot;temperature&quot;: 0.7
  }&#x27;</span>
</code></pre>
<hr>
<h3><a id="ome-advantages--pd--deepep--router-insights" class="anchor" href="#ome-advantages--pd--deepep--router-insights" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>OME Advantages &amp; PD + DeepEP + Router Insights</strong></h3>
<p>OME (Open Model Engine) offers a declarative, production-ready framework for deploying large models like Kimi K2. It abstracts the complexities of GPU topology, distributed configuration, and runtime tuning—eliminating the need for custom orchestration logic. With a single ClusterServingRuntime definition, teams can launch optimized multi-node inference workloads at scale.</p>
<p>This configuration demonstrates a powerful setup leveraging <strong>Prefill-Decode (PD) disaggregation</strong> and <strong>Large Scale EP</strong>, enabling:</p>
<ul>
<li><strong>Disaggregated scaling</strong> of prefill and decode workloads with independent resource control</li>
<li><strong>Low-latency decode</strong> via deepep-mode=low_latency and token-aware dispatch tuning</li>
<li><strong>Advanced expert routing</strong> with ep-dispatch-algorithm=dynamic and enable-eplb</li>
<li><strong>RDMA acceleration for high-throughput kv-cache transfer</strong></li>
</ul>
<p>The deployment is orchestrated by a lightweight <strong>SGLang Router</strong>, which provides:</p>
<ul>
<li><strong>Dynamic service discovery</strong> for prefill and decode nodes via label selectors</li>
<li><strong>Auto-scaling capabilities</strong> independent of engine and decoder workloads</li>
<li><strong>Least-privilege routing model</strong>—ideal for secure production environments</li>
<li><strong>Optimized load balancing</strong> tailored for disaggregated serving patterns</li>
</ul>
<p>Together, OME and the SGLang Router form a robust foundation for large-scale, low-latency, and maintainable inference infrastructure.</p>
<h3><a id="prefill-decode-disaggregation" class="anchor" href="#prefill-decode-disaggregation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Prefill-Decode Disaggregation</h3>
<p>We separate inference into two independent components:</p>
<table>
<thead>
<tr>
<th>Stage</th>
<th>Role</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Prefill</strong></td>
<td>Handles large prompt ingestion (e.g., 2000-token inputs). This is compute-bound and benefits from large batch parallelism.</td>
</tr>
<tr>
<td><strong>Decode</strong></td>
<td>Handles autoregressive generation (e.g., 100-token outputs). This is latency-sensitive and optimized for high-throughput outputs.</td>
</tr>
</tbody>
</table>
<p>Prefill and Decode are deployed as independent services, each scaled and optimized separately.</p>
<hr>
<h3><a id="large-scale-expert-parallelism-ep" class="anchor" href="#large-scale-expert-parallelism-ep" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Large-Scale Expert Parallelism (EP)</h3>
<p>Kimi K2 activates a subset of <strong>384 experts</strong> per token. We implemented:</p>
<ul>
<li><strong>96 redundant experts on decode nodes</strong> to balance MoE routing</li>
<li><strong>NUMA-aware GPU grouping</strong> for optimal NVLink and PCIe utilization on H200 clusters</li>
</ul>
<p>This design minimizes load imbalance and ensures even GPU utilization across the 128-card cluster.</p>
<hr>
<h2><a id="4️⃣-performance-2000-input-100-output-benchmark" class="anchor" href="#4️⃣-performance-2000-input-100-output-benchmark" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>4️⃣ Performance: 2000-Input, 100-Output Benchmark</h2>
<p>We benchmarked Kimi K2 using a typical LLM serving workload on <strong>128 H200 GPUs with 1P1D (4 nodes/P and 12 nodes/D)</strong>:</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Input Length</strong></td>
<td>2000 tokens</td>
</tr>
<tr>
<td><strong>Output Length</strong></td>
<td>100 tokens</td>
</tr>
<tr>
<td><strong>Decode Batch Size</strong></td>
<td>480</td>
</tr>
</tbody>
</table>
<p>We use the same benchmark setup as in the DeepSeek R1 deployment blog as an example. Longer output for agentic scenarios will be future work.</p>
<p>Note: The prefill-to-decode ratio is workload-dependent. We prioritized decode nodes to maximize the KV Cache pool size, which is critical for scaling batch size to 480.</p>
<hr>
<h3><a id="cluster-level-performance-128--h200-gpus" class="anchor" href="#cluster-level-performance-128--h200-gpus" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Cluster-Level Performance (128 × H200 GPUs)</h3>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Prefill Throughput</strong></td>
<td><strong>224k tokens/sec (4 P Nodes)</strong></td>
</tr>
<tr>
<td><strong>Decode Throughput</strong></td>
<td><strong>288k tokens/sec (12 D Nodes)</strong></td>
</tr>
<tr>
<td><strong>Cost per 1M Output Tokens</strong></td>
<td><strong>~$0.21</strong>(<strong>H200 $2.3/hour</strong>)</td>
</tr>
</tbody>
</table>
<hr>
<h3><a id="comparison-to-deepseek-r1-deployment" class="anchor" href="#comparison-to-deepseek-r1-deployment" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Comparison to DeepSeek R1 Deployment</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Experts</th>
<th>GPUs</th>
<th>Prefill Throughput (tokens/sec)</th>
<th>Decode Throughput (tokens/sec)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>DeepSeek R1</strong></td>
<td>256</td>
<td>96 × H100</td>
<td>52.3k / node</td>
<td>22.3k / node</td>
</tr>
<tr>
<td><strong>Kimi K2</strong></td>
<td>384</td>
<td>128 × H200</td>
<td>56k / node</td>
<td>24k / node</td>
</tr>
</tbody>
</table>
<p>Despite Kimi K2’s larger MoE and more complex routing, our deployment achieves:</p>
<ul>
<li><strong>Balanced expert activation</strong>, using expert-parallel load balancer (EPLB)</li>
<li><strong>High throughput per GPU</strong> by applying SGLang’s specific optimizations for DeepSeek V3 architecture to H200</li>
</ul>
<p>The next step involves evaluating and optimizing long-context scenarios. As K2 is a model designed for agentic tasks, it has been reported that the average input length in such scenarios can range from 30,000 to 50,000 tokens.</p>
<hr>
<h2><a id="5️⃣-conclusion-trillion-scale-inference-at-scale" class="anchor" href="#5️⃣-conclusion-trillion-scale-inference-at-scale" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>5️⃣ Conclusion: Trillion-Scale Inference at Scale</h2>
<p>By combining <strong>OME</strong>, <strong>SGLang</strong>, <strong>PD Disaggregation</strong>, and <strong>Large-Scale Expert Parallelism</strong>, we deployed Kimi K2 on <strong>128 H200 GPUs</strong>, achieving:</p>
<ul>
<li><strong>Cost-effective large-scale inference</strong> (~$0.21 per 1M output tokens on H200) is available for short-context scenarios, with ongoing efforts to optimize the long-context scenarios.</li>
<li><strong>Simplified deployment workflows</strong> with model-driven configuration</li>
</ul>
<p>All components of this deployment are <strong>fully open-source and reproducible</strong>. We welcome the community to build on this work.</p>
<p>This deployment was made possible not only by open collaboration between Mooncake and the SGLang community, but also through the generous infrastructure support from NVIDIA DGX Cloud. NVIDIA provided the SGLang team with access to 128 H200 GPUs via DGX Cloud, enabling us to accelerate the deployment of Kimi K2 from model release to production-grade inference very quickly. As a result, organizations can now leverage SGLang to serve Kimi K2 at scale, unlocking advanced reasoning capabilities with state-of-the-art performance.</p>
<hr>
<h3><a id="acknowledgments" class="anchor" href="#acknowledgments" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgments</h3>
<p>We would like to express our heartfelt gratitude to the following teams and collaborators:</p>
<ul>
<li><strong>Mooncake Team:</strong> Boxin Zhang, Shangming Cai, Mingxing Zhang, and colleagues.</li>
<li><strong>SGLang Team and community:</strong> Simo Lin, Jingyi Chen, Qiaolin Yu, Yanbo Yang, Yineng Zhang, and many others.</li>
</ul>
<p>We extend our thanks to the <strong>MoonshotAI Team</strong>—including Shaowei Liu, Zhengtao Wang, Weiran He, Xinran Xu, and others—for their support in tuning the big beautiful model K2.</p>
<hr>
<h2><a id="further-reading" class="anchor" href="#further-reading" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Further Reading</h2>
<ul>
<li><a href="https://lmsys.org/blog/2025-05-05-large-scale-ep/">Deploying DeepSeek R1 with PD Disaggregation and Large-Scale EP</a></li>
<li><a href="https://lmsys.org/blog/2025-07-08-ome/">OME: Model-Driven LLM Deployment</a></li>
<li><a href="https://moonshotai.github.io/Kimi-K2/">Kimi K2 Official Release</a></li>
<li><a href="https://github.com/sgl-project/sglang">SGLang GitHub Repository</a></li>
</ul>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Deploying Kimi K2 with PD Disaggregation and Large-Scale Expert Parallelism on 128 H200 GPUs","author":"The Mooncake Team","date":"July 20, 2025","previewImg":"/images/blog/k2_large_scale/preview.jpg"},"content":"\n\n## 1️⃣ Introduction: Deploying the Most Advanced Open-Source MoE Model\n\n**Kimi K2 is currently the most advanced open-source Mixture-of-Experts (MoE) model available.**\n\nReleased by Moonshot AI in 2025, it features:\n\n- **1 trillion total parameters**\n- **32 billion activated parameters per token**\n- **384 experts with dynamic routing**\n- **Multi-head Latent Attention (MLA)** for long context support\n\nKimi K2 achieves strong performance in **frontier knowledge, math, and coding**, and is optimized for **agentic tasks**—not just answering questions but taking multi-step actions.\n\nMoonshot AI open-sourced two versions:\n\n- **Kimi-K2-Base**: The foundation model for research and fine-tuning\n- **Kimi-K2-Instruct**: A post-trained model for general-purpose chat and agentic applications\n\nFor more details, please refer to the [official Kimi K2 release](https://moonshotai.github.io/Kimi-K2/).\n\n---\n\n### Why Large-Scale Deployment Matters\n\nLarge-scale deployment fully leverages hardware capabilities and reduces costs given the model’s architecture.\n\n- **Serve More Requests, Faster:** Higher throughput, lower latency, more concurrent sessions, and shorter queues.\n- **Lower $/Token:** Saturate hardware and amortize model load; efficiency improves at scale.\n\nHowever, the large-scale deployment of trillion-scale MoE models present unique challenges:\n\n- **Computational sparsity in MoE layers** necessitates large batch sizes to make matrix operations compute-intensive. Large-scale Expert Parallelism (EP) scales parallelism strategies across more GPUs, aggregates requests from multiple devices, reduces per-GPU memory pressure, and frees up VRAM for larger KV caches—effectively increasing batch size.\n- **Cross-node** communication takes a large amount of time and requires optimizations\n- **Sparse expert activation** leads to load imbalance\n\nEfficient deployment of Kimi K2 on **128 H200 GPUs** requires rethinking both system design and deployment workflows.\n\nIn this blog, we explain how we solved this problem using **OME** and **SGLang**.\n\n---\n\n## 2️⃣ Background: From DeepSeek R1 to Kimi K2\n\nIn May 2025, we published [Deploying DeepSeek R1 with PD Disaggregation and Large-Scale EP](https://lmsys.org/blog/2025-05-05-large-scale-ep/), where we demonstrated:\n\n- **Prefill-Decode (PD) Disaggregation** to separate compute-heavy and latency-sensitive tasks\n- **Large-Scale Expert Parallelism (EP)** to handle MoE routing across 96 GPUs\n- **5× throughput improvement** compared to vanilla tensor parallelism on H100s\n\nAt the same time, our [OME blog](https://lmsys.org/blog/2025-07-08-ome/) introduced **model-driven deployment**, solving the operational gap between:\n\n- **ML Engineers**, who design complex serving strategies\n- **Production Engineers**, who need simple and reliable deployments\n\nThe OME insight—the model should drive deployment, not vice-versa—proved productive for scaling to Kimi K2’s 1T-parameter architecture. This transition required adapting DeepSeek’s PD Disaggregation and EP to Kimi K2’s 384 experts while maintaining high performance.\n\n---\n\n## 3️⃣ Our Solution: OME + SGLang PD Disaggregation + Large-Scale Expert Parallelism\n\nFor Kimi K2, we combined the strengths of **OME** and **SGLang** to create an optimized, scalable deployment pipeline.\n\n### Model-Driven Deployment with OME\n\nOME (Open Model Engine) simplifies the deployment of advanced models like Kimi K2 by abstracting away the complexity of parallelism, sharding, scaling, and runtime configuration. With a declarative configuration model, OME enables production teams to deploy and manage large models without manual tuning or custom scripting.\n\n**OME Installation**\n\nInstall OME directly from the OCI registry using the following commands:\n\n```bash\n# Step 1: Install OME CRDs\nhelm upgrade --install ome-crd oci://ghcr.io/moirai-internal/charts/ome-crd --namespace ome --create-namespace\n\n# Step 2: Install OME core resources\nhelm upgrade --install ome oci://ghcr.io/moirai-internal/charts/ome-resources --namespace ome\n```\n\nFor detailed setup instructions, refer to the official [OME installation guide](https://docs.sglang.ai/ome/docs/installation/).\n\n**Registering the Kimi K2 Model**\nTo enable OME to manage the Kimi K2 model family, apply the following ClusterBaseModel resource:\n\n```bash\nkubectl apply -f https://raw.githubusercontent.com/sgl-project/ome/refs/heads/main/config/models/moonshotai/Kimi-K2-Instruct.yaml\n```\n\nNote: You may download the YAML file and customize the path field to specify where the model should be stored locally. OME will download the model directly from Hugging Face with optimized parallelism and automatically verify the artifact checksum to ensure integrity.\n\n**Installing the Kimi K2 latest SGLang Serving Runtime**\n\n```bash\nkubectl apply -f https://raw.githubusercontent.com/sgl-project/ome/refs/heads/main/config/runtimes/srt/kimi-k2-pd-rt.yaml\n```\n\n**Deploying the Model**\n\nOnce the model and runtime are registered, deploy the inference endpoint using:\n\n```bash\nkubectl apply -f https://raw.githubusercontent.com/sgl-project/ome/refs/heads/main/config/samples/isvc/moonshotai/kimi-k2-pd.yaml\n```\n\nWith these declarative resources in place, OME will automatically handle model downloading, runtime orchestration, and endpoint provisioning—enabling scalable, production-grade inference for the Kimi K2 model family.\n\n**Interacting with the Model**\n\nThis command forwards local port 8080 to model on port 80:\n```bash\nkubectl port-forward -n kimi-k2-instruct service/kimi-k2-instruct 8080:80\n```\nLeave this running in one terminal. It will route your local http://localhost:8080 to the SGlang router. After the port-forward is active, run this in a second terminal:\n```bash\ncurl -s -X POST http://localhost:8080/generate \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer None' \\\n  -d '{\n    \"text\": \"The future of AI is\",\n    \"max_new_tokens\": 50,\n    \"temperature\": 0.7\n  }'\n```\n\n---\n\n### **OME Advantages \u0026 PD + DeepEP + Router Insights**\n\nOME (Open Model Engine) offers a declarative, production-ready framework for deploying large models like Kimi K2. It abstracts the complexities of GPU topology, distributed configuration, and runtime tuning—eliminating the need for custom orchestration logic. With a single ClusterServingRuntime definition, teams can launch optimized multi-node inference workloads at scale.\n\nThis configuration demonstrates a powerful setup leveraging **Prefill-Decode (PD) disaggregation** and **Large Scale EP**, enabling:\n\n- **Disaggregated scaling** of prefill and decode workloads with independent resource control\n- **Low-latency decode** via deepep-mode=low_latency and token-aware dispatch tuning\n- **Advanced expert routing** with ep-dispatch-algorithm=dynamic and enable-eplb\n- **RDMA acceleration for high-throughput kv-cache transfer**\n\nThe deployment is orchestrated by a lightweight **SGLang Router**, which provides:\n\n- **Dynamic service discovery** for prefill and decode nodes via label selectors\n- **Auto-scaling capabilities** independent of engine and decoder workloads\n- **Least-privilege routing model**—ideal for secure production environments\n- **Optimized load balancing** tailored for disaggregated serving patterns\n\nTogether, OME and the SGLang Router form a robust foundation for large-scale, low-latency, and maintainable inference infrastructure.\n\n### Prefill-Decode Disaggregation\n\nWe separate inference into two independent components:\n\n| Stage | Role |\n| --- | --- |\n| **Prefill** | Handles large prompt ingestion (e.g., 2000-token inputs). This is compute-bound and benefits from large batch parallelism. |\n| **Decode** | Handles autoregressive generation (e.g., 100-token outputs). This is latency-sensitive and optimized for high-throughput outputs. |\n\nPrefill and Decode are deployed as independent services, each scaled and optimized separately.\n\n---\n\n### Large-Scale Expert Parallelism (EP)\n\nKimi K2 activates a subset of **384 experts** per token. We implemented:\n\n- **96 redundant experts on decode nodes** to balance MoE routing\n- **NUMA-aware GPU grouping** for optimal NVLink and PCIe utilization on H200 clusters\n\nThis design minimizes load imbalance and ensures even GPU utilization across the 128-card cluster.\n\n---\n\n## 4️⃣ Performance: 2000-Input, 100-Output Benchmark\n\nWe benchmarked Kimi K2 using a typical LLM serving workload on **128 H200 GPUs with 1P1D (4 nodes/P and 12 nodes/D)**:\n\n| Metric | Value |\n| --- | --- |\n| **Input Length** | 2000 tokens |\n| **Output Length** | 100 tokens |\n| **Decode Batch Size** | 480 |\n\nWe use the same benchmark setup as in the DeepSeek R1 deployment blog as an example. Longer output for agentic scenarios will be future work.\n\nNote: The prefill-to-decode ratio is workload-dependent. We prioritized decode nodes to maximize the KV Cache pool size, which is critical for scaling batch size to 480.\n\n---\n\n### Cluster-Level Performance (128 × H200 GPUs)\n\n| Metric | Value |\n| --- | --- |\n| **Prefill Throughput** | **224k tokens/sec (4 P Nodes)** |\n| **Decode Throughput** | **288k tokens/sec (12 D Nodes)** |\n| **Cost per 1M Output Tokens** | **~$0.21**(**H200 $2.3/hour**) |\n\n---\n\n### Comparison to DeepSeek R1 Deployment\n\n| Model | Experts | GPUs | Prefill Throughput (tokens/sec) | Decode Throughput (tokens/sec) |\n| --- | --- | --- | --- | --- |\n| **DeepSeek R1** | 256 | 96 × H100 | 52.3k / node | 22.3k / node |\n| **Kimi K2** | 384 | 128 × H200 | 56k / node | 24k / node |\n\nDespite Kimi K2’s larger MoE and more complex routing, our deployment achieves:\n\n- **Balanced expert activation**, using expert-parallel load balancer (EPLB)\n- **High throughput per GPU** by applying SGLang’s specific optimizations for DeepSeek V3 architecture to H200\n\nThe next step involves evaluating and optimizing long-context scenarios. As K2 is a model designed for agentic tasks, it has been reported that the average input length in such scenarios can range from 30,000 to 50,000 tokens.\n\n---\n\n## 5️⃣ Conclusion: Trillion-Scale Inference at Scale\n\nBy combining **OME**, **SGLang**, **PD Disaggregation**, and **Large-Scale Expert Parallelism**, we deployed Kimi K2 on **128 H200 GPUs**, achieving:\n\n- **Cost-effective large-scale inference** (~$0.21 per 1M output tokens on H200) is available for short-context scenarios, with ongoing efforts to optimize the long-context scenarios.\n- **Simplified deployment workflows** with model-driven configuration\n\nAll components of this deployment are **fully open-source and reproducible**. We welcome the community to build on this work.\n\nThis deployment was made possible not only by open collaboration between Mooncake and the SGLang community, but also through the generous infrastructure support from NVIDIA DGX Cloud. NVIDIA provided the SGLang team with access to 128 H200 GPUs via DGX Cloud, enabling us to accelerate the deployment of Kimi K2 from model release to production-grade inference very quickly. As a result, organizations can now leverage SGLang to serve Kimi K2 at scale, unlocking advanced reasoning capabilities with state-of-the-art performance.\n\n---\n\n### Acknowledgments\n\nWe would like to express our heartfelt gratitude to the following teams and collaborators:\n\n- **Mooncake Team:** Boxin Zhang, Shangming Cai, Mingxing Zhang, and colleagues.\n- **SGLang Team and community:** Simo Lin, Jingyi Chen, Qiaolin Yu, Yanbo Yang, Yineng Zhang, and many others.\n\nWe extend our thanks to the **MoonshotAI Team**—including Shaowei Liu, Zhengtao Wang, Weiran He, Xinran Xu, and others—for their support in tuning the big beautiful model K2.\n\n---\n\n## Further Reading\n\n- [Deploying DeepSeek R1 with PD Disaggregation and Large-Scale EP](https://lmsys.org/blog/2025-05-05-large-scale-ep/)\n- [OME: Model-Driven LLM Deployment](https://lmsys.org/blog/2025-07-08-ome/)\n- [Kimi K2 Official Release](https://moonshotai.github.io/Kimi-K2/)\n- [SGLang GitHub Repository](https://github.com/sgl-project/sglang)\n","slug":"2025-07-20-k2-large-scale-ep"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2025-07-20-k2-large-scale-ep"},"buildId":"WV1Nu7VeIDsdF7Dm-Pj_Z","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>