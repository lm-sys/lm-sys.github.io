<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>How to support new VLMs into SGLang: A Case Study with NVILA | LMSYS Org</title><meta name="title" content="How to support new VLMs into SGLang: A Case Study with NVILA | LMSYS Org"/><meta property="og:title" content="How to support new VLMs into SGLang: A Case Study with NVILA | LMSYS Org"/><meta name="twitter:title" content="How to support new VLMs into SGLang: A Case Study with NVILA | LMSYS Org"/><meta name="description" content="&lt;p&gt;The world of LLMs is evolving at a remarkable pace, with Visual Language Models (VLMs) at the forefront of this revolution. These models power application..."/><meta property="og:description" content="&lt;p&gt;The world of LLMs is evolving at a remarkable pace, with Visual Language Models (VLMs) at the forefront of this revolution. These models power application..."/><meta name="twitter:description" content="&lt;p&gt;The world of LLMs is evolving at a remarkable pace, with Visual Language Models (VLMs) at the forefront of this revolution. These models power application..."/><meta property="og:image" content="https://lmsys.org/images/blog/nvila/preview.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/nvila/preview.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2025-07-16-nvila"/><meta name="twitter:url" content="https://lmsys.org/blog/2025-07-16-nvila"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-0bb93d4b49319e30.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/wpBETYUmFzM_IORRWVyLq/_buildManifest.js" defer=""></script><script src="/_next/static/wpBETYUmFzM_IORRWVyLq/_ssgManifest.js" defer=""></script><script src="/_next/static/wpBETYUmFzM_IORRWVyLq/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">How to support new VLMs into SGLang: A Case Study with NVILA</h1><p class="text-xl pt-2 pb-2">by: <!-- -->The NVILA Team<!-- -->,<!-- --> <!-- -->Jul 16, 2025<!-- --></p><hr/><div class="pt-2 article"><p>The world of LLMs is evolving at a remarkable pace, with Visual Language Models (VLMs) at the forefront of this revolution. These models power applications that can understand and reason about both images and text. There are <a href="https://huggingface.co/models?pipeline_tag=image-text-to-text&amp;sort=trending">tons of new VLM models</a> emerging daily, and we want to integrate them into <a href="https://github.com/sgl-project/sglang">SGLang</a> to leverage its high-speed throughput. Today, we’ll provide a step-by-step walkthrough for integrating new VLMs into the SGLang ecosystem, using the recent <a href="https://arxiv.org/abs/2412.04468">NVILA model</a> as a real-world case study.</p>
<h2><a id="accelerating-the-nvila-visual-language-model-with-sglang" class="anchor" href="#accelerating-the-nvila-visual-language-model-with-sglang" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Accelerating the NVILA Visual Language Model with SGLang</h2>
<p>The benchmarks below compare the original VILA implementation against SGLang with different levels of concurrency</p>
<p><img src="/images/blog/nvila/image.png" alt="image.png"></p>
<p>In real world VLM development, we focus on two important metrics to evaluate a serving systems’ performance: <strong>Throughput (Token per Second, TPS)</strong> and and <strong>Time to First Token (TTFT)</strong>.</p>
<ul>
<li>For TPS, higher throughput means the system can generate more tokens simultaneously . SGLang's <code>RadixAttention</code> allows for efficient batching of requests, dramatically increasing the number of tokens generated per second. With a concurrency of 8, SGLang achieves over <strong>4.4x higher throughput</strong>.</li>
<li>For TTFT, a lower value means users get a faster response to receive the first token. SGLang's memory optimizations and efficient kernel implementations significantly reduce prefill latency. The benchmark shows SGLang responses up to <strong>2.2x faster</strong> when concurrency is 8.</li>
</ul>
<p>These performance gains make SGLang an excellent choice for deploying demanding VLMs like NVILA in production environments, and now can be easily deployed for sglang version ≥ 0.4.8</p>
<pre><code class="hljs language-bash">python3 -m sglang.launch_server \
  --model-path Efficient-Large-Model/NVILA-Lite-2B-hf-0626 \
  --host 0.0.0.0 --port 30000 --trust-remote-code \
  --attention-backend fa3
</code></pre>
<h2><a id="the-big-picture-how-vlms-like-nvila-work" class="anchor" href="#the-big-picture-how-vlms-like-nvila-work" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Big Picture: How VLMs like NVILA Work</h2>
<p>Before diving into code, it helps to understand what is happening under the hood. Most Vision–Language Models (llava-based) share a three-component architecture:</p>
<ol>
<li><strong>Vision encoder</strong> – a convolutional network or, more commonly, a Vision Transformer (ViT) that converts pixels into a sequence of visual tokens.</li>
<li><strong>Projector</strong> – a lightweight adapter (often an MLP) that aligns those tokens with the embedding space of a pretrained language model.</li>
<li><strong>Token processor</strong> – the language model itself, which mixes visual and textual tokens and autoregressively generates the answer.</li>
</ol>
<p><a href="https://nvlabs.github.io/VILA">NVILA follows the paradigm</a> while pushing the efficiency frontier through its <em><strong>scale-then-compress</strong></em> strategy. It first <em>scales</em> the spatial and temporal resolution so the vision tower sees high-fidelity images or long videos, then <em>compresses</em> the resulting token stream so the language model only attends to a handful of high-information tokens:</p>
<ul>
<li><strong>High-resolution SigLIP vision tower</strong> captures fine-grained details from images and multi-frame clips.</li>
<li><strong>Spatial token pooling</strong> reduces dense patches to a much smaller grid, preserving text and structure while cutting quadratic attention cost.</li>
<li><strong>Temporal pooling</strong> keeps just the most informative frames, so video inputs add only a few extra tokens.</li>
<li><strong>Two-layer MLP projector</strong> maps the compressed visual embeddings into LLM’s  embedding space.</li>
<li><strong>FP8 training, dataset pruning, and memory-efficient fine-tuning</strong> trim training cost by up to 5× and deliver sub-second prefilling on a single 4090 GPU.</li>
</ul>
<p><img src="https://arxiv.org/html/2412.04468v2/x5.png" alt="NVILA Architecture"></p>
<p>NVILA Architecture</p>
<p>This blueprint is generic enough to cover most modern VLMs, yet the compression blocks highlighted in green are what make NVILA both <em>accurate</em> and <em>fast</em> at scale.</p>
<h2><a id="supporting-new-models-in-sglang" class="anchor" href="#supporting-new-models-in-sglang" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Supporting New Models in SGLang</h2>
<p>SGLang has a streamlined process for adding new models, which centers on creating a single model definition file that utilizes SGLang’s core components. The key is to replace standard Hugging Face components with their SGLang-optimized counterparts. For VLMs, this involves a few steps.</p>
<h2><a id="step-1-register-the-model-as-multimodal" class="anchor" href="#step-1-register-the-model-as-multimodal" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 1: Register the Model as Multimodal</h2>
<p>First, SGLang needs to identify your model as multimodal. This is handled in <code>sglang/srt/configs/model_config.py</code>. SGLang determines this by checking if the model’s architecture class name is present in the <code>multimodal_model_archs</code> list.</p>
<p>For NVILA, we add <code>&quot;VILAForConditionalGeneration&quot;</code> to this list. This tells SGLang that any model with this architecture should be treated as a multimodal model.</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># python/sglang/srt/configs/model_config.py</span>
<span class="hljs-comment"># ... existing code ...</span>
multimodal_model_archs = [
    <span class="hljs-comment"># ... existing code ...    </span>
    <span class="hljs-string">&quot;VILAForConditionalGeneration&quot;</span>,
]
<span class="hljs-comment"># ... existing code ...</span>
</code></pre>
<h2><a id="step-2-register-a-new-chat-template" class="anchor" href="#step-2-register-a-new-chat-template" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 2: Register a New Chat Template</h2>
<p>VLMs often require specific chat templates to handle prompts containing both images and text. SGLang allows you to either define a new template or, more conveniently, match your model to an existing one.</p>
<p>For NVILA, which uses a format similar to ChatML, we can reuse the existing <code>chatml</code> conversation template. To associate our model with this template, we register a matching function in <code>python/sglang/srt/conversation.py</code>. This function, <code>match_vila</code>, inspects the model path and returns <code>&quot;chatml&quot;</code> if it finds a match, telling SGLang to apply the ChatML format for NVILA models.</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># python/sglang/srt/conversation.py</span>
<span class="hljs-comment"># ... existing code ...</span>

<span class="hljs-meta">@register_conv_template_matching_function</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">match_vila</span>(<span class="hljs-params">model_path: <span class="hljs-built_in">str</span></span>):
    <span class="hljs-comment"># ... existing code ...</span>
    <span class="hljs-keyword">if</span> re.search(<span class="hljs-string">r&quot;vila&quot;</span>, model_path, re.IGNORECASE):
        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;chatml&quot;</span>
</code></pre>
<p>This registration instructs SGLang on how to format the input string and where to expect image data.</p>
<p>To make this concrete, let’s examine the template structure that <code>chatml</code> provides and how it aligns with NVILA’s needs.</p>
<h3><a id="understanding-the-chatml-template-for-nvila" class="anchor" href="#understanding-the-chatml-template-for-nvila" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Understanding the ChatML Template for NVILA</h3>
<p>The ChatML format, which NVILA uses, employs special tokens to structure the conversation. For a multimodal prompt, the template ensures that both text and images are correctly placed.</p>
<p>A typical conversation is a sequence of messages, each with a <code>role</code> (<code>system</code>, <code>user</code>, or <code>assistant</code>) and <code>content</code>. Here’s how a user’s query with an image is formatted:</p>
<p><strong>Example User Message:</strong></p>
<pre><code class="hljs language-json"><span class="hljs-punctuation">{</span>
  <span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;user&quot;</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>
    <span class="hljs-punctuation">{</span> <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;text&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;text&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;What is in this image?&quot;</span> <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span>
    <span class="hljs-punctuation">{</span> <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;image&quot;</span> <span class="hljs-punctuation">}</span>
  <span class="hljs-punctuation">]</span>
<span class="hljs-punctuation">}</span>
</code></pre>
<p><strong>Resulting Prompt String:</strong></p>
<p>The template processor converts this into a single string for the model:</p>
<pre><code class="hljs">&lt;|im_start|&gt;system
<span class="hljs-type">You</span> are a helpful assistant.&lt;|im_end|&gt;
&lt;|im_start|&gt;user
<span class="hljs-type">What</span> is in this image?&lt;image&gt;&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
</code></pre>
<p>Here’s a breakdown of the components:</p>
<ul>
<li><code>&lt;|im_start|&gt;{role}</code> and <code>&lt;|im_end|&gt;</code>: These tokens define the boundaries of each message.</li>
<li>A default system prompt is automatically added if not provided.</li>
<li>Text content is included as-is.</li>
<li>Image content is replaced with a special <code>&lt;image&gt;</code> placeholder token.</li>
</ul>
<p>By matching NVILA to the <code>chatml</code> template, SGLang automatically handles this formatting. The engine knows to replace the <code>&lt;image&gt;</code> placeholder with the actual image features during processing, a process we will cover in Step 3.</p>
<p>If your model requires a completely new chat template, you can define it directly in the same file. For example, if NVILA had used a unique format, we would create a new conversation template:</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># python/sglang/srt/conversation.py</span>
<span class="hljs-comment"># ... existing code ...</span>
register_conv_template(
    Conversation(
        name=<span class="hljs-string">&quot;vila&quot;</span>,
        system_template=<span class="hljs-string">&quot;&lt;|im_start|&gt;system\\n{system_message}&quot;</span>,
        system_message=<span class="hljs-string">&quot;You are a helpful assistant.&quot;</span>,
        roles=(<span class="hljs-string">&quot;&lt;|im_start|&gt;user&quot;</span>, <span class="hljs-string">&quot;&lt;|im_start|&gt;assistant&quot;</span>),
        sep_style=SeparatorStyle.CHATML,
        sep=<span class="hljs-string">&quot;&lt;|im_end|&gt;&quot;</span>,
        stop_str=[<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>, <span class="hljs-string">&quot;&lt;|im_end|&gt;&quot;</span>],
    )
)
</code></pre>
<h2><a id="step-3-building-the-multimodal-data-processor" class="anchor" href="#step-3-building-the-multimodal-data-processor" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 3: Building the Multimodal Data Processor</h2>
<p>The data processor is the heart of the multimodal integration. It acts as the bridge that takes a user’s raw input—a mix of text and images—and meticulously prepares it for the model. For our NVILA integration, we’ll create a <code>VILAMultimodalProcessor</code> in <code>python/sglang/srt/managers/multimodal_processors/vila.py</code>. See <a href="https://github.com/sgl-project/sglang/pull/6106/files#diff-d9c481aa1e61e56644d3e89901895dafa9afd7e9d1147d48ab60c14435196bdb">here</a> for the full diff.</p>
<p>This class inherits from SGLang’s <code>BaseMultimodalProcessor</code>, and its main workhorse is the <code>process_mm_data_async</code> method. Let’s build this method step-by-step to see how it transforms raw data into a model-ready format.</p>
<h3><a id="the-processors-skeleton" class="anchor" href="#the-processors-skeleton" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Processor’s Skeleton</h3>
<p>First define the class. A crucial component is the <code>models</code> class attribute, which links the processor to its corresponding SGLang model class. This registration allows SGLang to  to recognize the associated processor of the model when loading  <code>VILAForConditionalGeneration</code>. The <code>__init__</code> method is also updated to cache special token IDs like <code>image_token_id</code> directly from the model's configuration. The processor's main method, <code>process_mm_data_async</code>, then takes the raw request details and returns the processed data that the SGLang engine can understand.</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># python/sglang/srt/multimodal/processors/vila.py</span>
<span class="hljs-comment"># ... imports ...</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">VILAMultimodalProcessor</span>(<span class="hljs-title class_ inherited__">BaseMultimodalProcessor</span>):
    models: <span class="hljs-type">List</span>[<span class="hljs-type">Type</span>[nn.Module]] = [VILAForConditionalGeneration]

    <span class="hljs-comment"># This holds the Hugging Face processor</span>
    _processor: VILAProcessor

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">
        self,
        hf_config: PretrainedConfig,
        server_args: ServerArgs,
        _processor: VILAProcessor,
    </span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().__init__(hf_config, server_args, _processor)
        <span class="hljs-comment"># Store special token IDs</span>
        <span class="hljs-variable language_">self</span>.IM_TOKEN_ID = hf_config.image_token_id
        <span class="hljs-variable language_">self</span>.VIDEO_TOKEN_ID = hf_config.video_token_id

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_mm_data_async</span>(<span class="hljs-params">
        self,
        image_data: <span class="hljs-type">Optional</span>[ImageDataItem | <span class="hljs-type">List</span>[ImageDataItem]],
        input_text: <span class="hljs-built_in">str</span> | <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>],
        request_obj: GenerateReqInput | EmbeddingReqInput,
        max_req_input_len: <span class="hljs-built_in">int</span>,
        **kwargs,
    </span>) -&gt; <span class="hljs-type">Optional</span>[<span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]]:
        <span class="hljs-comment"># ... implementation below ...</span>
</code></pre>
<h3><a id="from-raw-input-to-processed-data" class="anchor" href="#from-raw-input-to-processed-data" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>From Raw Input to Processed Data</h3>
<p>Inside <code>process_mm_data_async</code>, a streamlined pipeline handles the inputs.</p>
<ol>
<li><strong>Load and Process</strong>: The logic consolidates data handling. We first call <code>self.load_mm_data</code> from the base class to load image files and locate the special <code>&lt;image&gt;</code> tokens in the prompt. This output is then passed to <code>self.process_and_combine_mm_data</code>, a method that encapsulates the logic for tokenizing text, transforming images, and packaging the data.</li>
<li><strong>Assemble the Final Package</strong>: This process directly produces the final dictionary for the SGLang engine. This dictionary contains the tokenized <code>input_ids</code>, the assembled <code>mm_items</code> (which include processed image data and their locations), and explicitly includes the <code>im_token_id</code> and <code>video_token_id</code>.</li>
</ol>
<p>Here’s the complete implementation of <code>process_mm_data_async</code>:</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># (Inside VILAMultimodalProcessor)</span>
<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_mm_data_async</span>(<span class="hljs-params">
    self,
    image_data: <span class="hljs-type">Optional</span>[ImageDataItem | <span class="hljs-type">List</span>[ImageDataItem]],
    input_text: <span class="hljs-built_in">str</span> | <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>],
    request_obj: GenerateReqInput | EmbeddingReqInput,
    max_req_input_len: <span class="hljs-built_in">int</span>,
    **kwargs,
</span>) -&gt; <span class="hljs-type">Optional</span>[<span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]]:
    <span class="hljs-comment"># Load image data and find special tokens</span>
    base_output = <span class="hljs-variable language_">self</span>.load_mm_data(
        prompt=input_text,
        multimodal_tokens=MultimodalSpecialTokens(
            image_token=<span class="hljs-variable language_">self</span>._processor.tokenizer.image_token
        ),
        max_req_input_len=max_req_input_len,
        image_data=image_data,
    )

    <span class="hljs-comment"># Process images and text, then combine into final format</span>
    mm_items, input_ids = <span class="hljs-variable language_">self</span>.process_and_combine_mm_data(base_output)

    <span class="hljs-comment"># Return the complete package for the engine</span>
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">&quot;input_ids&quot;</span>: input_ids.tolist(),
        <span class="hljs-string">&quot;mm_items&quot;</span>: mm_items,
        <span class="hljs-string">&quot;im_token_id&quot;</span>: <span class="hljs-variable language_">self</span>.IM_TOKEN_ID,
        <span class="hljs-string">&quot;video_token_id&quot;</span>: <span class="hljs-variable language_">self</span>.VIDEO_TOKEN_ID,
    }
</code></pre>
<p>And that’s it! This clean design centralizes the core processing logic, making the processor more maintainable and easier to extend.</p>
<h2><a id="step-4-create-the-core-model-definition" class="anchor" href="#step-4-create-the-core-model-definition" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 4: Create the Core Model Definition</h2>
<p>The final major step is to define the model’s main class in a new file, <code>python/sglang/srt/models/vila.py</code>. This involves porting the Hugging Face model by replacing standard layers with SGLang’s high-performance equivalents.</p>
<h3><a id="adapting-attention-mechanisms" class="anchor" href="#adapting-attention-mechanisms" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Adapting Attention Mechanisms</h3>
<p>To unlock maximum performance, standard attention layers are replaced:</p>
<ul>
<li><strong>Language Model Attention</strong>: Original <code>Attention</code> layers are replaced with SGLang’s <code>RadixAttention</code>. This is the key to SGLang’s high-performance batching and memory management.</li>
<li><strong>Vision Model Attention</strong>: <code>Attention</code> layers in the Vision Transformer (ViT) are replaced with SGLang’s <code>VisionAttention</code> to manage the vision tower’s KV cache.</li>
</ul>
<p>NVILA uses Qwen2 as its language model, thus we directly adapt SGLang’s <code>Qwen2ForCausalLM</code> class and do not need to manually replace the attention layers. See the diff <a href="https://github.com/sgl-project/sglang/pull/6106/files#diff-bd699d67711317806e08a0811bdf770194e95bee8b6ebe2d972ffa5e93c5a4e0">here</a>.</p>
<h3><a id="handling-multimodal-inputs-with-pad_input_ids" class="anchor" href="#handling-multimodal-inputs-with-pad_input_ids" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Handling Multimodal Inputs with <code>pad_input_ids</code></h3>
<p>The <code>pad_input_ids</code> function is a critical component for handling multimodal inputs in SGLang. Its primary role is to prepare the input token sequence for <code>RadixAttention</code>. It achieves this by replacing placeholder tokens for images (or other modalities) with unique multimodal data hashes. These hashes, or <code>pad_value</code>, allow <code>RadixAttention</code> to correctly associate segments of the input with the corresponding image data, even when processing multiple requests in a single batch. This is essential for efficient multi-user serving.</p>
<p>SGLang provides different padding strategy tool classes to accommodate various ways models represent multimodal inputs. The choice of pattern depends on how the model’s tokenizer and chat template are designed.</p>
<p><strong>Pattern 1: Contiguous Multimodal Tokens</strong></p>
<p>Some models like NVILA and Kimi-VL represent an image with a contiguous block of special tokens (e.g., <code>&lt;image&gt;&lt;image&gt;...&lt;image&gt;</code>). For these models, we use <code>MultiModalityDataPaddingPatternMultimodalTokens</code>. The <code>pad_input_ids</code> function signature is generalized to accept <code>mm_inputs</code>, which can contain various modalities. For NVILA, the implementation is straightforward and concise:</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># python/sglang/srt/models/vila.py</span>
<span class="hljs-comment"># ...</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">pad_input_ids</span>(<span class="hljs-params">
    self,
    input_ids: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>],
    mm_inputs: MultimodalInputs,
</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:
    <span class="hljs-comment"># Use a pattern that finds contiguous blocks of the image token</span>
    pattern = MultiModalityDataPaddingPatternMultimodalTokens()
    <span class="hljs-keyword">return</span> pattern.pad_input_tokens(input_ids, mm_inputs)
</code></pre>
<p><strong>Pattern 2: Token Pairs</strong></p>
<p>Other models like InternVL and MiniCPM-o use a pair of special tokens to mark the start and end of a region where image features should be inserted (e.g., <code>&lt;image&gt;...&lt;/image&gt;</code>). In this case, <code>MultiModalityDataPaddingPatternTokenPairs</code> is the correct choice. This pattern identifies the content between the start and end tokens and replaces it with the image’s unique hash.</p>
<p>If a model used this pattern, the implementation would look like this:</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># python/sglang/srt/models/some_other_model.py</span>
<span class="hljs-comment"># ...    </span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">pad_input_ids</span>(<span class="hljs-params">
    self,
    input_ids: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>],
    image_inputs: MultimodalInputs,
</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:
    <span class="hljs-comment"># Use a pattern that finds regions enclosed by start/end token pairs     </span>
    pattern = MultiModalityDataPaddingPatternTokenPairs(
        data_token_pairs=[
            (<span class="hljs-variable language_">self</span>.config.image_start_token_id, <span class="hljs-variable language_">self</span>.config.image_end_token_id)
        ],
    )
    <span class="hljs-keyword">return</span> pattern.pad_input_tokens(input_ids, image_inputs)
</code></pre>
<p>By selecting the appropriate padding pattern, you ensure that SGLang’s engine can correctly interpret the multimodal structure of your model’s input.</p>
<h3><a id="handling-image-features" class="anchor" href="#handling-image-features" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Handling Image Features</h3>
<p>Once the input sequence is padded, SGLang’s engine must fetch the actual image features to substitute for the placeholder hashes. This is handled by the <code>get_image_feature</code> method. Its job is to take the raw image data (<code>pixel_values</code>) and generate embeddings that can be combined with the text embeddings.</p>
<p>This process for VILA involves a few steps:</p>
<ol>
<li>The <code>pixel_values</code> are sent to the <code>vision_tower</code>, which is a pre-trained vision encoder (e.g., a Vision Transformer). This extracts a rich feature representation from the image.</li>
<li>The features are then passed through the <code>mm_projector</code>, a small network that aligns the vision features with the language model’s embedding space. The resulting <code>image_embedding</code> is then ready to be used by the model.</li>
</ol>
<p>This function is called by <code>mm_utils.general_mm_embed_routine</code>, a utility in SGLang that manages the process of replacing placeholder hashes with these computed image embeddings before feeding them to the main language model.</p>
<p>Here is the implementation in <code>python/sglang/srt/models/vila.py</code>:</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># python/sglang/srt/models/vila.py    </span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_image_feature</span>(<span class="hljs-params">self, mm_input: <span class="hljs-type">List</span>[MultimodalDataItem]</span>) -&gt; Tensor:
    pixel_values = cast(Tensor, mm_input[<span class="hljs-number">0</span>].pixel_values)
    vision_tower_output: BaseModelOutputWithPooling = <span class="hljs-variable language_">self</span>.vision_tower.__call__(
        pixel_values.to(
            device=<span class="hljs-variable language_">self</span>.vision_tower.device, dtype=<span class="hljs-variable language_">self</span>.vision_tower.dtype
        ),
        output_hidden_states=<span class="hljs-literal">True</span>,
    )
    mm_projector_input = <span class="hljs-variable language_">self</span>._vision_tower_output_to_mm_projector_input(
        vision_tower_output
    )
    image_embedding: Tensor = <span class="hljs-variable language_">self</span>.mm_projector.__call__(
        mm_projector_input.to(
            device=<span class="hljs-variable language_">self</span>.mm_projector.device, dtype=<span class="hljs-variable language_">self</span>.mm_projector.dtype
        )
    )
    <span class="hljs-keyword">return</span> image_embedding
</code></pre>
<p>This modular design ensures that image features are computed and seamlessly integrated into the language model’s input stream.</p>
<h3><a id="defining-the-forward-pass" class="anchor" href="#defining-the-forward-pass" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Defining the <code>forward</code> pass</h3>
<p>The <code>forward</code> method is adapted to work with SGLang’s batching strategy. It takes the combined text and images and processes them through the decoder layers using <code>RadixAttention</code>.</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># python/sglang/srt/models/vila.py</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">
    self,
    input_ids: Tensor,
    positions: Tensor,
    forward_batch: ForwardBatch,
    get_embedding: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,
</span>) -&gt; LogitsProcessorOutput:
    output = mm_utils.general_mm_embed_routine(
        input_ids=input_ids,
        forward_batch=forward_batch,
        language_model=<span class="hljs-variable language_">self</span>.llm,
        image_data_embedding_func=<span class="hljs-variable language_">self</span>.get_image_feature,
        get_embedding=get_embedding,
        positions=positions,
    )
    <span class="hljs-keyword">return</span> cast(LogitsProcessorOutput, output)
</code></pre>
<h3><a id="implementing-load_weights" class="anchor" href="#implementing-load_weights" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Implementing <code>load_weights</code></h3>
<p>Because SGLang uses custom-optimized layers, the <code>load_weights</code> function is responsible for carefully mapping and sometimes transforming weights from a Hugging Face checkpoint to fit the new model structure. This process is highly dependent on the model’s implementation in Transformers.</p>
<p>To load weights incrementally, we recommend using the <code>load_weights</code> method on the submodules. For other weights, <code>weight_utils.default_weight_loader</code> can be used.</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># python/sglang/srt/models/vila.py</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">load_weights</span>(<span class="hljs-params">self, weights: Iterable[<span class="hljs-type">Tuple</span>[<span class="hljs-built_in">str</span>, Tensor]]</span>) -&gt; <span class="hljs-literal">None</span>:
    params_dict = <span class="hljs-built_in">dict</span>(<span class="hljs-variable language_">self</span>.named_parameters())
    <span class="hljs-keyword">for</span> name, loaded_weight <span class="hljs-keyword">in</span> weights:
        <span class="hljs-keyword">if</span> name.startswith(<span class="hljs-string">&quot;llm.&quot;</span>):
            <span class="hljs-variable language_">self</span>.llm.load_weights([(name[<span class="hljs-built_in">len</span>(<span class="hljs-string">&quot;llm.&quot;</span>) :], loaded_weight)])
        <span class="hljs-keyword">else</span>:
            param = params_dict[name]
            weight_loader = <span class="hljs-built_in">getattr</span>(
                param, <span class="hljs-string">&quot;weight_loader&quot;</span>, weight_utils.default_weight_loader
            )
            weight_loader(param, loaded_weight)
</code></pre>
<p>Finally, an <code>EntryClass</code> is added at the end of the file to tell the SGLang server which class is the main entry point for the model.</p>
<pre><code class="hljs language-python"><span class="hljs-comment"># python/sglang/srt/models/vila.py</span>
<span class="hljs-comment"># ... model definition ...</span>
<span class="hljs-comment"># Entry class for the SGLang server</span>
EntryClass = [VILAForConditionalGeneration]
</code></pre>
<h2><a id="step-5-add-integration-tests" class="anchor" href="#step-5-add-integration-tests" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 5: Add Integration Tests</h2>
<p>No integration is complete without thorough testing. It is a best practice to validate the new model implementation in two key ways. For NVILA, we added a test case in <a href="https://github.com/sgl-project/sglang/pull/6106/files#diff-40b31588286beebffb07cbcba6ac68278f65bfab723831352e4084b49d6ab84e"><code>test/srt/test_vision_openai_server_b.py</code></a>.</p>
<h2><a id="conclusion" class="anchor" href="#conclusion" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>Integrating a cutting-edge VLM like NVILA into a high-performance serving engine like SGLang is a detailed yet well-defined process. By replacing key components with SGLang’s optimized versions like <code>RadixAttention</code>, you can serve these powerful models with maximum efficiency and unlock advanced features like multi-user batching.</p>
<p>The key steps are:</p>
<ol>
<li><strong>Configuration</strong>: Registering the model’s multimodal nature and its chat template.</li>
<li><strong>Data Handling</strong>: Creating a dedicated processor to manage image and text inputs.</li>
<li><strong>Model Definition</strong>: Porting the architecture, replacing standard layers with SGLang’s optimized versions, and correctly handling multimodal inputs.</li>
<li><strong>Testing</strong>: Rigorously verifying the implementation against reference outputs and adding integration tests.</li>
</ol>
<p>We hope this detailed walkthrough has demystified the process and encourages you to contribute to the exciting open-source development happening at SGLang.</p>
<h2><a id="acknowledgements" class="anchor" href="#acknowledgements" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgements</h2>
<p>We thank all contributors for their efforts in developing and integrating NVILA into SGLang.</p>
<p><strong>NVILA Team:</strong> Zijian Zhang, Ligeng Zhu</p>
<p><strong>SGLang Community:</strong> Mick Qian, Xinyuan Tong, Qiujiang Chen, Xinpeng Wei, Chenyang Zhao</p>
<hr>
<p><strong>Further Reading:</strong></p>
<ul>
<li><a href="https://github.com/sgl-project/sglang">SGLang on GitHub</a></li>
<li><a href="https://docs.sglang.ai/supported_models/support_new_models.html">How to Support New Models in SGLang</a></li>
<li><a href="https://github.com/sgl-project/sglang/pull/6106">SGLang PR for NVILA integration</a></li>
<li><a href="https://github.com/sgl-project/sglang/pull/7629">SGLang PR for multimodal processors</a></li>
</ul>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"How to support new VLMs into SGLang: A Case Study with NVILA","author":"The NVILA Team","date":"July 16, 2025","previewImg":"/images/blog/nvila/preview.png"},"content":"\nThe world of LLMs is evolving at a remarkable pace, with Visual Language Models (VLMs) at the forefront of this revolution. These models power applications that can understand and reason about both images and text. There are [tons of new VLM models](https://huggingface.co/models?pipeline_tag=image-text-to-text\u0026sort=trending) emerging daily, and we want to integrate them into [SGLang](https://github.com/sgl-project/sglang) to leverage its high-speed throughput. Today, we’ll provide a step-by-step walkthrough for integrating new VLMs into the SGLang ecosystem, using the recent [NVILA model](https://arxiv.org/abs/2412.04468) as a real-world case study.\n\n## Accelerating the NVILA Visual Language Model with SGLang\n\nThe benchmarks below compare the original VILA implementation against SGLang with different levels of concurrency \n\n![image.png](/images/blog/nvila/image.png)\n\nIn real world VLM development, we focus on two important metrics to evaluate a serving systems’ performance: **Throughput (Token per Second, TPS)** and and **Time to First Token (TTFT)**. \n\n- For TPS, higher throughput means the system can generate more tokens simultaneously . SGLang's `RadixAttention` allows for efficient batching of requests, dramatically increasing the number of tokens generated per second. With a concurrency of 8, SGLang achieves over **4.4x higher throughput**.\n- For TTFT, a lower value means users get a faster response to receive the first token. SGLang's memory optimizations and efficient kernel implementations significantly reduce prefill latency. The benchmark shows SGLang responses up to **2.2x faster** when concurrency is 8.\n\nThese performance gains make SGLang an excellent choice for deploying demanding VLMs like NVILA in production environments, and now can be easily deployed for sglang version ≥ 0.4.8\n\n```bash\npython3 -m sglang.launch_server \\\n  --model-path Efficient-Large-Model/NVILA-Lite-2B-hf-0626 \\\n  --host 0.0.0.0 --port 30000 --trust-remote-code \\\n  --attention-backend fa3\n```\n\n## The Big Picture: How VLMs like NVILA Work\n\nBefore diving into code, it helps to understand what is happening under the hood. Most Vision–Language Models (llava-based) share a three-component architecture:\n\n1. **Vision encoder** – a convolutional network or, more commonly, a Vision Transformer (ViT) that converts pixels into a sequence of visual tokens.\n2. **Projector** – a lightweight adapter (often an MLP) that aligns those tokens with the embedding space of a pretrained language model.\n3. **Token processor** – the language model itself, which mixes visual and textual tokens and autoregressively generates the answer.\n\n[NVILA follows the paradigm](https://nvlabs.github.io/VILA) while pushing the efficiency frontier through its ***scale-then-compress*** strategy. It first *scales* the spatial and temporal resolution so the vision tower sees high-fidelity images or long videos, then *compresses* the resulting token stream so the language model only attends to a handful of high-information tokens:\n\n- **High-resolution SigLIP vision tower** captures fine-grained details from images and multi-frame clips.\n- **Spatial token pooling** reduces dense patches to a much smaller grid, preserving text and structure while cutting quadratic attention cost.\n- **Temporal pooling** keeps just the most informative frames, so video inputs add only a few extra tokens.\n- **Two-layer MLP projector** maps the compressed visual embeddings into LLM’s  embedding space.\n- **FP8 training, dataset pruning, and memory-efficient fine-tuning** trim training cost by up to 5× and deliver sub-second prefilling on a single 4090 GPU.\n\n![NVILA Architecture](https://arxiv.org/html/2412.04468v2/x5.png)\n\nNVILA Architecture\n\nThis blueprint is generic enough to cover most modern VLMs, yet the compression blocks highlighted in green are what make NVILA both *accurate* and *fast* at scale.\n\n## Supporting New Models in SGLang\n\nSGLang has a streamlined process for adding new models, which centers on creating a single model definition file that utilizes SGLang’s core components. The key is to replace standard Hugging Face components with their SGLang-optimized counterparts. For VLMs, this involves a few steps.\n\n## Step 1: Register the Model as Multimodal\n\nFirst, SGLang needs to identify your model as multimodal. This is handled in `sglang/srt/configs/model_config.py`. SGLang determines this by checking if the model’s architecture class name is present in the `multimodal_model_archs` list.\n\nFor NVILA, we add `\"VILAForConditionalGeneration\"` to this list. This tells SGLang that any model with this architecture should be treated as a multimodal model.\n\n```python\n# python/sglang/srt/configs/model_config.py\n# ... existing code ...\nmultimodal_model_archs = [\n    # ... existing code ...    \n    \"VILAForConditionalGeneration\",\n]\n# ... existing code ...\n```\n\n## Step 2: Register a New Chat Template\n\nVLMs often require specific chat templates to handle prompts containing both images and text. SGLang allows you to either define a new template or, more conveniently, match your model to an existing one.\n\nFor NVILA, which uses a format similar to ChatML, we can reuse the existing `chatml` conversation template. To associate our model with this template, we register a matching function in `python/sglang/srt/conversation.py`. This function, `match_vila`, inspects the model path and returns `\"chatml\"` if it finds a match, telling SGLang to apply the ChatML format for NVILA models.\n\n```python\n# python/sglang/srt/conversation.py\n# ... existing code ...\n\n@register_conv_template_matching_function\ndef match_vila(model_path: str):\n    # ... existing code ...\n    if re.search(r\"vila\", model_path, re.IGNORECASE):\n        return \"chatml\"\n```\n\nThis registration instructs SGLang on how to format the input string and where to expect image data.\n\nTo make this concrete, let’s examine the template structure that `chatml` provides and how it aligns with NVILA’s needs.\n\n### Understanding the ChatML Template for NVILA\n\nThe ChatML format, which NVILA uses, employs special tokens to structure the conversation. For a multimodal prompt, the template ensures that both text and images are correctly placed.\n\nA typical conversation is a sequence of messages, each with a `role` (`system`, `user`, or `assistant`) and `content`. Here’s how a user’s query with an image is formatted:\n\n**Example User Message:**\n\n```json\n{\n  \"role\": \"user\",\n  \"content\": [\n    { \"type\": \"text\", \"text\": \"What is in this image?\" },\n    { \"type\": \"image\" }\n  ]\n}\n```\n\n**Resulting Prompt String:**\n\nThe template processor converts this into a single string for the model:\n\n```\n\u003c|im_start|\u003esystem\nYou are a helpful assistant.\u003c|im_end|\u003e\n\u003c|im_start|\u003euser\nWhat is in this image?\u003cimage\u003e\u003c|im_end|\u003e\n\u003c|im_start|\u003eassistant\n```\n\nHere’s a breakdown of the components:\n- `\u003c|im_start|\u003e{role}` and `\u003c|im_end|\u003e`: These tokens define the boundaries of each message.\n- A default system prompt is automatically added if not provided.\n- Text content is included as-is.\n- Image content is replaced with a special `\u003cimage\u003e` placeholder token.\n\nBy matching NVILA to the `chatml` template, SGLang automatically handles this formatting. The engine knows to replace the `\u003cimage\u003e` placeholder with the actual image features during processing, a process we will cover in Step 3.\n\nIf your model requires a completely new chat template, you can define it directly in the same file. For example, if NVILA had used a unique format, we would create a new conversation template:\n\n```python\n# python/sglang/srt/conversation.py\n# ... existing code ...\nregister_conv_template(\n    Conversation(\n        name=\"vila\",\n        system_template=\"\u003c|im_start|\u003esystem\\\\n{system_message}\",\n        system_message=\"You are a helpful assistant.\",\n        roles=(\"\u003c|im_start|\u003euser\", \"\u003c|im_start|\u003eassistant\"),\n        sep_style=SeparatorStyle.CHATML,\n        sep=\"\u003c|im_end|\u003e\",\n        stop_str=[\"\u003c|endoftext|\u003e\", \"\u003c|im_end|\u003e\"],\n    )\n)\n```\n\n## Step 3: Building the Multimodal Data Processor\n\nThe data processor is the heart of the multimodal integration. It acts as the bridge that takes a user’s raw input—a mix of text and images—and meticulously prepares it for the model. For our NVILA integration, we’ll create a `VILAMultimodalProcessor` in `python/sglang/srt/managers/multimodal_processors/vila.py`. See [here](https://github.com/sgl-project/sglang/pull/6106/files#diff-d9c481aa1e61e56644d3e89901895dafa9afd7e9d1147d48ab60c14435196bdb) for the full diff.\n\nThis class inherits from SGLang’s `BaseMultimodalProcessor`, and its main workhorse is the `process_mm_data_async` method. Let’s build this method step-by-step to see how it transforms raw data into a model-ready format.\n\n### The Processor’s Skeleton\n\nFirst define the class. A crucial component is the `models` class attribute, which links the processor to its corresponding SGLang model class. This registration allows SGLang to  to recognize the associated processor of the model when loading  `VILAForConditionalGeneration`. The `__init__` method is also updated to cache special token IDs like `image_token_id` directly from the model's configuration. The processor's main method, `process_mm_data_async`, then takes the raw request details and returns the processed data that the SGLang engine can understand.\n\n```python\n# python/sglang/srt/multimodal/processors/vila.py\n# ... imports ...\nclass VILAMultimodalProcessor(BaseMultimodalProcessor):\n    models: List[Type[nn.Module]] = [VILAForConditionalGeneration]\n\n    # This holds the Hugging Face processor\n    _processor: VILAProcessor\n\n    def __init__(\n        self,\n        hf_config: PretrainedConfig,\n        server_args: ServerArgs,\n        _processor: VILAProcessor,\n    ) -\u003e None:\n        super().__init__(hf_config, server_args, _processor)\n        # Store special token IDs\n        self.IM_TOKEN_ID = hf_config.image_token_id\n        self.VIDEO_TOKEN_ID = hf_config.video_token_id\n\n    async def process_mm_data_async(\n        self,\n        image_data: Optional[ImageDataItem | List[ImageDataItem]],\n        input_text: str | List[int],\n        request_obj: GenerateReqInput | EmbeddingReqInput,\n        max_req_input_len: int,\n        **kwargs,\n    ) -\u003e Optional[Dict[str, Any]]:\n        # ... implementation below ...\n```\n\n### From Raw Input to Processed Data\n\nInside `process_mm_data_async`, a streamlined pipeline handles the inputs.\n\n1. **Load and Process**: The logic consolidates data handling. We first call `self.load_mm_data` from the base class to load image files and locate the special `\u003cimage\u003e` tokens in the prompt. This output is then passed to `self.process_and_combine_mm_data`, a method that encapsulates the logic for tokenizing text, transforming images, and packaging the data.\n2. **Assemble the Final Package**: This process directly produces the final dictionary for the SGLang engine. This dictionary contains the tokenized `input_ids`, the assembled `mm_items` (which include processed image data and their locations), and explicitly includes the `im_token_id` and `video_token_id`.\n\nHere’s the complete implementation of `process_mm_data_async`:\n\n```python\n# (Inside VILAMultimodalProcessor)\nasync def process_mm_data_async(\n    self,\n    image_data: Optional[ImageDataItem | List[ImageDataItem]],\n    input_text: str | List[int],\n    request_obj: GenerateReqInput | EmbeddingReqInput,\n    max_req_input_len: int,\n    **kwargs,\n) -\u003e Optional[Dict[str, Any]]:\n    # Load image data and find special tokens\n    base_output = self.load_mm_data(\n        prompt=input_text,\n        multimodal_tokens=MultimodalSpecialTokens(\n            image_token=self._processor.tokenizer.image_token\n        ),\n        max_req_input_len=max_req_input_len,\n        image_data=image_data,\n    )\n\n    # Process images and text, then combine into final format\n    mm_items, input_ids = self.process_and_combine_mm_data(base_output)\n\n    # Return the complete package for the engine\n    return {\n        \"input_ids\": input_ids.tolist(),\n        \"mm_items\": mm_items,\n        \"im_token_id\": self.IM_TOKEN_ID,\n        \"video_token_id\": self.VIDEO_TOKEN_ID,\n    }\n```\n\nAnd that’s it! This clean design centralizes the core processing logic, making the processor more maintainable and easier to extend.\n\n## Step 4: Create the Core Model Definition\n\nThe final major step is to define the model’s main class in a new file, `python/sglang/srt/models/vila.py`. This involves porting the Hugging Face model by replacing standard layers with SGLang’s high-performance equivalents.\n\n### Adapting Attention Mechanisms\n\nTo unlock maximum performance, standard attention layers are replaced:\n\n- **Language Model Attention**: Original `Attention` layers are replaced with SGLang’s `RadixAttention`. This is the key to SGLang’s high-performance batching and memory management.\n- **Vision Model Attention**: `Attention` layers in the Vision Transformer (ViT) are replaced with SGLang’s `VisionAttention` to manage the vision tower’s KV cache.\n\nNVILA uses Qwen2 as its language model, thus we directly adapt SGLang’s `Qwen2ForCausalLM` class and do not need to manually replace the attention layers. See the diff [here](https://github.com/sgl-project/sglang/pull/6106/files#diff-bd699d67711317806e08a0811bdf770194e95bee8b6ebe2d972ffa5e93c5a4e0).\n\n### Handling Multimodal Inputs with `pad_input_ids`\n\nThe `pad_input_ids` function is a critical component for handling multimodal inputs in SGLang. Its primary role is to prepare the input token sequence for `RadixAttention`. It achieves this by replacing placeholder tokens for images (or other modalities) with unique multimodal data hashes. These hashes, or `pad_value`, allow `RadixAttention` to correctly associate segments of the input with the corresponding image data, even when processing multiple requests in a single batch. This is essential for efficient multi-user serving.\n\nSGLang provides different padding strategy tool classes to accommodate various ways models represent multimodal inputs. The choice of pattern depends on how the model’s tokenizer and chat template are designed.\n\n**Pattern 1: Contiguous Multimodal Tokens**\n\nSome models like NVILA and Kimi-VL represent an image with a contiguous block of special tokens (e.g., `\u003cimage\u003e\u003cimage\u003e...\u003cimage\u003e`). For these models, we use `MultiModalityDataPaddingPatternMultimodalTokens`. The `pad_input_ids` function signature is generalized to accept `mm_inputs`, which can contain various modalities. For NVILA, the implementation is straightforward and concise:\n\n```python\n# python/sglang/srt/models/vila.py\n# ...\ndef pad_input_ids(\n    self,\n    input_ids: List[int],\n    mm_inputs: MultimodalInputs,\n) -\u003e List[int]:\n    # Use a pattern that finds contiguous blocks of the image token\n    pattern = MultiModalityDataPaddingPatternMultimodalTokens()\n    return pattern.pad_input_tokens(input_ids, mm_inputs)\n```\n\n**Pattern 2: Token Pairs**\n\nOther models like InternVL and MiniCPM-o use a pair of special tokens to mark the start and end of a region where image features should be inserted (e.g., `\u003cimage\u003e...\u003c/image\u003e`). In this case, `MultiModalityDataPaddingPatternTokenPairs` is the correct choice. This pattern identifies the content between the start and end tokens and replaces it with the image’s unique hash.\n\nIf a model used this pattern, the implementation would look like this:\n\n```python\n# python/sglang/srt/models/some_other_model.py\n# ...    \ndef pad_input_ids(\n    self,\n    input_ids: List[int],\n    image_inputs: MultimodalInputs,\n) -\u003e List[int]:\n    # Use a pattern that finds regions enclosed by start/end token pairs     \n    pattern = MultiModalityDataPaddingPatternTokenPairs(\n        data_token_pairs=[\n            (self.config.image_start_token_id, self.config.image_end_token_id)\n        ],\n    )\n    return pattern.pad_input_tokens(input_ids, image_inputs)\n```\n\nBy selecting the appropriate padding pattern, you ensure that SGLang’s engine can correctly interpret the multimodal structure of your model’s input.\n\n### Handling Image Features\n\nOnce the input sequence is padded, SGLang’s engine must fetch the actual image features to substitute for the placeholder hashes. This is handled by the `get_image_feature` method. Its job is to take the raw image data (`pixel_values`) and generate embeddings that can be combined with the text embeddings.\n\nThis process for VILA involves a few steps:\n\n1. The `pixel_values` are sent to the `vision_tower`, which is a pre-trained vision encoder (e.g., a Vision Transformer). This extracts a rich feature representation from the image.\n2. The features are then passed through the `mm_projector`, a small network that aligns the vision features with the language model’s embedding space. The resulting `image_embedding` is then ready to be used by the model.\n\nThis function is called by `mm_utils.general_mm_embed_routine`, a utility in SGLang that manages the process of replacing placeholder hashes with these computed image embeddings before feeding them to the main language model.\n\nHere is the implementation in `python/sglang/srt/models/vila.py`:\n\n```python\n# python/sglang/srt/models/vila.py    \ndef get_image_feature(self, mm_input: List[MultimodalDataItem]) -\u003e Tensor:\n    pixel_values = cast(Tensor, mm_input[0].pixel_values)\n    vision_tower_output: BaseModelOutputWithPooling = self.vision_tower.__call__(\n        pixel_values.to(\n            device=self.vision_tower.device, dtype=self.vision_tower.dtype\n        ),\n        output_hidden_states=True,\n    )\n    mm_projector_input = self._vision_tower_output_to_mm_projector_input(\n        vision_tower_output\n    )\n    image_embedding: Tensor = self.mm_projector.__call__(\n        mm_projector_input.to(\n            device=self.mm_projector.device, dtype=self.mm_projector.dtype\n        )\n    )\n    return image_embedding\n```\n\nThis modular design ensures that image features are computed and seamlessly integrated into the language model’s input stream.\n\n### Defining the `forward` pass\n\nThe `forward` method is adapted to work with SGLang’s batching strategy. It takes the combined text and images and processes them through the decoder layers using `RadixAttention`.\n\n```python\n# python/sglang/srt/models/vila.py\ndef forward(\n    self,\n    input_ids: Tensor,\n    positions: Tensor,\n    forward_batch: ForwardBatch,\n    get_embedding: bool = False,\n) -\u003e LogitsProcessorOutput:\n    output = mm_utils.general_mm_embed_routine(\n        input_ids=input_ids,\n        forward_batch=forward_batch,\n        language_model=self.llm,\n        image_data_embedding_func=self.get_image_feature,\n        get_embedding=get_embedding,\n        positions=positions,\n    )\n    return cast(LogitsProcessorOutput, output)\n```\n\n### Implementing `load_weights`\n\nBecause SGLang uses custom-optimized layers, the `load_weights` function is responsible for carefully mapping and sometimes transforming weights from a Hugging Face checkpoint to fit the new model structure. This process is highly dependent on the model’s implementation in Transformers.\n\nTo load weights incrementally, we recommend using the `load_weights` method on the submodules. For other weights, `weight_utils.default_weight_loader` can be used.\n\n```python\n# python/sglang/srt/models/vila.py\ndef load_weights(self, weights: Iterable[Tuple[str, Tensor]]) -\u003e None:\n    params_dict = dict(self.named_parameters())\n    for name, loaded_weight in weights:\n        if name.startswith(\"llm.\"):\n            self.llm.load_weights([(name[len(\"llm.\") :], loaded_weight)])\n        else:\n            param = params_dict[name]\n            weight_loader = getattr(\n                param, \"weight_loader\", weight_utils.default_weight_loader\n            )\n            weight_loader(param, loaded_weight)\n```\n\nFinally, an `EntryClass` is added at the end of the file to tell the SGLang server which class is the main entry point for the model.\n\n```python\n# python/sglang/srt/models/vila.py\n# ... model definition ...\n# Entry class for the SGLang server\nEntryClass = [VILAForConditionalGeneration]\n```\n\n## Step 5: Add Integration Tests\n\nNo integration is complete without thorough testing. It is a best practice to validate the new model implementation in two key ways. For NVILA, we added a test case in [`test/srt/test_vision_openai_server_b.py`](https://github.com/sgl-project/sglang/pull/6106/files#diff-40b31588286beebffb07cbcba6ac68278f65bfab723831352e4084b49d6ab84e).\n\n## Conclusion\n\nIntegrating a cutting-edge VLM like NVILA into a high-performance serving engine like SGLang is a detailed yet well-defined process. By replacing key components with SGLang’s optimized versions like `RadixAttention`, you can serve these powerful models with maximum efficiency and unlock advanced features like multi-user batching.\n\nThe key steps are:\n\n1. **Configuration**: Registering the model’s multimodal nature and its chat template.\n2. **Data Handling**: Creating a dedicated processor to manage image and text inputs.\n3. **Model Definition**: Porting the architecture, replacing standard layers with SGLang’s optimized versions, and correctly handling multimodal inputs.\n4. **Testing**: Rigorously verifying the implementation against reference outputs and adding integration tests.\n\nWe hope this detailed walkthrough has demystified the process and encourages you to contribute to the exciting open-source development happening at SGLang.\n\n## Acknowledgements\n\nWe thank all contributors for their efforts in developing and integrating NVILA into SGLang.\n\n**NVILA Team:** Zijian Zhang, Ligeng Zhu\n\n**SGLang Community:** Mick Qian, Xinyuan Tong, Qiujiang Chen, Xinpeng Wei, Chenyang Zhao\n\n---\n\n**Further Reading:**\n\n- [SGLang on GitHub](https://github.com/sgl-project/sglang)\n- [How to Support New Models in SGLang](https://docs.sglang.ai/supported_models/support_new_models.html)\n- [SGLang PR for NVILA integration](https://github.com/sgl-project/sglang/pull/6106)\n- [SGLang PR for multimodal processors](https://github.com/sgl-project/sglang/pull/7629)\n","slug":"2025-07-16-nvila"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2025-07-16-nvila"},"buildId":"wpBETYUmFzM_IORRWVyLq","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>