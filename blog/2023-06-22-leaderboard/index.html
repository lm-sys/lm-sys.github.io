<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Chatbot Arena Leaderboard Week 8: Introducing MT-Bench and Vicuna-33B | LMSYS Org</title><meta name="title" content="Chatbot Arena Leaderboard Week 8: Introducing MT-Bench and Vicuna-33B | LMSYS Org"/><meta property="og:title" content="Chatbot Arena Leaderboard Week 8: Introducing MT-Bench and Vicuna-33B | LMSYS Org"/><meta name="twitter:title" content="Chatbot Arena Leaderboard Week 8: Introducing MT-Bench and Vicuna-33B | LMSYS Org"/><meta name="description" content="&lt;p&gt;In this blog post, we share the latest update on Chatbot Arena leaderboard, which now includes more open models and three metrics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Ch..."/><meta property="og:description" content="&lt;p&gt;In this blog post, we share the latest update on Chatbot Arena leaderboard, which now includes more open models and three metrics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Ch..."/><meta name="twitter:description" content="&lt;p&gt;In this blog post, we share the latest update on Chatbot Arena leaderboard, which now includes more open models and three metrics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Ch..."/><meta property="og:image" content="https://lmsys.org/images/blog/leaderboard_week8/ability_breakdown.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/leaderboard_week8/ability_breakdown.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2023-06-22-leaderboard"/><meta name="twitter:url" content="https://lmsys.org/blog/2023-06-22-leaderboard"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="19"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/9aa18d40847551db.css" as="style"/><link rel="stylesheet" href="/_next/static/css/9aa18d40847551db.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-a0cb9ddcff62d761.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-3d139df5a55b1694.js" defer=""></script><script src="/_next/static/RgGbYtj92MeqCJ2OZxXwk/_buildManifest.js" defer=""></script><script src="/_next/static/RgGbYtj92MeqCJ2OZxXwk/_ssgManifest.js" defer=""></script><script src="/_next/static/RgGbYtj92MeqCJ2OZxXwk/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://arena.lmsys.org" target="_blank" rel="noopener noreferrer">Chatbot Arena</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://arena.lmsys.org" target="_blank" rel="noopener noreferrer">Chatbot Arena</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">Chatbot Arena Leaderboard Week 8: Introducing MT-Bench and Vicuna-33B</h1><p class="text-xl pt-2 pb-2">by: <!-- -->Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Hao Zhang<!-- -->,<!-- --> <!-- -->Jun 22, 2023<!-- --></p><hr/><div class="pt-2 article"><p>In this blog post, we share the latest update on Chatbot Arena leaderboard, which now includes more open models and three metrics:</p>
<ol>
<li><strong>Chatbot Arena Elo</strong>, based on 42K anonymous votes from <a href="https://lmsys.org/blog/2023-05-03-arena/">Chatbot Arena</a> using the Elo rating system.</li>
<li><strong>MT-Bench score</strong>, based on a challenging multi-turn benchmark and GPT-4 grading, proposed and validated in our <a href="https://arxiv.org/abs/2306.05685">Judging LLM-as-a-judge paper</a>.</li>
<li><strong>MMLU</strong>, a widely adopted <a href="https://arxiv.org/abs/2009.03300">benchmark</a>.</li>
</ol>
<p>Furthermore, we’re excited to introduce our <strong>new series of Vicuna-v1.3 models</strong>, ranging from 7B to 33B parameters, trained on an extended set of user-shared conversations.
Their weights are now <a href="https://github.com/lm-sys/FastChat/tree/main#vicuna-weights">available</a>.</p>
<h2><a id="updated-leaderboard-and-new-models" class="anchor" href="#updated-leaderboard-and-new-models" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Updated Leaderboard and New Models</h2>
<style>
th {text-align: left}
td {text-align: left}

table {
  border-collapse: collapse;
  width: 100%;
}


th {
  cursor: pointer;
}

th:hover {
  background-color: #ddd;
}

.arrow {
  display: inline-block;
  width: 0;
  height: 0;
  vertical-align: middle;
  margin-left: 5px;
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
}

.arrow-up {
  border-bottom: 5px solid #000;
}

.arrow-down {
  border-top: 5px solid #000;
}

/* Initially sort arrow for descending order */
th:nth-child(1) .arrow-down {
  border-top: 5px solid #000;
}
</style>
<script>
    let sortOrder = ['desc', undefined, undefined];

    function sortTable(columnIndex, table_id) {
      let table, rows, switching, i, x, y, shouldSwitch;
      table = document.getElementById(table_id);
      switching = true;
      let sortAsc = sortOrder[columnIndex] === 'asc';

      while (switching) {
        switching = false;
        rows = table.getElementsByTagName("tr");

        for (i = 1; i < (rows.length - 1); i++) {
          shouldSwitch = false;
          x = rows[i].getElementsByTagName("td")[columnIndex];
          y = rows[i + 1].getElementsByTagName("td")[columnIndex];
          x_char = x.innerHTML.toLowerCase();
          y_char = y.innerHTML.toLowerCase();
          if (sortAsc) {
            if (x_char === "-") {
                x_val = 9999;
            } else {
                x_val = Number(x_char);
            }
            if (y_char === "-") {
                y_val = 9999;
            } else {
                y_val = Number(y_char);
            }
            if (x_val > y_val) {
              shouldSwitch = true;
              break;
            }
          } else {
            if (x_char === "-") {
                x_val = 0.0;
            } else {
                x_val = Number(x_char);
            }
            if (y_char === "-") {
                y_val = 0.0;
            } else {
                y_val = Number(y_char);
            }

            if (x_val < y_val) {
              shouldSwitch = true;
              break;
            }
          }
        }

        if (shouldSwitch) {
          rows[i].parentNode.insertBefore(rows[i + 1], rows[i]);
          switching = true;
        }
      }

      let arrowElements = document.getElementsByClassName("arrow");
      for (let j = 0; j < arrowElements.length; j++) {
        arrowElements[j].classList.remove("arrow-up", "arrow-down");
      }

      let arrowElement = document.getElementsByTagName("th")[columnIndex].getElementsByClassName("arrow")[0];
      arrowElement.classList.add(sortAsc ? "arrow-up" : "arrow-down");
      sortOrder[columnIndex] = sortAsc ? 'desc' : 'asc';
    }
</script>
<br>
<p style="color:gray; text-align: center;">Table 1. LLM Leaderboard (Timeframe: April 24 - June 19, 2023). The latest and detailed version <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard" target="_blank">here</a>.</p>
<div style="display: flex; justify-content: center;">
<table id="Table1" >
<tbody>
<tr> <th>Model</th> <th onclick="sortTable(1, 'Table1')">MT-bench (score) <span class="arrow arrow-down"></span></th> <th onclick="sortTable(2, 'Table1')">Arena Elo Rating <span class="arrow"></span></th> <th onclick="sortTable(3, 'Table1')">MMLU <span class="arrow"></span></th> <th>License</th> </tr>
<tr> <td><a target="_blank" href="https://openai.com/research/gpt-4"> GPT-4 </a></td>  <td>8.99</td>  <td>1227</td>  <td>86.4</td>  <td>Proprietary</td> </tr>
<tr> <td><a target="_blank" href="https://openai.com/blog/chatgpt"> GPT-3.5-turbo </a></td>  <td>7.94</td>  <td>1130</td>  <td>70.0</td>  <td>Proprietary</td> </tr>
<tr> <td><a target="_blank" href="https://www.anthropic.com/index/introducing-claude"> Claude-v1 </a></td>  <td>7.90</td>  <td>1178</td>  <td>75.6</td>  <td>Proprietary</td> </tr>
<tr> <td><a target="_blank" href="https://www.anthropic.com/index/introducing-claude"> Claude-instant-v1 </a></td>  <td>7.85</td>  <td>1156</td>  <td>61.3</td>  <td>Proprietary</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/lmsys/vicuna-33b-v1.3"> Vicuna-33B </a></td>  <td>7.12</td>  <td>-</td>  <td>59.2</td>  <td>Non-commercial</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/WizardLM/WizardLM-30B-V1.0"> WizardLM-30B </a></td>  <td>7.01</td>  <td>-</td>  <td>58.7</td>  <td>Non-commercial</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/timdettmers/guanaco-33b-merged"> Guanaco-33B </a></td>  <td>6.53</td>  <td>1065</td>  <td>57.6</td>  <td>Non-commercial</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/allenai/tulu-30b"> Tulu-30B </a></td>  <td>6.43</td>  <td>-</td>  <td>58.1</td>  <td>Non-commercial</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/timdettmers/guanaco-65b-merged"> Guanaco-65B </a></td>  <td>6.41</td>  <td>-</td>  <td>62.1</td>  <td>Non-commercial</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor"> OpenAssistant-LLaMA-30B </a></td>  <td>6.41</td>  <td>-</td>  <td>56.0</td>  <td>Non-commercial</td> </tr>
<tr> <td><a target="_blank" href="https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#foundation_models"> PaLM-Chat-Bison-001 </a></td>  <td>6.40</td>  <td>1038</td>  <td>-</td>  <td>Proprietary</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/lmsys/vicuna-13b-v1.3"> Vicuna-13B </a></td>  <td>6.39</td>  <td>1061</td>  <td>52.1</td>  <td>Non-commercial</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-30b-chat"> MPT-30B-chat </a></td>  <td>6.39</td>  <td>-</td>  <td>50.4</td>  <td>CC-BY-NC-SA-4.0</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/WizardLM/WizardLM-13B-V1.0"> WizardLM-13B </a></td>  <td>6.35</td>  <td>1048</td>  <td>52.3</td>  <td>Non-commercial</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/lmsys/vicuna-7b-v1.3"> Vicuna-7B </a></td>  <td>6.00</td>  <td>1008</td>  <td>47.1</td>  <td>Non-commercial</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/project-baize/baize-v2-13b"> Baize-v2-13B </a></td>  <td>5.75</td>  <td>-</td>  <td>48.9</td>  <td>Non-commercial</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/NousResearch/Nous-Hermes-13b"> Nous-Hermes-13B </a></td>  <td>5.51</td>  <td>-</td>  <td>49.3</td>  <td>Non-commercial</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-7b-chat"> MPT-7B-Chat </a></td>  <td>5.42</td>  <td>956</td>  <td>32.0</td>  <td>CC-BY-NC-SA-4.0</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/nomic-ai/gpt4all-13b-snoozy"> GPT4All-13B-Snoozy </a></td>  <td>5.41</td>  <td>986</td>  <td>43.0</td>  <td>Non-commercial</td> </tr>
<tr> <td><a target="_blank" href="https://bair.berkeley.edu/blog/2023/04/03/koala/"> Koala-13B </a></td>  <td>5.35</td>  <td>992</td>  <td>44.7</td>  <td>Non-commercial</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-30b-instruct"> MPT-30B-Instruct </a></td>  <td>5.22</td>  <td>-</td>  <td>47.8</td>  <td>CC-BY-SA 3.0</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/tiiuae/falcon-40b-instruct"> Falcon-40B-Instruct </a></td>  <td>5.17</td>  <td>-</td>  <td>54.7</td>  <td>Apache 2.0</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-13b"> H2O-Oasst-OpenLLaMA-13B </a></td>  <td>4.63</td>  <td>-</td>  <td>42.8</td>  <td>Apache 2.0</td> </tr>
<tr> <td><a target="_blank" href="https://crfm.stanford.edu/2023/03/13/alpaca.html"> Alpaca-13B </a></td>  <td>4.53</td>  <td>930</td>  <td>48.1</td>  <td>Non-commercial</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/THUDM/chatglm-6b"> ChatGLM-6B </a></td>  <td>4.50</td>  <td>905</td>  <td>36.1</td>  <td>Non-commercial</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5"> OpenAssistant-Pythia-12B </a></td>  <td>4.32</td>  <td>924</td>  <td>27.0</td>  <td>Apache 2.0</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/BlinkDL/rwkv-4-raven"> RWKV-4-Raven-14B </a></td>  <td>3.98</td>  <td>950</td>  <td>25.6</td>  <td>Apache 2.0</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/databricks/dolly-v2-12b"> Dolly-V2-12B </a></td>  <td>3.28</td>  <td>850</td>  <td>25.7</td>  <td>MIT</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/lmsys/fastchat-t5-3b-v1.0"> FastChat-T5-3B </a></td>  <td>3.04</td>  <td>897</td>  <td>47.7</td>  <td>Apache 2.0</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b"> StableLM-Tuned-Alpha-7B </a></td>  <td>2.75</td>  <td>871</td>  <td>24.4</td>  <td>CC-BY-NC-SA-4.0</td> </tr>
<tr> <td><a target="_blank" href="https://arxiv.org/abs/2302.13971"> LLaMA-13B </a></td>  <td>2.61</td>  <td>826</td>  <td>47.0</td>  <td>Non-commercial</td> </tr>
</tbody>
</table>
</div>
<p>­</p>
<p>Welcome to try the Chatbot Arena voting <a href="https://chat.lmsys.org/?arena">demo</a>.
Keep in mind that each benchmark has its limitations. Please consider the results as guiding references. See our discussion below for more technical details.</p>
<h2><a id="evaluating-chatbots-with-mt-bench-and-arena" class="anchor" href="#evaluating-chatbots-with-mt-bench-and-arena" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Evaluating Chatbots with MT-bench and Arena</h2>
<h3><a id="motivation" class="anchor" href="#motivation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Motivation</h3>
<p>While several benchmarks exist for evaluating Large Language Model's (LLM) performance, such as <a href="https://arxiv.org/abs/2009.03300">MMLU</a>, <a href="https://arxiv.org/abs/1905.07830">HellaSwag</a>, and <a href="https://github.com/openai/human-eval">HumanEval</a>,
we noticed that these benchmarks might fall short when assessing LLMs' human preferences.
Traditional benchmarks often test LLMs on close-ended questions with concise outputs (e.g., multiple choices), which do not reflect the typical use cases of LLM-based chat assistants.</p>
<p>To fill this gap, in this leaderboard update, in addition to the Chatbot Arena Elo system, we add a new benchmark: MT-Bench.</p>
<ul>
<li><a href="https://arxiv.org/abs/2306.05685">MT-bench</a> is a challenging multi-turn question set designed to evaluate the conversational and instruction-following ability of models. You can view sample questions and answers of MT-bench <a href="https://huggingface.co/spaces/lmsys/mt-bench">here</a>.</li>
<li><a href="https://chat.lmsys.org/?arena">Chatbot Arena</a> is a crowd-sourced battle platform, where users ask chatbots any question and vote for their preferred answer.</li>
</ul>
<p>Both benchmarks are designed to use human preferences as the primary metric.</p>
<h3><a id="why-mt-bench" class="anchor" href="#why-mt-bench" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why MT-Bench?</h3>
<p>MT-Bench is a carefully curated benchmark that includes 80 high-quality, multi-turn questions.
These questions are tailored to assess the conversation flow and instruction-following capabilities of models in multi-turn dialogues.
They include both common use cases and challenging instructions meant to distinguish between chatbots.
MT-Bench serves as a <strong>quality-controlled complement</strong> to our crowd-sourced based evaluation -- Chatbot Arena.</p>
<p>Through running the Chatbot Arena for 2 months and analyzing our users' prompts, we've identified 8 primary categories of user prompts: Writing, Roleplay, Extraction, Reasoning, Math, Coding, Knowledge I (STEM), and Knowledge II (humanities/social science).
We crafted 10 multi-turn questions per category, yielding a set of 160 questions in total. We display some sample questions below in Figure 1. You can find more <a href="https://huggingface.co/spaces/lmsys/mt-bench">here</a>.</p>
<p><img src="/images/blog/leaderboard_week8/sample_question.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;"></img></p>
<p style="color:gray; text-align: center;">Figure 1: Sample questions from the MT-Bench.</p>
<h3><a id="but-still-how-to-grade-chatbots-answers" class="anchor" href="#but-still-how-to-grade-chatbots-answers" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>But Still, How to Grade Chatbots' Answers?</h3>
<p>Though we believe human preference is the gold standard, it is notoriously slow and expensive to collect.
In our first <a href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna blogpost</a>, we explored an automated evaluation pipeline based on GPT-4.
This approach has since got popular and adopted in several <a href="#related-work">concurrent and follow-up works</a>.</p>
<p>In our latest paper, <a href="https://arxiv.org/abs/2306.05685">&quot;Judging LLM-as-a-judge&quot;</a>, we conducted a systematic study to answer how reliable those LLM judges are.
We provide a brief overview of conclusions here but recommend reading the paper for more details.</p>
<p>We begin by acknowledging potential limitations of LLM-as-a-judge:</p>
<ul>
<li><strong>Position bias</strong> where LLM judges may favor the first answer in a pairwise comparison.</li>
<li><strong>Verbosity bias</strong> where LLM judges may favor lengthier answers, regardless of their quality.</li>
<li><strong>Self-enhancement bias</strong> where LLM judges may favor their own responses.</li>
<li><strong>Limited reasoning ability</strong> referring to LLM judges' possible shortcomings in grading math and reasoning questions.</li>
</ul>
<p>Our study then explores how few-shot judge, chain-of-thought judge, reference-based judge, and fine-tuned judge can help to mitigate these limitations.</p>
<p>Upon implementing some of these solutions, we discovered that despite limitations, strong LLM judges like GPT-4 can align impressively well with both controlled and crowdsourced human preferences, achieving over 80% agreement.
This level of agreement is comparable to the agreement between two different human judges.
Therefore, if used carefully, LLM-as-a-judge can act as a <em>scalable</em> and <em>explainable</em> approximation of human preferences.</p>
<p>We also found that single-answer grading based on GPT-4, without pairwise comparison, can also rank models effectively and match human preferences well.
In Table 1, we present the MT-Bench as a column on the leaderboard based on single-answer grading with GPT-4.</p>
<h2><a id="results-and-analysis" class="anchor" href="#results-and-analysis" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Results and Analysis</h2>
<h3><a id="mt-bench-effectively-distinguishes-among-chatbots" class="anchor" href="#mt-bench-effectively-distinguishes-among-chatbots" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>MT-Bench Effectively Distinguishes Among Chatbots</h3>
<p>Table 1 provides a detailed rundown of the MT-bench-enhanced leaderboard, where we conduct an exhaustive evaluation of 28 popular instruction-tuned models.
We observe a clear distinction among chatbots of varying abilities, with scores showing a high correlation with the Chatbot Arena Elo rating.
In particular, MT-Bench reveals noticeable performance gaps between GPT-4 and GPT-3.5/Claude, and between open and proprietary models.</p>
<p>To delve deeper into the distinguishing factors among chatbots, we select a few representative chatbots and break down their performance per category in Figure 2.
GPT-4 shows superior performance in Coding and Reasoning compared to GPT-3.5/Claude, while Vicuna-13B lags significantly behind in several specific categories: Extraction, Coding, and Math.
This suggests there is still ample room for improvement for open-source models.</p>
<p><img src="/images/blog/leaderboard_week8/ability_breakdown.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;"></img></p>
<p style="color:gray; text-align: center;">Figure 2: The comparison of 6 representative LLMs regarding their abilities in 8 categories: Writing, Roleplay, Reasoning, Math, Coding, Extraction, STEM, Humanities.</p>
<h3><a id="multi-turn-conversation-capabilities" class="anchor" href="#multi-turn-conversation-capabilities" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Multi-turn Conversation Capabilities</h3>
<p>We next analyze the multi-turn scores of selected models, presented in Table 2.</p>
<br>
<p style="color:gray; text-align: center;">Table 2. The breakdown of LLMs' MT-bench scores in the 1st and 2nd turn of a dialogue. Full score is 10.</p>
<div style="display: flex; justify-content: center;">
<table>
<tbody>
<tr> <th>Model</th> <th>Average 1st Turn Score</th> <th>Average 2nd Turn Score</th> <th>Score Difference</th>
<tr><td><a href="https://chat.openai.com/" target="_blank">GPT-4</a></td> <td>8.96</td> <td>9.03</td> <td>0.07</td>  </tr>
<tr><td><a href="https://www.anthropic.com/index/introducing-claude" target="_blank">Claude-v1</a></td> <td>8.15</td> <td>7.65</td> <td>-0.50</td> </tr>
<tr><td><a href="https://chat.openai.com/" target="_blank">GPT-3.5-turbo</a></td> <td>8.08</td> <td>7.81</td> <td>-0.26</td> </tr>
<tr><td><a href="https://github.com/lm-sys/FastChat#vicuna-weights" target="_blank">Vicuna-33B</a></td> <td>7.46</td> <td>6.79</td> <td>-0.67</td> </tr>
<tr><td><a href="https://huggingface.co/WizardLM/WizardLM-30B-V1.0" target="_blank">WizardLM-30B</a></td> <td>7.13</td> <td>6.89</td> <td>-0.24</td> </tr>
<tr><td><a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.0" target="_blank">WizardLM-13B</a></td> <td>7.12</td> <td>5.59</td> <td>-1.53</td> </tr>
<tr><td><a href="https://huggingface.co/timdettmers/guanaco-33b-merged" target="_blank">Guanaco-33B</a></td> <td>6.88</td> <td>6.18</td> <td>-0.71</td> </tr>
<tr><td><a href="https://github.com/lm-sys/FastChat#vicuna-weights" target="_blank">Vicuna-13B</a></td> <td>6.81</td> <td>5.96</td> <td>-0.85</td> </tr>
<tr><td><a href="https://cloud.google.com/vertex-ai/docs/release-notes#May_10_2023" target="_blank">PaLM2-Chat-Bison</a></td> <td>6.71</td> <td>6.09</td> <td>-0.63</td> </tr>
<tr><td><a href="https://github.com/lm-sys/FastChat#vicuna-weights" target="_blank">Vicuna-7B</a></td> <td>6.69</td> <td>5.30</td> <td>-1.39</td> </tr>
<tr><td><a href="https://huggingface.co/young-geng/koala" target="_blank">Koala-13B</a></td> <td>6.08</td> <td>4.63</td> <td>-1.45</td> </tr>
<tr><td><a href="https://huggingface.co/mosaicml/mpt-7b-chat" target="_blank">MPT-7B-Chat</a></td> <td>5.85</td> <td>4.99</td> <td>-0.86</td> </tr>
<tr><td><a href="https://huggingface.co/tiiuae/falcon-40b-instruct" target="_blank">Falcon-40B-instruct</a></td> <td>5.81</td> <td>4.53</td> <td>-1.29</td> </tr>
<tr><td><a href="https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-13b" target="_blank">H2OGPT-Oasst-Open-LLaMA-13B</a></td> <td>5.51</td> <td>3.74</td> <td>-1.78</td> </tr>
</tbody>
</table>
</div>
<p>­</p>
<p>The MT-bench incorporates challenging follow-up questions as part of its design.
For open models, The performance drops significantly from the first to the second turn (e.g., Vicuna-7B, WizardLM-13B), while strong proprietary models maintain consistency.
We also notice a considerable performance gap between LLaMA-based models and those with permissive licenses (MPT-7B, Falcon-40B, and instruction-tuned Open-LLaMA).</p>
<h3><a id="explainability-in-llm-judges" class="anchor" href="#explainability-in-llm-judges" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Explainability in LLM judges</h3>
<p>Another advantage of LLM judges is their ability to provide explainable evaluations.
Figure 3 presents an instance of GPT-4's judgment on an MT-bench question, with answers from alpaca-13b and gpt-3.5-turbo.
GPT-4 provides thorough and logical feedback to support its judgment.
Our <a href="https://arxiv.org/abs/2306.05685">study</a> found that such reviews are beneficial in guiding humans to make better-informed decisions (refer to Section 4.2 for more details).
All the GPT-4 judgments can be found on our <a href="https://huggingface.co/spaces/lmsys/mt-bench">demo site</a>.</p>
<p><img src="/images/blog/leaderboard_week8/explainability_sample.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;"></img></p>
<p style="color:gray; text-align: center;">Figure 3: MT-bench provides more explainability in evaluating LLMs' human preferences.</p>
<p>In conclusion, we have shown that MT-Bench effectively differentiates between chatbots of varying capabilities.
It's scalable, offers valuable insights with category breakdowns, and provides explainability for human judges to verify.
However, LLM judges should be used carefully. It can still make errors, especially when grading math/reasoning questions.</p>
<h2><a id="how-to-evaluate-new-models-on-mt-bench" class="anchor" href="#how-to-evaluate-new-models-on-mt-bench" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How to Evaluate New Models on MT-Bench?</h2>
<p>Evaluating models on MT-bench is simple and fast. Our script supports all huggingface models, and we’ve provided <a href="https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge#mt-bench">detailed instructions</a>,
in which you can generate model’s answers to the MT-bench questions and their GPT-4 judgments. You can also examine the answers and reviews on our gradio browsing demo.</p>
<h2><a id="next-steps" class="anchor" href="#next-steps" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Next steps</h2>
<p><strong>Release of Conversations Data</strong></p>
<p>We're in the process of releasing Chatbot Arena conversations data to the broader research community. Stay tuned for updates!</p>
<p><strong>MT-bench-1K</strong></p>
<p>MT-Bench currently consists of a concise set of 80 carefully curated questions, ensuring the highest quality.
We're actively expanding the question set to MT-Bench-1K by integrating high-quality prompts from the Chatbot Arena and generating new ones automatically using LLMs.
If you have any good ideas, we'd be delighted to hear from you.</p>
<p><strong>Invitation for collaborations</strong></p>
<p>We're engaging with various organizations to explore possibilities for standardizing the evaluation of human preferences for LLMs at scale.
If this interests you, please feel free to reach out to us.</p>
<h2><a id="related-work" class="anchor" href="#related-work" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Related work</h2>
<p>There has been a great amount of interesting work studying how to evaluate human preferences and how to use strong LLM as judges for evaluation.
You are welcome to check them out and see more opinions on this topic:</p>
<ul>
<li><a href="https://arxiv.org/abs/2306.05685">Judging LLM-as-a-judge with MT-Bench and Chatbot Arena</a></li>
<li><a href="https://huggingface.co/blog/llm-leaderboard">Can foundation models label data like humans?</a></li>
<li><a href="https://arxiv.org/abs/2306.04751">How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</a></li>
<li><a href="https://arxiv.org/abs/2305.15717">The False Promise of Imitating Proprietary LLMs</a></li>
<li><a href="https://github.com/tatsu-lab/alpaca_eval">AlpacaEval and AlpacaFarm</a></li>
<li><a href="https://arxiv.org/abs/2305.17926">Large Language Models are not Fair Evaluators</a></li>
</ul>
<h2><a id="links" class="anchor" href="#links" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Links</h2>
<p>Below are readily available tools and code to run MT-bench and other metrics used in this blogpost:</p>
<ul>
<li>The MT-bench uses <a href="https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge">fastchat.llm_judge</a>,</li>
<li>The <a href="https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing">Arena Elo calculator</a>.</li>
<li>The MMLU is based on <a href="https://github.com/declare-lab/instruct-eval/blob/main/mmlu.py">InstructEval</a> and <a href="https://github.com/FranxYao/chain-of-thought-hub/tree/main/MMLU">Chain-of-Thought Hub</a>.</li>
</ul>
<p>If you wish to see more models on leaderboard, we invite you to <a href="https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model">contribute to FastChat</a> or <a href="mailto:lmsysorg@gmail.com">contact us</a> to provide us with API access.</p>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Chatbot Arena Leaderboard Week 8: Introducing MT-Bench and Vicuna-33B","author":"Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Hao Zhang","date":"June 22, 2023","previewImg":"/images/blog/leaderboard_week8/ability_breakdown.png"},"content":"\nIn this blog post, we share the latest update on Chatbot Arena leaderboard, which now includes more open models and three metrics:\n\n1. **Chatbot Arena Elo**, based on 42K anonymous votes from [Chatbot Arena](https://lmsys.org/blog/2023-05-03-arena/) using the Elo rating system.\n2. **MT-Bench score**, based on a challenging multi-turn benchmark and GPT-4 grading, proposed and validated in our [Judging LLM-as-a-judge paper](https://arxiv.org/abs/2306.05685).\n3. **MMLU**, a widely adopted [benchmark](https://arxiv.org/abs/2009.03300).\n\nFurthermore, we’re excited to introduce our **new series of Vicuna-v1.3 models**, ranging from 7B to 33B parameters, trained on an extended set of user-shared conversations.\nTheir weights are now [available](https://github.com/lm-sys/FastChat/tree/main#vicuna-weights).\n\n## Updated Leaderboard and New Models\n\n\u003cstyle\u003e\nth {text-align: left}\ntd {text-align: left}\n\ntable {\n  border-collapse: collapse;\n  width: 100%;\n}\n\n\nth {\n  cursor: pointer;\n}\n\nth:hover {\n  background-color: #ddd;\n}\n\n.arrow {\n  display: inline-block;\n  width: 0;\n  height: 0;\n  vertical-align: middle;\n  margin-left: 5px;\n  border-left: 5px solid transparent;\n  border-right: 5px solid transparent;\n}\n\n.arrow-up {\n  border-bottom: 5px solid #000;\n}\n\n.arrow-down {\n  border-top: 5px solid #000;\n}\n\n/* Initially sort arrow for descending order */\nth:nth-child(1) .arrow-down {\n  border-top: 5px solid #000;\n}\n\u003c/style\u003e\n\n\n\u003cscript\u003e\n    let sortOrder = ['desc', undefined, undefined];\n\n    function sortTable(columnIndex, table_id) {\n      let table, rows, switching, i, x, y, shouldSwitch;\n      table = document.getElementById(table_id);\n      switching = true;\n      let sortAsc = sortOrder[columnIndex] === 'asc';\n\n      while (switching) {\n        switching = false;\n        rows = table.getElementsByTagName(\"tr\");\n\n        for (i = 1; i \u003c (rows.length - 1); i++) {\n          shouldSwitch = false;\n          x = rows[i].getElementsByTagName(\"td\")[columnIndex];\n          y = rows[i + 1].getElementsByTagName(\"td\")[columnIndex];\n          x_char = x.innerHTML.toLowerCase();\n          y_char = y.innerHTML.toLowerCase();\n          if (sortAsc) {\n            if (x_char === \"-\") {\n                x_val = 9999;\n            } else {\n                x_val = Number(x_char);\n            }\n            if (y_char === \"-\") {\n                y_val = 9999;\n            } else {\n                y_val = Number(y_char);\n            }\n            if (x_val \u003e y_val) {\n              shouldSwitch = true;\n              break;\n            }\n          } else {\n            if (x_char === \"-\") {\n                x_val = 0.0;\n            } else {\n                x_val = Number(x_char);\n            }\n            if (y_char === \"-\") {\n                y_val = 0.0;\n            } else {\n                y_val = Number(y_char);\n            }\n\n            if (x_val \u003c y_val) {\n              shouldSwitch = true;\n              break;\n            }\n          }\n        }\n\n        if (shouldSwitch) {\n          rows[i].parentNode.insertBefore(rows[i + 1], rows[i]);\n          switching = true;\n        }\n      }\n\n      let arrowElements = document.getElementsByClassName(\"arrow\");\n      for (let j = 0; j \u003c arrowElements.length; j++) {\n        arrowElements[j].classList.remove(\"arrow-up\", \"arrow-down\");\n      }\n\n      let arrowElement = document.getElementsByTagName(\"th\")[columnIndex].getElementsByClassName(\"arrow\")[0];\n      arrowElement.classList.add(sortAsc ? \"arrow-up\" : \"arrow-down\");\n      sortOrder[columnIndex] = sortAsc ? 'desc' : 'asc';\n    }\n\u003c/script\u003e\n\n\n\n\u003cbr\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 1. LLM Leaderboard (Timeframe: April 24 - June 19, 2023). The latest and detailed version \u003ca href=\"https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: center;\"\u003e\n\u003ctable id=\"Table1\" \u003e\n\u003ctbody\u003e\n\n\u003ctr\u003e \u003cth\u003eModel\u003c/th\u003e \u003cth onclick=\"sortTable(1, 'Table1')\"\u003eMT-bench (score) \u003cspan class=\"arrow arrow-down\"\u003e\u003c/span\u003e\u003c/th\u003e \u003cth onclick=\"sortTable(2, 'Table1')\"\u003eArena Elo Rating \u003cspan class=\"arrow\"\u003e\u003c/span\u003e\u003c/th\u003e \u003cth onclick=\"sortTable(3, 'Table1')\"\u003eMMLU \u003cspan class=\"arrow\"\u003e\u003c/span\u003e\u003c/th\u003e \u003cth\u003eLicense\u003c/th\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://openai.com/research/gpt-4\"\u003e GPT-4 \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e8.99\u003c/td\u003e  \u003ctd\u003e1227\u003c/td\u003e  \u003ctd\u003e86.4\u003c/td\u003e  \u003ctd\u003eProprietary\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://openai.com/blog/chatgpt\"\u003e GPT-3.5-turbo \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e7.94\u003c/td\u003e  \u003ctd\u003e1130\u003c/td\u003e  \u003ctd\u003e70.0\u003c/td\u003e  \u003ctd\u003eProprietary\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://www.anthropic.com/index/introducing-claude\"\u003e Claude-v1 \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e7.90\u003c/td\u003e  \u003ctd\u003e1178\u003c/td\u003e  \u003ctd\u003e75.6\u003c/td\u003e  \u003ctd\u003eProprietary\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://www.anthropic.com/index/introducing-claude\"\u003e Claude-instant-v1 \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e7.85\u003c/td\u003e  \u003ctd\u003e1156\u003c/td\u003e  \u003ctd\u003e61.3\u003c/td\u003e  \u003ctd\u003eProprietary\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/lmsys/vicuna-33b-v1.3\"\u003e Vicuna-33B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e7.12\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003e59.2\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/WizardLM/WizardLM-30B-V1.0\"\u003e WizardLM-30B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e7.01\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003e58.7\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/timdettmers/guanaco-33b-merged\"\u003e Guanaco-33B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e6.53\u003c/td\u003e  \u003ctd\u003e1065\u003c/td\u003e  \u003ctd\u003e57.6\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/allenai/tulu-30b\"\u003e Tulu-30B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e6.43\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003e58.1\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/timdettmers/guanaco-65b-merged\"\u003e Guanaco-65B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e6.41\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003e62.1\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor\"\u003e OpenAssistant-LLaMA-30B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e6.41\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003e56.0\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#foundation_models\"\u003e PaLM-Chat-Bison-001 \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e6.40\u003c/td\u003e  \u003ctd\u003e1038\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003eProprietary\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/lmsys/vicuna-13b-v1.3\"\u003e Vicuna-13B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e6.39\u003c/td\u003e  \u003ctd\u003e1061\u003c/td\u003e  \u003ctd\u003e52.1\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-30b-chat\"\u003e MPT-30B-chat \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e6.39\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003e50.4\u003c/td\u003e  \u003ctd\u003eCC-BY-NC-SA-4.0\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/WizardLM/WizardLM-13B-V1.0\"\u003e WizardLM-13B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e6.35\u003c/td\u003e  \u003ctd\u003e1048\u003c/td\u003e  \u003ctd\u003e52.3\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/lmsys/vicuna-7b-v1.3\"\u003e Vicuna-7B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e6.00\u003c/td\u003e  \u003ctd\u003e1008\u003c/td\u003e  \u003ctd\u003e47.1\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/project-baize/baize-v2-13b\"\u003e Baize-v2-13B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e5.75\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003e48.9\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/NousResearch/Nous-Hermes-13b\"\u003e Nous-Hermes-13B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e5.51\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003e49.3\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-7b-chat\"\u003e MPT-7B-Chat \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e5.42\u003c/td\u003e  \u003ctd\u003e956\u003c/td\u003e  \u003ctd\u003e32.0\u003c/td\u003e  \u003ctd\u003eCC-BY-NC-SA-4.0\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/nomic-ai/gpt4all-13b-snoozy\"\u003e GPT4All-13B-Snoozy \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e5.41\u003c/td\u003e  \u003ctd\u003e986\u003c/td\u003e  \u003ctd\u003e43.0\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://bair.berkeley.edu/blog/2023/04/03/koala/\"\u003e Koala-13B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e5.35\u003c/td\u003e  \u003ctd\u003e992\u003c/td\u003e  \u003ctd\u003e44.7\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-30b-instruct\"\u003e MPT-30B-Instruct \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e5.22\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003e47.8\u003c/td\u003e  \u003ctd\u003eCC-BY-SA 3.0\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-40b-instruct\"\u003e Falcon-40B-Instruct \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e5.17\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003e54.7\u003c/td\u003e  \u003ctd\u003eApache 2.0\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-13b\"\u003e H2O-Oasst-OpenLLaMA-13B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e4.63\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003e42.8\u003c/td\u003e  \u003ctd\u003eApache 2.0\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\"\u003e Alpaca-13B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e4.53\u003c/td\u003e  \u003ctd\u003e930\u003c/td\u003e  \u003ctd\u003e48.1\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/THUDM/chatglm-6b\"\u003e ChatGLM-6B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e4.50\u003c/td\u003e  \u003ctd\u003e905\u003c/td\u003e  \u003ctd\u003e36.1\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\u003e OpenAssistant-Pythia-12B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e4.32\u003c/td\u003e  \u003ctd\u003e924\u003c/td\u003e  \u003ctd\u003e27.0\u003c/td\u003e  \u003ctd\u003eApache 2.0\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/BlinkDL/rwkv-4-raven\"\u003e RWKV-4-Raven-14B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e3.98\u003c/td\u003e  \u003ctd\u003e950\u003c/td\u003e  \u003ctd\u003e25.6\u003c/td\u003e  \u003ctd\u003eApache 2.0\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/databricks/dolly-v2-12b\"\u003e Dolly-V2-12B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e3.28\u003c/td\u003e  \u003ctd\u003e850\u003c/td\u003e  \u003ctd\u003e25.7\u003c/td\u003e  \u003ctd\u003eMIT\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/lmsys/fastchat-t5-3b-v1.0\"\u003e FastChat-T5-3B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e3.04\u003c/td\u003e  \u003ctd\u003e897\u003c/td\u003e  \u003ctd\u003e47.7\u003c/td\u003e  \u003ctd\u003eApache 2.0\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b\"\u003e StableLM-Tuned-Alpha-7B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e2.75\u003c/td\u003e  \u003ctd\u003e871\u003c/td\u003e  \u003ctd\u003e24.4\u003c/td\u003e  \u003ctd\u003eCC-BY-NC-SA-4.0\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://arxiv.org/abs/2302.13971\"\u003e LLaMA-13B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e2.61\u003c/td\u003e  \u003ctd\u003e826\u003c/td\u003e  \u003ctd\u003e47.0\u003c/td\u003e  \u003ctd\u003eNon-commercial\u003c/td\u003e \u003c/tr\u003e\n\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\n\u0026shy;\n\nWelcome to try the Chatbot Arena voting [demo](https://chat.lmsys.org/?arena).\nKeep in mind that each benchmark has its limitations. Please consider the results as guiding references. See our discussion below for more technical details.\n\n## Evaluating Chatbots with MT-bench and Arena\n\n### Motivation\n\nWhile several benchmarks exist for evaluating Large Language Model's (LLM) performance, such as [MMLU](https://arxiv.org/abs/2009.03300), [HellaSwag](https://arxiv.org/abs/1905.07830), and [HumanEval](https://github.com/openai/human-eval), \nwe noticed that these benchmarks might fall short when assessing LLMs' human preferences. \nTraditional benchmarks often test LLMs on close-ended questions with concise outputs (e.g., multiple choices), which do not reflect the typical use cases of LLM-based chat assistants.\n\nTo fill this gap, in this leaderboard update, in addition to the Chatbot Arena Elo system, we add a new benchmark: MT-Bench.\n- [MT-bench](https://arxiv.org/abs/2306.05685) is a challenging multi-turn question set designed to evaluate the conversational and instruction-following ability of models. You can view sample questions and answers of MT-bench [here](https://huggingface.co/spaces/lmsys/mt-bench).\n- [Chatbot Arena](https://chat.lmsys.org/?arena) is a crowd-sourced battle platform, where users ask chatbots any question and vote for their preferred answer.\n\nBoth benchmarks are designed to use human preferences as the primary metric.\n\n### Why MT-Bench?\n\nMT-Bench is a carefully curated benchmark that includes 80 high-quality, multi-turn questions. \nThese questions are tailored to assess the conversation flow and instruction-following capabilities of models in multi-turn dialogues. \nThey include both common use cases and challenging instructions meant to distinguish between chatbots. \nMT-Bench serves as a **quality-controlled complement** to our crowd-sourced based evaluation -- Chatbot Arena.\n\nThrough running the Chatbot Arena for 2 months and analyzing our users' prompts, we've identified 8 primary categories of user prompts: Writing, Roleplay, Extraction, Reasoning, Math, Coding, Knowledge I (STEM), and Knowledge II (humanities/social science). \nWe crafted 10 multi-turn questions per category, yielding a set of 160 questions in total. We display some sample questions below in Figure 1. You can find more [here](https://huggingface.co/spaces/lmsys/mt-bench).\n\n\u003cimg src=\"/images/blog/leaderboard_week8/sample_question.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 1: Sample questions from the MT-Bench.\u003c/p\u003e\n\n### But Still, How to Grade Chatbots' Answers?\nThough we believe human preference is the gold standard, it is notoriously slow and expensive to collect. \nIn our first [Vicuna blogpost](https://lmsys.org/blog/2023-03-30-vicuna/), we explored an automated evaluation pipeline based on GPT-4. \nThis approach has since got popular and adopted in several [concurrent and follow-up works](#related-work).\n\nIn our latest paper, [\"Judging LLM-as-a-judge\"](https://arxiv.org/abs/2306.05685), we conducted a systematic study to answer how reliable those LLM judges are. \nWe provide a brief overview of conclusions here but recommend reading the paper for more details.\n\nWe begin by acknowledging potential limitations of LLM-as-a-judge:\n\n- **Position bias** where LLM judges may favor the first answer in a pairwise comparison.\n- **Verbosity bias** where LLM judges may favor lengthier answers, regardless of their quality.\n- **Self-enhancement bias** where LLM judges may favor their own responses.\n- **Limited reasoning ability** referring to LLM judges' possible shortcomings in grading math and reasoning questions.\n\nOur study then explores how few-shot judge, chain-of-thought judge, reference-based judge, and fine-tuned judge can help to mitigate these limitations.\n\nUpon implementing some of these solutions, we discovered that despite limitations, strong LLM judges like GPT-4 can align impressively well with both controlled and crowdsourced human preferences, achieving over 80% agreement. \nThis level of agreement is comparable to the agreement between two different human judges. \nTherefore, if used carefully, LLM-as-a-judge can act as a *scalable* and *explainable* approximation of human preferences.\n\nWe also found that single-answer grading based on GPT-4, without pairwise comparison, can also rank models effectively and match human preferences well. \nIn Table 1, we present the MT-Bench as a column on the leaderboard based on single-answer grading with GPT-4.\n\n## Results and Analysis\n\n### MT-Bench Effectively Distinguishes Among Chatbots\n\nTable 1 provides a detailed rundown of the MT-bench-enhanced leaderboard, where we conduct an exhaustive evaluation of 28 popular instruction-tuned models. \nWe observe a clear distinction among chatbots of varying abilities, with scores showing a high correlation with the Chatbot Arena Elo rating. \nIn particular, MT-Bench reveals noticeable performance gaps between GPT-4 and GPT-3.5/Claude, and between open and proprietary models.\n\nTo delve deeper into the distinguishing factors among chatbots, we select a few representative chatbots and break down their performance per category in Figure 2. \nGPT-4 shows superior performance in Coding and Reasoning compared to GPT-3.5/Claude, while Vicuna-13B lags significantly behind in several specific categories: Extraction, Coding, and Math. \nThis suggests there is still ample room for improvement for open-source models.\n\n\u003cimg src=\"/images/blog/leaderboard_week8/ability_breakdown.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 2: The comparison of 6 representative LLMs regarding their abilities in 8 categories: Writing, Roleplay, Reasoning, Math, Coding, Extraction, STEM, Humanities.\u003c/p\u003e\n\n\n### Multi-turn Conversation Capabilities\n\nWe next analyze the multi-turn scores of selected models, presented in Table 2. \n\n\u003cbr\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 2. The breakdown of LLMs' MT-bench scores in the 1st and 2nd turn of a dialogue. Full score is 10.\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: center;\"\u003e\n\u003ctable\u003e\n\u003ctbody\u003e\n\u003ctr\u003e \u003cth\u003eModel\u003c/th\u003e \u003cth\u003eAverage 1st Turn Score\u003c/th\u003e \u003cth\u003eAverage 2nd Turn Score\u003c/th\u003e \u003cth\u003eScore Difference\u003c/th\u003e\n\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://chat.openai.com/\" target=\"_blank\"\u003eGPT-4\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e8.96\u003c/td\u003e \u003ctd\u003e9.03\u003c/td\u003e \u003ctd\u003e0.07\u003c/td\u003e  \u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://www.anthropic.com/index/introducing-claude\" target=\"_blank\"\u003eClaude-v1\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e8.15\u003c/td\u003e \u003ctd\u003e7.65\u003c/td\u003e \u003ctd\u003e-0.50\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://chat.openai.com/\" target=\"_blank\"\u003eGPT-3.5-turbo\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e8.08\u003c/td\u003e \u003ctd\u003e7.81\u003c/td\u003e \u003ctd\u003e-0.26\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://github.com/lm-sys/FastChat#vicuna-weights\" target=\"_blank\"\u003eVicuna-33B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e7.46\u003c/td\u003e \u003ctd\u003e6.79\u003c/td\u003e \u003ctd\u003e-0.67\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://huggingface.co/WizardLM/WizardLM-30B-V1.0\" target=\"_blank\"\u003eWizardLM-30B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e7.13\u003c/td\u003e \u003ctd\u003e6.89\u003c/td\u003e \u003ctd\u003e-0.24\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://huggingface.co/WizardLM/WizardLM-13B-V1.0\" target=\"_blank\"\u003eWizardLM-13B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e7.12\u003c/td\u003e \u003ctd\u003e5.59\u003c/td\u003e \u003ctd\u003e-1.53\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://huggingface.co/timdettmers/guanaco-33b-merged\" target=\"_blank\"\u003eGuanaco-33B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e6.88\u003c/td\u003e \u003ctd\u003e6.18\u003c/td\u003e \u003ctd\u003e-0.71\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://github.com/lm-sys/FastChat#vicuna-weights\" target=\"_blank\"\u003eVicuna-13B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e6.81\u003c/td\u003e \u003ctd\u003e5.96\u003c/td\u003e \u003ctd\u003e-0.85\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://cloud.google.com/vertex-ai/docs/release-notes#May_10_2023\" target=\"_blank\"\u003ePaLM2-Chat-Bison\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e6.71\u003c/td\u003e \u003ctd\u003e6.09\u003c/td\u003e \u003ctd\u003e-0.63\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://github.com/lm-sys/FastChat#vicuna-weights\" target=\"_blank\"\u003eVicuna-7B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e6.69\u003c/td\u003e \u003ctd\u003e5.30\u003c/td\u003e \u003ctd\u003e-1.39\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://huggingface.co/young-geng/koala\" target=\"_blank\"\u003eKoala-13B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e6.08\u003c/td\u003e \u003ctd\u003e4.63\u003c/td\u003e \u003ctd\u003e-1.45\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://huggingface.co/mosaicml/mpt-7b-chat\" target=\"_blank\"\u003eMPT-7B-Chat\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e5.85\u003c/td\u003e \u003ctd\u003e4.99\u003c/td\u003e \u003ctd\u003e-0.86\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://huggingface.co/tiiuae/falcon-40b-instruct\" target=\"_blank\"\u003eFalcon-40B-instruct\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e5.81\u003c/td\u003e \u003ctd\u003e4.53\u003c/td\u003e \u003ctd\u003e-1.29\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e\u003ctd\u003e\u003ca href=\"https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-13b\" target=\"_blank\"\u003eH2OGPT-Oasst-Open-LLaMA-13B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e5.51\u003c/td\u003e \u003ctd\u003e3.74\u003c/td\u003e \u003ctd\u003e-1.78\u003c/td\u003e \u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\n\u0026shy;\n\nThe MT-bench incorporates challenging follow-up questions as part of its design. \nFor open models, The performance drops significantly from the first to the second turn (e.g., Vicuna-7B, WizardLM-13B), while strong proprietary models maintain consistency. \nWe also notice a considerable performance gap between LLaMA-based models and those with permissive licenses (MPT-7B, Falcon-40B, and instruction-tuned Open-LLaMA).\n\n\n### Explainability in LLM judges \n\nAnother advantage of LLM judges is their ability to provide explainable evaluations. \nFigure 3 presents an instance of GPT-4's judgment on an MT-bench question, with answers from alpaca-13b and gpt-3.5-turbo. \nGPT-4 provides thorough and logical feedback to support its judgment. \nOur [study](https://arxiv.org/abs/2306.05685) found that such reviews are beneficial in guiding humans to make better-informed decisions (refer to Section 4.2 for more details). \nAll the GPT-4 judgments can be found on our [demo site](https://huggingface.co/spaces/lmsys/mt-bench).\n\n\u003cimg src=\"/images/blog/leaderboard_week8/explainability_sample.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 3: MT-bench provides more explainability in evaluating LLMs' human preferences.\u003c/p\u003e\n\nIn conclusion, we have shown that MT-Bench effectively differentiates between chatbots of varying capabilities. \nIt's scalable, offers valuable insights with category breakdowns, and provides explainability for human judges to verify. \nHowever, LLM judges should be used carefully. It can still make errors, especially when grading math/reasoning questions.\n\n\n## How to Evaluate New Models on MT-Bench?\n\nEvaluating models on MT-bench is simple and fast. Our script supports all huggingface models, and we’ve provided [detailed instructions](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge#mt-bench), \nin which you can generate model’s answers to the MT-bench questions and their GPT-4 judgments. You can also examine the answers and reviews on our gradio browsing demo.\n\n## Next steps\n**Release of Conversations Data**\n\nWe're in the process of releasing Chatbot Arena conversations data to the broader research community. Stay tuned for updates!\n\n**MT-bench-1K**\n\nMT-Bench currently consists of a concise set of 80 carefully curated questions, ensuring the highest quality. \nWe're actively expanding the question set to MT-Bench-1K by integrating high-quality prompts from the Chatbot Arena and generating new ones automatically using LLMs. \nIf you have any good ideas, we'd be delighted to hear from you.\n\n**Invitation for collaborations**\n\nWe're engaging with various organizations to explore possibilities for standardizing the evaluation of human preferences for LLMs at scale. \nIf this interests you, please feel free to reach out to us.\n\n## Related work\nThere has been a great amount of interesting work studying how to evaluate human preferences and how to use strong LLM as judges for evaluation. \nYou are welcome to check them out and see more opinions on this topic:\n- [Judging LLM-as-a-judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)\n- [Can foundation models label data like humans?](https://huggingface.co/blog/llm-leaderboard)\n- [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources](https://arxiv.org/abs/2306.04751)\n- [The False Promise of Imitating Proprietary LLMs](https://arxiv.org/abs/2305.15717)\n- [AlpacaEval and AlpacaFarm](https://github.com/tatsu-lab/alpaca_eval)\n- [Large Language Models are not Fair Evaluators](https://arxiv.org/abs/2305.17926) \n\n## Links\nBelow are readily available tools and code to run MT-bench and other metrics used in this blogpost:\n- The MT-bench uses [fastchat.llm_judge](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge),\n- The [Arena Elo calculator](https://colab.research.google.com/drive/1RAWb22-PFNI-X1gPVzc927SGUdfr6nsR?usp=sharing).\n- The MMLU is based on [InstructEval](https://github.com/declare-lab/instruct-eval/blob/main/mmlu.py) and [Chain-of-Thought Hub](https://github.com/FranxYao/chain-of-thought-hub/tree/main/MMLU).\n\nIf you wish to see more models on leaderboard, we invite you to [contribute to FastChat](https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model) or [contact us](mailto:lmsysorg@gmail.com) to provide us with API access.\n","slug":"2023-06-22-leaderboard"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2023-06-22-leaderboard"},"buildId":"RgGbYtj92MeqCJ2OZxXwk","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>