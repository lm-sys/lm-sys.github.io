<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>SGLang for gpt-oss: From Day 0 Support to Enhanced Performance | LMSYS Org</title><meta name="title" content="SGLang for gpt-oss: From Day 0 Support to Enhanced Performance | LMSYS Org"/><meta property="og:title" content="SGLang for gpt-oss: From Day 0 Support to Enhanced Performance | LMSYS Org"/><meta name="twitter:title" content="SGLang for gpt-oss: From Day 0 Support to Enhanced Performance | LMSYS Org"/><meta name="description" content="&lt;p&gt;We are excited to announce a major update for SGLang, focusing on deep performance optimizations and new features for the recently released openai/gpt-oss..."/><meta property="og:description" content="&lt;p&gt;We are excited to announce a major update for SGLang, focusing on deep performance optimizations and new features for the recently released openai/gpt-oss..."/><meta name="twitter:description" content="&lt;p&gt;We are excited to announce a major update for SGLang, focusing on deep performance optimizations and new features for the recently released openai/gpt-oss..."/><meta property="og:image" content="https://lmsys.org/images/blog/gpt_oss/gpt_oss_preview.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/gpt_oss/gpt_oss_preview.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2025-08-27-gpt-oss"/><meta name="twitter:url" content="https://lmsys.org/blog/2025-08-27-gpt-oss"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-0bb93d4b49319e30.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/duMpKvhw40cCUQOchi6CB/_buildManifest.js" defer=""></script><script src="/_next/static/duMpKvhw40cCUQOchi6CB/_ssgManifest.js" defer=""></script><script src="/_next/static/duMpKvhw40cCUQOchi6CB/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">SGLang for gpt-oss: From Day 0 Support to Enhanced Performance</h1><p class="text-xl pt-2 pb-2">by: <!-- -->Liangsheng Yin, Ke Bao<!-- -->,<!-- --> <!-- -->Aug 27, 2025<!-- --></p><hr/><div class="pt-2 article"><p>We are excited to announce a major update for SGLang, focusing on deep performance optimizations and new features for the recently released openai/gpt-oss-120b model. <strong>While we had support from day zero, we took the last few weeks to enhance our engine to ensure you get the best possible performance.</strong></p>
<p>This post highlights our latest achievements: a significant performance improvement for gpt-oss with up to <strong>2.1x</strong> higher throughput on prefill and <strong>2.25x</strong> higher throughput on decode, out-of-the-box support for NVIDIA Blackwell &amp; Hopper and AMD MI350 GPUs, speculative decoding support, and enhanced APIs to power complex agentic applications—all while maintaining the model's high accuracy.</p>
<p>All changes are now available in our main branch.</p>
<h3><a id="get-started-with-sglang" class="anchor" href="#get-started-with-sglang" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Get Started with SGLang</h3>
<pre><code class="hljs language-bash">pip install <span class="hljs-string">&quot;sglang[all]&gt;=0.5.1.post3&quot;</span>
python3 -m sglang.launch_server --model-path openai/gpt-oss-120b --tp 4
</code></pre>
<p>For detailed instructions on environment setup and how to gain the best performance, please see our <a href="https://github.com/sgl-project/awesome-sglang/tree/main/gpt-oss">guide in awesome-sglang</a>.</p>
<h2><a id="by-the-numbers-comprehensive-benchmark-results-" class="anchor" href="#by-the-numbers-comprehensive-benchmark-results-" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>By the Numbers: Comprehensive Benchmark Results 📊</h2>
<p>To show the impact of our optimizations, we benchmarked SGLang across a range of hardware configurations. For all the results, the reproduction command can be found <a href="https://github.com/sgl-project/sglang/tree/main/benchmark/gpt_oss">here</a>.</p>
<h5><a id="low-latency-performance-batch-size--1" class="anchor" href="#low-latency-performance-batch-size--1" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Low-Latency Performance (Batch Size = 1)</h5>
<p>For latency-sensitive applications, we measured single-batch decode throughput across B200 and H100 GPUs, showcasing excellent performance.</p>
<table>
<thead>
<tr>
<th>Hardware / Precision</th>
<th>NVIDIA B200</th>
<th>NVIDIA H100</th>
</tr>
</thead>
<tbody>
<tr>
<td>MXFP4</td>
<td>416.02 tok/s</td>
<td>318.53 tok/s</td>
</tr>
<tr>
<td>BF16</td>
<td>315.63 tok/s</td>
<td>293.12 tok/s</td>
</tr>
</tbody>
</table>
<span style="color: grey; font-size: 12px;">
B200 was tested with TP=4, H100 was tested with TP=8 and triton attention.
</span>
<h5><a id="high-throughput-performance-batch-size--32" class="anchor" href="#high-throughput-performance-batch-size--32" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>High-Throughput Performance (Batch Size = 32)</h5>
<p>For high-throughput applications, SGLang delivers significant performance gains over our initial Day 0 support and have shown great performance on both prefill and decode on different hardwares.</p>
<!-- grey text -->
<span style="color: grey; font-size: 12px;">
The results of AMD MI350 were tested with triton backend which is not fully optimized yet, and more optimizations with AMD AITER will be released soon.
</span>
<p><img src="/images/blog/gpt_oss/combined_prefill_performance.svg" alt="combined_prefill_performance.svg" style="display:block; margin-left: auto; margin-right: auto; width: 75%"></img></p>
<p><img src="/images/blog/gpt_oss/combined_decode_performance.svg" alt="combined_decode_performance.svg" style="display:block; margin-left: auto; margin-right: auto; width: 75%"></img></p>
<h2><a id="performance-deep-dive-" class="anchor" href="#performance-deep-dive-" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Performance Deep Dive 🚀</h2>
<p>Our performance gains come from several key optimizations at the kernel level:</p>
<ul>
<li><strong>FlashInfer Kernels for Blackwell</strong>: To unlock peak performance for gpt-oss on Blackwell GPUs, we integrated highly optimized kernels from FlashInfer. This accelerates core components, including multi-head attention and Mixture of Experts (MoE) layers, on the new hardware.</li>
<li><strong>FlashAttention-3 for Hopper</strong>: We modified the FlashAttention-3 kernels to support attention sinks, providing a significant speedup for inference on Hopper GPUs.</li>
<li><strong>Kernel Fusion and Reduction</strong>: We performed several low-level fusions to reduce overhead. This includes fusing the RMS norm with all-reduce, merging the set KV buffer operation into RoPE, and fusing hidden states padding into quantization. We also removed unnecessary kernels, enabled <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programmatic-dependent-launch-and-synchronization">PDL</a> for some kernels, and reduced CPU overhead for greater efficiency.</li>
</ul>
<h2><a id="accuracy-alignment-with-official-report-" class="anchor" href="#accuracy-alignment-with-official-report-" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Accuracy Alignment with Official Report 🎯</h2>
<p>We validated our optimized gpt-oss implementation against the GPQA benchmark and confirmed that our results align closely with the official model card, ensuring that these speedups do not compromise the model's reasoning capabilities.</p>
<table>
<thead>
<tr>
<th>Reasoning Effort</th>
<th>SGLang</th>
<th><a href="https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html#accuracy-evaluation-panels">vLLM</a></th>
<th><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf">Official</a></th>
</tr>
</thead>
<tbody>
<tr>
<td>Low</td>
<td>65.6</td>
<td>65.3</td>
<td>67.1</td>
</tr>
<tr>
<td>Medium</td>
<td>72.1</td>
<td>72.4</td>
<td>73.1</td>
</tr>
<tr>
<td>High</td>
<td>79.8</td>
<td>79.4</td>
<td>80.1</td>
</tr>
</tbody>
</table>
<h2><a id="speculative-decoding-support-" class="anchor" href="#speculative-decoding-support-" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Speculative Decoding Support 🦅</h2>
<p><strong>Speculative Decoding</strong> is a key technique for improving LLM inference performance. <a href="https://arxiv.org/abs/2503.01840"><strong>EAGLE3</strong></a> is the current state-of-the-art speculative decoding method, and SGLang was the first framework to support it, thanks to close collaboration with EAGLE team.</p>
<p>In SGLang, you can easily launch gpt-oss model with EAGLE3 speculative decoding:</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># On Hopper:</span>
<span class="hljs-comment"># - Tree decoding (topk &gt; 1) and chain decoding (topk = 1) are supported on both FA3 and Triton backends.</span>
python3 -m sglang.launch_server --model openai/gpt-oss-120b --speculative-algorithm EAGLE3 --speculative-draft-model-path lmsys/EAGLE3-gpt-oss-120b-bf16 --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 --tp 4
python3 -m sglang.launch_server --model openai/gpt-oss-120b --speculative-algorithm EAGLE3 --speculative-draft-model-path lmsys/EAGLE3-gpt-oss-120b-bf16 --speculative-num-steps 5 --speculative-eagle-topk 4 --speculative-num-draft-tokens 8 --tp 4

<span class="hljs-comment"># On Blackwell:</span>
<span class="hljs-comment"># - Chain decoding (topk = 1) is supported on TRTLLM-MHA backend. Tree decoding (topk &gt; 1) is in progress, stay tuned!</span>
<span class="hljs-comment"># - Both tree decoding (topk &gt; 1) and chain decoding (topk = 1) are supported on the Triton backend.</span>
python3 -m sglang.launch_server --model openai/gpt-oss-120b --speculative-algorithm EAGLE3 --speculative-draft lmsys/EAGLE3-gpt-oss-120b-bf16 --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 --tp 4
python3 -m sglang.launch_server --model openai/gpt-oss-120b --speculative-algorithm EAGLE3 --speculative-draft lmsys/EAGLE3-gpt-oss-120b-bf16 --speculative-num-steps 5 --speculative-eagle-topk 4 --speculative-num-draft-tokens 8 --attention-backend triton --tp 4
</code></pre>
<p>For <code>openai/gpt-oss-120b</code> model, we trained an EAGLE3 draft model, <a href="https://huggingface.co/lmsys/EAGLE3-gpt-oss-120b-bf16"><code>lmsys/EAGLE3-gpt-oss-120b-bf16</code></a> with <a href="https://github.com/sgl-project/SpecForge">SpecForge</a>, an efficient framework for speculative draft model training. Our trained draft model achieves a higher average acceptance length compared to <a href="https://huggingface.co/nvidia/gpt-oss-120b-Eagle3">NVIDIA’s gpt-oss draft model</a>.</p>
<p><img src="/images/blog/gpt_oss/accept_length.svg" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%"></img></p>
<p>We also benchmarked <code>openai/gpt-oss-120b</code> with EAGLE3 on H200 TP4 and observed promising results across several standard benchmark datasets:</p>
<p><img src="/images/blog/gpt_oss/gpt_oss_eagle3_results.svg" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%"></img></p>
<p>which achieves:</p>
<ul>
<li><strong>1.39x</strong> speedup with the <code>steps=3, topk=1, num_draft_tokens=4</code> setting.</li>
<li><strong>1.52x</strong> speedup with the <code>steps=5, topk=4, num_draft_tokens=8</code> setting.</li>
</ul>
<h2><a id="powering-agentic-applications-" class="anchor" href="#powering-agentic-applications-" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Powering Agentic Applications 🤖</h2>
<p>To better enable agentic workflows, SGLang offers <a href="https://docs.sglang.ai/basic_usage/gpt_oss.html#responses-api">OpenAI Response API support</a> and <a href="https://docs.sglang.ai/advanced_features/tool_parser.html">native chat completion support</a>. Here is an example of how to build a simple web search agent with SGLang (<code>python3.12</code> and <code>gpt-oss</code> package are required for built-in tools, more setup details can be found <a href="https://docs.sglang.ai/basic_usage/gpt_oss.html#responses-api">here</a>).</p>
<p>Launch the server:</p>
<pre><code class="hljs language-bash"><span class="hljs-built_in">export</span> EXA_API_KEY=YOUR_EXA_KEY
python3 -m sglang.launch_server --port 30000 --model-path openai/gpt-oss-120b --tp 4 --tool-server demo 
</code></pre>
<p>Use Response API to build a web search agent:</p>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> openai

client = openai.OpenAI(
    base_url=<span class="hljs-string">&quot;http://localhost:30000/v1&quot;</span>,
    api_key=<span class="hljs-string">&quot;EMPTY&quot;</span>
)
response = client.responses.create(
    model=<span class="hljs-string">&quot;openai/gpt-oss-120b&quot;</span>,
    tools=[{<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;web_search_preview&quot;</span>}],
    <span class="hljs-built_in">input</span>=<span class="hljs-string">&quot;What does SGLang update today?&quot;</span>
)

<span class="hljs-built_in">print</span>(response.output_text)
</code></pre>
<h2><a id="whats-next-" class="anchor" href="#whats-next-" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What's Next? 🔮</h2>
<p>None of the Day-0 support or the subsequent optimizations would have been possible without the collective effort of the SGLang community. Shout-out to the SGLang team, SpecForge team, FlashInfer team, Oracle team, Eigen AI team, NVIDIA team and AMD team for pushing this forward together!</p>
<p>We will continue pushing the boundaries of LLM inference. On our roadmap are further explorations into SWA (Sliding Window Attention) optimizations, AMD AITER integration, along with new advances in speculative decoding, to deliver even greater performance gains.</p>
<p>We invite you to try the latest version of SGLang and share your feedback. Thank you for being an essential part of this journey!</p>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"SGLang for gpt-oss: From Day 0 Support to Enhanced Performance","author":"Liangsheng Yin, Ke Bao","date":"August 27, 2025","previewImg":"/images/blog/gpt_oss/gpt_oss_preview.png"},"content":"\nWe are excited to announce a major update for SGLang, focusing on deep performance optimizations and new features for the recently released openai/gpt-oss-120b model. **While we had support from day zero, we took the last few weeks to enhance our engine to ensure you get the best possible performance.**\n\nThis post highlights our latest achievements: a significant performance improvement for gpt-oss with up to **2.1x** higher throughput on prefill and **2.25x** higher throughput on decode, out-of-the-box support for NVIDIA Blackwell \u0026 Hopper and AMD MI350 GPUs, speculative decoding support, and enhanced APIs to power complex agentic applications—all while maintaining the model's high accuracy.\n\nAll changes are now available in our main branch.\n\n### Get Started with SGLang\n\n```bash\npip install \"sglang[all]\u003e=0.5.1.post3\"\npython3 -m sglang.launch_server --model-path openai/gpt-oss-120b --tp 4\n```\n\nFor detailed instructions on environment setup and how to gain the best performance, please see our [guide in awesome-sglang](https://github.com/sgl-project/awesome-sglang/tree/main/gpt-oss).\n\n## By the Numbers: Comprehensive Benchmark Results 📊\n\nTo show the impact of our optimizations, we benchmarked SGLang across a range of hardware configurations. For all the results, the reproduction command can be found [here](https://github.com/sgl-project/sglang/tree/main/benchmark/gpt_oss).\n\n##### Low-Latency Performance (Batch Size = 1)\n\nFor latency-sensitive applications, we measured single-batch decode throughput across B200 and H100 GPUs, showcasing excellent performance.\n\n| Hardware / Precision | NVIDIA B200  | NVIDIA H100  |\n| -------------------- | ------------ | ------------ |\n| MXFP4                | 416.02 tok/s | 318.53 tok/s |\n| BF16                 | 315.63 tok/s | 293.12 tok/s |\n\n\u003cspan style=\"color: grey; font-size: 12px;\"\u003e\nB200 was tested with TP=4, H100 was tested with TP=8 and triton attention.\n\u003c/span\u003e\n\n##### High-Throughput Performance (Batch Size = 32)\n\nFor high-throughput applications, SGLang delivers significant performance gains over our initial Day 0 support and have shown great performance on both prefill and decode on different hardwares.\n\n\u003c!-- grey text --\u003e\n\n\u003cspan style=\"color: grey; font-size: 12px;\"\u003e\nThe results of AMD MI350 were tested with triton backend which is not fully optimized yet, and more optimizations with AMD AITER will be released soon.\n\u003c/span\u003e\n\n\u003cimg src=\"/images/blog/gpt_oss/combined_prefill_performance.svg\" alt=\"combined_prefill_performance.svg\" style=\"display:block; margin-left: auto; margin-right: auto; width: 75%\"\u003e\u003c/img\u003e\n\n\u003cimg src=\"/images/blog/gpt_oss/combined_decode_performance.svg\" alt=\"combined_decode_performance.svg\" style=\"display:block; margin-left: auto; margin-right: auto; width: 75%\"\u003e\u003c/img\u003e\n\n## Performance Deep Dive 🚀\n\nOur performance gains come from several key optimizations at the kernel level:\n\n- **FlashInfer Kernels for Blackwell**: To unlock peak performance for gpt-oss on Blackwell GPUs, we integrated highly optimized kernels from FlashInfer. This accelerates core components, including multi-head attention and Mixture of Experts (MoE) layers, on the new hardware.\n- **FlashAttention-3 for Hopper**: We modified the FlashAttention-3 kernels to support attention sinks, providing a significant speedup for inference on Hopper GPUs.\n- **Kernel Fusion and Reduction**: We performed several low-level fusions to reduce overhead. This includes fusing the RMS norm with all-reduce, merging the set KV buffer operation into RoPE, and fusing hidden states padding into quantization. We also removed unnecessary kernels, enabled [PDL](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programmatic-dependent-launch-and-synchronization) for some kernels, and reduced CPU overhead for greater efficiency.\n\n## Accuracy Alignment with Official Report 🎯\n\nWe validated our optimized gpt-oss implementation against the GPQA benchmark and confirmed that our results align closely with the official model card, ensuring that these speedups do not compromise the model's reasoning capabilities.\n\n| Reasoning Effort | SGLang | [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html#accuracy-evaluation-panels) | [Official](https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf) |\n| ---------------- | ------ | ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------ |\n| Low              | 65.6   | 65.3                                                                                                   | 67.1                                                                                                   |\n| Medium           | 72.1   | 72.4                                                                                                   | 73.1                                                                                                   |\n| High             | 79.8   | 79.4                                                                                                   | 80.1                                                                                                   |\n\n## Speculative Decoding Support 🦅\n\n**Speculative Decoding** is a key technique for improving LLM inference performance. [**EAGLE3**](https://arxiv.org/abs/2503.01840) is the current state-of-the-art speculative decoding method, and SGLang was the first framework to support it, thanks to close collaboration with EAGLE team.\n\nIn SGLang, you can easily launch gpt-oss model with EAGLE3 speculative decoding:\n\n```bash\n# On Hopper:\n# - Tree decoding (topk \u003e 1) and chain decoding (topk = 1) are supported on both FA3 and Triton backends.\npython3 -m sglang.launch_server --model openai/gpt-oss-120b --speculative-algorithm EAGLE3 --speculative-draft-model-path lmsys/EAGLE3-gpt-oss-120b-bf16 --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 --tp 4\npython3 -m sglang.launch_server --model openai/gpt-oss-120b --speculative-algorithm EAGLE3 --speculative-draft-model-path lmsys/EAGLE3-gpt-oss-120b-bf16 --speculative-num-steps 5 --speculative-eagle-topk 4 --speculative-num-draft-tokens 8 --tp 4\n\n# On Blackwell:\n# - Chain decoding (topk = 1) is supported on TRTLLM-MHA backend. Tree decoding (topk \u003e 1) is in progress, stay tuned!\n# - Both tree decoding (topk \u003e 1) and chain decoding (topk = 1) are supported on the Triton backend.\npython3 -m sglang.launch_server --model openai/gpt-oss-120b --speculative-algorithm EAGLE3 --speculative-draft lmsys/EAGLE3-gpt-oss-120b-bf16 --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 --tp 4\npython3 -m sglang.launch_server --model openai/gpt-oss-120b --speculative-algorithm EAGLE3 --speculative-draft lmsys/EAGLE3-gpt-oss-120b-bf16 --speculative-num-steps 5 --speculative-eagle-topk 4 --speculative-num-draft-tokens 8 --attention-backend triton --tp 4\n```\n\nFor `openai/gpt-oss-120b` model, we trained an EAGLE3 draft model, [`lmsys/EAGLE3-gpt-oss-120b-bf16`](https://huggingface.co/lmsys/EAGLE3-gpt-oss-120b-bf16) with [SpecForge](https://github.com/sgl-project/SpecForge), an efficient framework for speculative draft model training. Our trained draft model achieves a higher average acceptance length compared to [NVIDIA’s gpt-oss draft model](https://huggingface.co/nvidia/gpt-oss-120b-Eagle3).\n\n\u003cimg src=\"/images/blog/gpt_oss/accept_length.svg\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%\"\u003e\u003c/img\u003e\n\nWe also benchmarked `openai/gpt-oss-120b` with EAGLE3 on H200 TP4 and observed promising results across several standard benchmark datasets:\n\n\u003cimg src=\"/images/blog/gpt_oss/gpt_oss_eagle3_results.svg\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%\"\u003e\u003c/img\u003e\n\nwhich achieves:\n- **1.39x** speedup with the `steps=3, topk=1, num_draft_tokens=4` setting.\n- **1.52x** speedup with the `steps=5, topk=4, num_draft_tokens=8` setting.\n\n## Powering Agentic Applications 🤖\n\nTo better enable agentic workflows, SGLang offers [OpenAI Response API support](https://docs.sglang.ai/basic_usage/gpt_oss.html#responses-api) and [native chat completion support](https://docs.sglang.ai/advanced_features/tool_parser.html). Here is an example of how to build a simple web search agent with SGLang (`python3.12` and `gpt-oss` package are required for built-in tools, more setup details can be found [here](https://docs.sglang.ai/basic_usage/gpt_oss.html#responses-api)).\n\nLaunch the server:\n\n```bash\nexport EXA_API_KEY=YOUR_EXA_KEY\npython3 -m sglang.launch_server --port 30000 --model-path openai/gpt-oss-120b --tp 4 --tool-server demo \n```\n\nUse Response API to build a web search agent:\n\n```python\nimport openai\n\nclient = openai.OpenAI(\n    base_url=\"http://localhost:30000/v1\",\n    api_key=\"EMPTY\"\n)\nresponse = client.responses.create(\n    model=\"openai/gpt-oss-120b\",\n    tools=[{\"type\": \"web_search_preview\"}],\n    input=\"What does SGLang update today?\"\n)\n\nprint(response.output_text)\n```\n\n## What's Next? 🔮\n\nNone of the Day-0 support or the subsequent optimizations would have been possible without the collective effort of the SGLang community. Shout-out to the SGLang team, SpecForge team, FlashInfer team, Oracle team, Eigen AI team, NVIDIA team and AMD team for pushing this forward together!\n\nWe will continue pushing the boundaries of LLM inference. On our roadmap are further explorations into SWA (Sliding Window Attention) optimizations, AMD AITER integration, along with new advances in speculative decoding, to deliver even greater performance gains.\n\nWe invite you to try the latest version of SGLang and share your feedback. Thank you for being an essential part of this journey!\n","slug":"2025-08-27-gpt-oss"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2025-08-27-gpt-oss"},"buildId":"duMpKvhw40cCUQOchi6CB","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>