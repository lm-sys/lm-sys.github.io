<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>SpecBundle &amp; SpecForge v0.2: Production-Ready Speculative Decoding Models and Framework | LMSYS Org</title><meta name="title" content="SpecBundle &amp; SpecForge v0.2: Production-Ready Speculative Decoding Models and Framework | LMSYS Org"/><meta property="og:title" content="SpecBundle &amp; SpecForge v0.2: Production-Ready Speculative Decoding Models and Framework | LMSYS Org"/><meta name="twitter:title" content="SpecBundle &amp; SpecForge v0.2: Production-Ready Speculative Decoding Models and Framework | LMSYS Org"/><meta name="description" content="&lt;h2&gt;&lt;a id=&quot;tldr&quot; class=&quot;anchor&quot; href=&quot;#tldr&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 1..."/><meta property="og:description" content="&lt;h2&gt;&lt;a id=&quot;tldr&quot; class=&quot;anchor&quot; href=&quot;#tldr&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 1..."/><meta name="twitter:description" content="&lt;h2&gt;&lt;a id=&quot;tldr&quot; class=&quot;anchor&quot; href=&quot;#tldr&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1&quot; viewbox=&quot;0 0 1..."/><meta property="og:image" content="https://lmsys.org/images/blog/specbundle-phase1/preview.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/specbundle-phase1/preview.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2025-12-23-spec-bundle-phase-1"/><meta name="twitter:url" content="https://lmsys.org/blog/2025-12-23-spec-bundle-phase-1"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d62cc293bc63f5ee.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/pk4Ah5DI2m-wiqjA2l-6I/_buildManifest.js" defer=""></script><script src="/_next/static/pk4Ah5DI2m-wiqjA2l-6I/_ssgManifest.js" defer=""></script><script src="/_next/static/pk4Ah5DI2m-wiqjA2l-6I/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.io" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">SpecBundle &amp; SpecForge v0.2: Production-Ready Speculative Decoding Models and Framework</h1><p class="text-xl pt-2 pb-2">by: <!-- -->SpecForge Team, Ant Group AQ Team, Nex-AGI Team, EigenAI Team<!-- -->,<!-- --> <!-- -->Dec 23, 2025<!-- --></p><hr/><div class="pt-2 article"><h2><a id="tldr" class="anchor" href="#tldr" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TL;DR</h2>
<p>The SpecForge team has collaborated with multiple industry partners - including <strong>Ant, Meituan, Nex-AGI, and EigenAI</strong> - to release <a href="https://huggingface.co/collections/lmsys/specbundle"><strong>SpecBundle (Phase 1)</strong></a>, a collection of production-grade EAGLE-3 model checkpoints trained on large-scale datasets. <strong>SpecBundle</strong> is designed to improve the availability and real-world performance of speculative decoding, with Phase 1 focusing on instruct-tuned models.</p>
<p>Alongside this release, <a href="https://github.com/sgl-project/SpecForge"><strong>SpecForge v0.2</strong></a> delivers major system upgrades, including extensive refactoring for improved usability and support for multiple execution backends, further enhancing scalability and production readiness.</p>
<h2><a id="background" class="anchor" href="#background" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Background</h2>
<p><a href="https://arxiv.org/abs/2302.01318">Speculative decoding</a> was first introduced in 2023 as a promising technique for accelerating large language model (LLM) inference by using a lightweight draft model to propose multiple tokens that are subsequently verified by a stronger target model. In principle, this approach can substantially reduce decoding latency without compromising output quality, making it appealing for both local and enterprise deployments. Over the past few years, the research community has continued to refine this paradigm, proposing increasingly sophisticated methods that culminate in state-of-the-art approaches such as <a href="https://arxiv.org/abs/2503.01840">EAGLE3</a>, which demonstrate strong theoretical guarantees and empirical gains in both token acceptance rate and end-to-end speedup.</p>
<h3><a id="existing-problems" class="anchor" href="#existing-problems" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Existing Problems</h3>
<p>Despite these advances, speculative decoding‚Äîparticularly SOTA methods like EAGLE3, has not yet seen widespread adoption in the open-source community. We attribute this gap primarily to three factors.</p>
<p><strong>Factor 1:</strong> There is a lack of accessible, production-ready tooling for training speculative decoding models. Most existing implementations remain research prototypes that are either poorly maintained or narrowly scoped, while others offer only simplistic implementations without sufficient system-level optimization. As a result, these tools struggle to support the diverse range of model architectures and scales commonly used in today‚Äôs LLM ecosystem.</p>
<p><strong>Factor 2:</strong> The availability of high-quality draft models remains a major bottleneck. Effective speculative decoding critically depends on the strength of the draft model, yet such models are scarce in the open community, as summarized in the table below. Methods like EAGLE3 require additional draft-model training, and the publicly available EAGLE3 checkpoints are largely limited to releases mainly from the original authors. This constrained supply significantly hampers broader adoption.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Native MTP</th>
<th>Community EAGLE3</th>
<th>SpecBundle</th>
</tr>
</thead>
<tbody>
<tr>
<td>meta-llama/Llama-3.1-8B-Instruct</td>
<td>‚ùå</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>meta-llama/Llama-3.3-70B-Instruct</td>
<td>‚ùå</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>meta-llama/Llama-4-Scout-17B-16E-Instruct</td>
<td>‚ùå</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>Qwen/Qwen3-30B-A3B-Instruct-2507</td>
<td>‚ùå</td>
<td>‚ùå</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>Qwen/Qwen3-235B-A22B-Instruct-2507</td>
<td>‚ùå</td>
<td>‚úÖ</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>Qwen/Qwen3-Next-80B-A3B-Instruct-FP8</td>
<td>‚úÖ</td>
<td>‚ùå</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>Qwen/Qwen3-Coder-30B-A3B-Instruct</td>
<td>‚ùå</td>
<td>‚ùå</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>Qwen/Qwen3-Coder-480B-A35B-Instruct</td>
<td>‚ùå</td>
<td>‚ùå</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>inclusionAI/Ling-flash-2.0</td>
<td>‚ùå</td>
<td>‚ùå</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>moonshotai/Kimi-K2-Instruct</td>
<td>‚ùå</td>
<td>‚ùå</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>nex-agi/Qwen3-30B-A3B-Nex-N1</td>
<td>‚ùå</td>
<td>‚ùå</td>
<td>‚úÖ</td>
</tr>
<tr>
<td>nex-agi/Qwen3-32B-Nex-N1</td>
<td>‚ùå</td>
<td>‚ùå</td>
<td>‚úÖ</td>
</tr>
</tbody>
</table>
<p><strong>Factor 3</strong>: Most existing draft models are trained on relatively small or curated datasets and are not scaled to the large, diverse corpora used in modern LLM training. Consequently, these models often exhibit limited generalization and lower token acceptance rates when paired with strong target models, reducing the practical speedups achievable through speculative decoding. Without large-scale, production-grade draft models, the full potential of advanced approaches such as EAGLE3 remains largely unrealized.</p>
<h3><a id="motivation" class="anchor" href="#motivation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Motivation</h3>
<p>These gaps as mentioned above motivate the release of <a href="https://github.com/sgl-project/SpecForge"><strong>SpecForge v0.2</strong></a> and <a href="https://huggingface.co/collections/lmsys/specbundle"><strong>SpecBundle</strong></a>. As a neutral, open-source community, the SpecForge team aims to proactively advance speculative decoding by providing production-grade training frameworks and high-performance draft models, making the technique both practical and accessible to the broader community.</p>
<p>This initiative delivers several key benefits:</p>
<ol>
<li>Expand research possibilities by offering more standardized and scalable baselines for advancing speculative decoding methods.</li>
<li>Enable faster local inference and model serving, supporting lightweight deployment scenarios through tools such as <a href="https://github.com/ollama/ollama">Ollama</a>.</li>
<li>Lower the cost of enterprise deployments by improving inference throughput without sacrificing output quality with inference engines such as <a href="https://github.com/sgl-project/sglang">SGLang</a>.</li>
<li>Provide strong initialization points in the form of EAGLE3 checkpoints that can be efficiently fine-tuned for domain-specific tasks.</li>
<li>Improve the efficiency of reinforcement learning workflows by enabling techniques such as <a href="https://arxiv.org/abs/2510.26475">ReSpec</a> to be integrated into existing RL frameworks like <a href="https://github.com/THUDM/slime">slime</a>.</li>
</ol>
<h2><a id="specforge-v02" class="anchor" href="#specforge-v02" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>SpecForge v0.2</h2>
<p>It has been about five months since <strong>SpecForge</strong> was open-sourced, and thanks to the support of an amazing community, the system has evolved into a solution that is significantly more reliable, efficient, and scalable. Over the past two months, as we trained a wide range of models for SpecBundle, we identified several limitations in the original design of SpecForge. These insights motivated a comprehensive upgrade of the framework to improve both performance and usability. The major changes in <strong>SpecForge v0.2</strong> are summarized below.</p>
<h3><a id="user-friendliness-enhancment" class="anchor" href="#user-friendliness-enhancment" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>User-friendliness Enhancment</h3>
<p>In the early versions of SpecForge, some features were developed independently without sufficient consideration for long-term maintainability or user experience, which occasionally led to confusion for users. Over the past two months, we have prioritized usability and conducted substantial refactoring across the framework. Key improvements include:</p>
<ol>
<li>Refactored data processing pipelines to eliminate duplication and improve efficiency. For example, data regeneration is now up to <strong>10√ó faster</strong> than in v0.1 through data parallelism and asynchronous processing.</li>
<li>Unified online and offline training scripts into a single implementation. This consolidation ensures consistent training logic and avoids divergence between online and offline training modes.</li>
<li>Improved documentation structure and clarity, with a clearer logical workflow and better readability to help users get started and iterate more effectively.</li>
</ol>
<h3><a id="multi-backend-support" class="anchor" href="#multi-backend-support" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Multi-backend Support</h3>
<p>Earlier versions of SpecForge relied heavily on in-house implementations of target models, making model support both tedious and error-prone. To address this limitation and better leverage the broader ecosystem, we introduced a unified interface for target model integration.</p>
<p>In <strong>v0.2</strong>, we introduce the <code>Eagle3TargetModel</code> interface, which enables seamless support for multiple execution backends. Currently, SpecForge integrates both <strong>SGLang</strong> and <strong>Hugging Face Transformers</strong> as backends. Adding a new backend now requires implementing only the <code>Eagle3TargetModel.generate_eagle3_data</code> method, significantly lowering the barrier to extension and improving long-term maintainability.</p>
<pre><code class="hljs language-python">target_model = get_eagle3_target_model(
                pretrained_model_name_or_path=<span class="hljs-string">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span>,
                backend=<span class="hljs-string">&quot;sglang&quot;</span>,
                torch_dtype=torch.bfloat16,
                device=<span class="hljs-string">&quot;cuda&quot;</span>,
                cache_dir=args.model_download_dir,
                **target_model_kwargs,
            )
</code></pre>
<p>These backends not only reduce the burden of model implementation and performance optimization for developers, but also provide users with flexible choices across different training scenarios. With multiple backend options available, users can select the most suitable backend for their specific development and runtime requirements.</p>
<p align="center">
  <img src="/images/blog/specbundle-phase1/backend.png" alt="Logo preview">
  <br>
</p>
<h2><a id="the-specbundle-initiative" class="anchor" href="#the-specbundle-initiative" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The SpecBundle Initiative</h2>
<p>As discussed above, the open-source community continues to face significant bottlenecks in both the availability and performance of speculative decoding solutions. SpecBundle is a direct response to these challenges‚Äîan initiative driven jointly by the open-source community and industry partners to close these gaps. To the best of our knowledge, this represents the first open effort aimed at democratizing the adoption of speculative decoding by equipping mainstream open-source models with high-performance EAGLE3 draft model weights.</p>
<div style="border-left: 4px solid #3b82f6; padding: 10px 12px; margin: 12px 0; background: #eff6ff; border-radius: 8px;">
  <strong>üëâ Check out the <a href="https://docs.sglang.io/SpecForge/community_resources/specbundle.html">documentation on SpecBundle</a>.</strong>
</div>
<p>In this initial release, the SpecBundle roadmap focuses exclusively on instruct-tuned models, as outlined below. We believe that expanding speculative decoding support across a broader range of models will further reduce the cost of both local and enterprise deployments, while also enabling more efficient rollout in reinforcement learning (RL) training pipelines.</p>
<p align="center">
  <img src="/images/blog/specbundle-phase1/roadmap.png" alt="Logo preview">
  <br>
</p>
<p>At the same time, SpecBundle serves as a large-scale validation of SpecForge, demonstrating its efficiency, extensibility, and scalability. By successfully training draft models for target models ranging from 8B to 1T parameters, we confirm that recent architectural and system improvements have elevated SpecForge to production readiness.</p>
<p>We warmly welcome community contributions and industrial collaboration. If you share our vision of accelerating LLM inference and training, we invite you to join us in pushing the boundaries of what speculative decoding can achieve.</p>
<h3><a id="performance" class="anchor" href="#performance" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Performance</h3>
<p>For all models released in SpecBundle, we regenerated model responses using SGLang to better align the training data distribution with the actual model outputs. This alignment significantly improves token acceptance rates in speculative decoding.</p>
<p>In contrast to the original EAGLE papers, which rely on the ShareGPT and UltraChat datasets comprising approximately 320K samples, SpecBundle is trained on the <a href="https://huggingface.co/datasets/mlabonne/open-perfectblend">Perfect-Blend</a> dataset, which contains <strong>1.4M samples</strong> spanning a much broader set of domains‚Äîparticularly in coding and mathematics.</p>
<p>As a result, SpecBundle not only supports a wide range of mainstream instruct-tuned models, but also delivers strong and consistent speedup across diverse benchmarks, achieving <strong>up to 4√ó end-to-end inference speedup</strong> over standard decoding baselines.</p>
<ul>
<li><strong>Comparison with the existing open-sourced weights</strong></li>
</ul>
<div style="display:flex; gap:16px;">
  <img src="/images/blog/specbundle-phase1/llama4-perf.png" alt="Llama-4 scout" style="width:50%;">
  <img src="/images/blog/specbundle-phase1/qwen-235b-perf.png" alt="Qwen-235B" style="width:50%;">
</div>
<ul>
<li><strong>SpecBundle for models with more than 100B parameters</strong></li>
</ul>
<div style="display:flex; gap:16px;">
  <img src="/images/blog/specbundle-phase1/ling-perf.png" alt="ling-flash-v2" style="width:50%;">
  <img src="/images/blog/specbundle-phase1/kimi-perf.png" alt="kimi-k2" style="width:50%;">
</div>
<div style="border-left: 4px solid #3b82f6; padding: 10px 12px; margin: 12px 0; background: #eff6ff; border-radius: 8px;">
  <strong>üëâ Check out the <a href="https://huggingface.co/collections/lmsys/specbundle">full model collection</a>.</strong>
</div>
<p>We have published comprehensive benchmark results on the <a href="https://docs.sglang.io/SpecForge/SpecBundle/index.html">SpecBundle website</a>. Please visit the site for more detailed evaluation results.</p>
<h2><a id="roadmap" class="anchor" href="#roadmap" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Roadmap</h2>
<p>Last but not least, the SpecForge team will continue building and expanding the LLM ecosystem throughout 2026. We have just published the 2026 Q1 roadmap, and we would love to hear your feedback and have you join us on this journey.</p>
<p>Our upcoming efforts will primarily focus on:</p>
<ul>
<li>Long-context training</li>
<li>Vision‚ÄìLanguage Model (VLM) support</li>
<li>System-level performance enhancements</li>
<li>MTP finetuning</li>
<li>SpecBundle Phase 2 (reasoning models) and Phase 3 (VLMs)</li>
</ul>
<p>Whether you are a researcher, practitioner, or industry partner, we warmly welcome your ideas and contributions as we work together to push the boundaries of scalable and efficient LLM systems.</p>
<div style="border-left: 4px solid #3b82f6; padding: 10px 12px; margin: 12px 0; background: #eff6ff; border-radius: 8px;">
  <strong>üëâ Check out the <a href="https://github.com/sgl-project/SpecForge/issues/374">full roadmap</a>.</strong>
</div>
<h2><a id="acknowledgement" class="anchor" href="#acknowledgement" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgement</h2>
<p>We sincerely appreciate the collective efforts from both the developers in the open-source community and our industrial partners, especially <strong>Ant Group AQ Team</strong>, <strong>Meituan</strong>, <strong>Nex-AGI (Qiji Zhifeng)</strong>, and <strong>EigenAI</strong> for their invaluable contributions to the development of SpecBundle and SpecForge.</p>
<ul>
<li>SpecForge Team: Shenggui Li, Chao Wang, Yubo Wang, Yefei Chen, Yikai Zhu, Jiaping Wang, Jin Pan, Tao Liu, Fan Yin, Shuai Shi, Yineng Zhang</li>
<li>Ant Group AQ Team: Ji Li, Yanan Gao, Zhiling Ye</li>
<li>Meituan Search Team: Laixin Xie</li>
<li>Nex-AGI (Qiji Zhifeng) Team: Qiaoling Chen, Guoteng Wang, Peng Sun</li>
<li>EigenAI Team: Xiaomin Dong, Jinglei Cheng</li>
</ul>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"SpecBundle \u0026 SpecForge v0.2: Production-Ready Speculative Decoding Models and Framework","author":"SpecForge Team, Ant Group AQ Team, Nex-AGI Team, EigenAI Team","date":"December 23, 2025","previewImg":"/images/blog/specbundle-phase1/preview.png"},"content":"\n## TL;DR\n\nThe SpecForge team has collaborated with multiple industry partners - including **Ant, Meituan, Nex-AGI, and EigenAI** - to release [**SpecBundle (Phase 1)**](https://huggingface.co/collections/lmsys/specbundle), a collection of production-grade EAGLE-3 model checkpoints trained on large-scale datasets. **SpecBundle** is designed to improve the availability and real-world performance of speculative decoding, with Phase 1 focusing on instruct-tuned models.\n\nAlongside this release, [**SpecForge v0.2**](https://github.com/sgl-project/SpecForge) delivers major system upgrades, including extensive refactoring for improved usability and support for multiple execution backends, further enhancing scalability and production readiness.\n\n## Background\n\n[Speculative decoding](https://arxiv.org/abs/2302.01318) was first introduced in 2023 as a promising technique for accelerating large language model (LLM) inference by using a lightweight draft model to propose multiple tokens that are subsequently verified by a stronger target model. In principle, this approach can substantially reduce decoding latency without compromising output quality, making it appealing for both local and enterprise deployments. Over the past few years, the research community has continued to refine this paradigm, proposing increasingly sophisticated methods that culminate in state-of-the-art approaches such as [EAGLE3](https://arxiv.org/abs/2503.01840), which demonstrate strong theoretical guarantees and empirical gains in both token acceptance rate and end-to-end speedup.\n\n### Existing Problems\n\nDespite these advances, speculative decoding‚Äîparticularly SOTA methods like EAGLE3, has not yet seen widespread adoption in the open-source community. We attribute this gap primarily to three factors.\n\n**Factor 1:** There is a lack of accessible, production-ready tooling for training speculative decoding models. Most existing implementations remain research prototypes that are either poorly maintained or narrowly scoped, while others offer only simplistic implementations without sufficient system-level optimization. As a result, these tools struggle to support the diverse range of model architectures and scales commonly used in today‚Äôs LLM ecosystem.\n\n**Factor 2:** The availability of high-quality draft models remains a major bottleneck. Effective speculative decoding critically depends on the strength of the draft model, yet such models are scarce in the open community, as summarized in the table below. Methods like EAGLE3 require additional draft-model training, and the publicly available EAGLE3 checkpoints are largely limited to releases mainly from the original authors. This constrained supply significantly hampers broader adoption.\n\n| Model                                     | Native MTP | Community EAGLE3 | SpecBundle |\n| ----------------------------------------- | ---------- | ---------------- | ---------- |\n| meta-llama/Llama-3.1-8B-Instruct          | ‚ùå         | ‚úÖ               | ‚úÖ         |\n| meta-llama/Llama-3.3-70B-Instruct         | ‚ùå         | ‚úÖ               | ‚úÖ         |\n| meta-llama/Llama-4-Scout-17B-16E-Instruct | ‚ùå         | ‚úÖ               | ‚úÖ         |\n| Qwen/Qwen3-30B-A3B-Instruct-2507          | ‚ùå         | ‚ùå               | ‚úÖ         |\n| Qwen/Qwen3-235B-A22B-Instruct-2507        | ‚ùå         | ‚úÖ               | ‚úÖ         |\n| Qwen/Qwen3-Next-80B-A3B-Instruct-FP8      | ‚úÖ         | ‚ùå               | ‚úÖ         |\n| Qwen/Qwen3-Coder-30B-A3B-Instruct         | ‚ùå         | ‚ùå               | ‚úÖ         |\n| Qwen/Qwen3-Coder-480B-A35B-Instruct       | ‚ùå         | ‚ùå               | ‚úÖ         |\n| inclusionAI/Ling-flash-2.0                | ‚ùå         | ‚ùå               | ‚úÖ         |\n| moonshotai/Kimi-K2-Instruct               | ‚ùå         | ‚ùå               | ‚úÖ         |\n| nex-agi/Qwen3-30B-A3B-Nex-N1              | ‚ùå         | ‚ùå               | ‚úÖ         |\n| nex-agi/Qwen3-32B-Nex-N1                  | ‚ùå         | ‚ùå               | ‚úÖ         |\n\n**Factor 3**: Most existing draft models are trained on relatively small or curated datasets and are not scaled to the large, diverse corpora used in modern LLM training. Consequently, these models often exhibit limited generalization and lower token acceptance rates when paired with strong target models, reducing the practical speedups achievable through speculative decoding. Without large-scale, production-grade draft models, the full potential of advanced approaches such as EAGLE3 remains largely unrealized.\n\n### Motivation\n\nThese gaps as mentioned above motivate the release of [**SpecForge v0.2**](https://github.com/sgl-project/SpecForge) and [**SpecBundle**](https://huggingface.co/collections/lmsys/specbundle). As a neutral, open-source community, the SpecForge team aims to proactively advance speculative decoding by providing production-grade training frameworks and high-performance draft models, making the technique both practical and accessible to the broader community.\n\nThis initiative delivers several key benefits:\n\n1. Expand research possibilities by offering more standardized and scalable baselines for advancing speculative decoding methods.\n2. Enable faster local inference and model serving, supporting lightweight deployment scenarios through tools such as [Ollama](https://github.com/ollama/ollama).\n3. Lower the cost of enterprise deployments by improving inference throughput without sacrificing output quality with inference engines such as [SGLang](https://github.com/sgl-project/sglang).\n4. Provide strong initialization points in the form of EAGLE3 checkpoints that can be efficiently fine-tuned for domain-specific tasks.\n5. Improve the efficiency of reinforcement learning workflows by enabling techniques such as [ReSpec](https://arxiv.org/abs/2510.26475) to be integrated into existing RL frameworks like [slime](https://github.com/THUDM/slime).\n\n## SpecForge v0.2\n\nIt has been about five months since **SpecForge** was open-sourced, and thanks to the support of an amazing community, the system has evolved into a solution that is significantly more reliable, efficient, and scalable. Over the past two months, as we trained a wide range of models for SpecBundle, we identified several limitations in the original design of SpecForge. These insights motivated a comprehensive upgrade of the framework to improve both performance and usability. The major changes in **SpecForge v0.2** are summarized below.\n\n### User-friendliness Enhancment\n\nIn the early versions of SpecForge, some features were developed independently without sufficient consideration for long-term maintainability or user experience, which occasionally led to confusion for users. Over the past two months, we have prioritized usability and conducted substantial refactoring across the framework. Key improvements include:\n\n1. Refactored data processing pipelines to eliminate duplication and improve efficiency. For example, data regeneration is now up to **10√ó faster** than in v0.1 through data parallelism and asynchronous processing.\n2. Unified online and offline training scripts into a single implementation. This consolidation ensures consistent training logic and avoids divergence between online and offline training modes.\n3. Improved documentation structure and clarity, with a clearer logical workflow and better readability to help users get started and iterate more effectively.\n\n### Multi-backend Support\n\nEarlier versions of SpecForge relied heavily on in-house implementations of target models, making model support both tedious and error-prone. To address this limitation and better leverage the broader ecosystem, we introduced a unified interface for target model integration.\n\nIn **v0.2**, we introduce the `Eagle3TargetModel` interface, which enables seamless support for multiple execution backends. Currently, SpecForge integrates both **SGLang** and **Hugging Face Transformers** as backends. Adding a new backend now requires implementing only the `Eagle3TargetModel.generate_eagle3_data` method, significantly lowering the barrier to extension and improving long-term maintainability.\n\n```python\ntarget_model = get_eagle3_target_model(\n                pretrained_model_name_or_path=\"meta-llama/Llama-3.1-8B-Instruct\",\n                backend=\"sglang\",\n                torch_dtype=torch.bfloat16,\n                device=\"cuda\",\n                cache_dir=args.model_download_dir,\n                **target_model_kwargs,\n            )\n```\n\nThese backends not only reduce the burden of model implementation and performance optimization for developers, but also provide users with flexible choices across different training scenarios. With multiple backend options available, users can select the most suitable backend for their specific development and runtime requirements.\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/specbundle-phase1/backend.png\" alt=\"Logo preview\"\u003e\n  \u003cbr\u003e\n\u003c/p\u003e\n\n## The SpecBundle Initiative\n\nAs discussed above, the open-source community continues to face significant bottlenecks in both the availability and performance of speculative decoding solutions. SpecBundle is a direct response to these challenges‚Äîan initiative driven jointly by the open-source community and industry partners to close these gaps. To the best of our knowledge, this represents the first open effort aimed at democratizing the adoption of speculative decoding by equipping mainstream open-source models with high-performance EAGLE3 draft model weights.\n\n\u003cdiv style=\"border-left: 4px solid #3b82f6; padding: 10px 12px; margin: 12px 0; background: #eff6ff; border-radius: 8px;\"\u003e\n  \u003cstrong\u003eüëâ Check out the \u003ca href=\"https://docs.sglang.io/SpecForge/community_resources/specbundle.html\"\u003edocumentation on SpecBundle\u003c/a\u003e.\u003c/strong\u003e\n\u003c/div\u003e\n\nIn this initial release, the SpecBundle roadmap focuses exclusively on instruct-tuned models, as outlined below. We believe that expanding speculative decoding support across a broader range of models will further reduce the cost of both local and enterprise deployments, while also enabling more efficient rollout in reinforcement learning (RL) training pipelines.\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/specbundle-phase1/roadmap.png\" alt=\"Logo preview\"\u003e\n  \u003cbr\u003e\n\u003c/p\u003e\n\nAt the same time, SpecBundle serves as a large-scale validation of SpecForge, demonstrating its efficiency, extensibility, and scalability. By successfully training draft models for target models ranging from 8B to 1T parameters, we confirm that recent architectural and system improvements have elevated SpecForge to production readiness.\n\nWe warmly welcome community contributions and industrial collaboration. If you share our vision of accelerating LLM inference and training, we invite you to join us in pushing the boundaries of what speculative decoding can achieve.\n\n### Performance\n\nFor all models released in SpecBundle, we regenerated model responses using SGLang to better align the training data distribution with the actual model outputs. This alignment significantly improves token acceptance rates in speculative decoding.\n\nIn contrast to the original EAGLE papers, which rely on the ShareGPT and UltraChat datasets comprising approximately 320K samples, SpecBundle is trained on the [Perfect-Blend](https://huggingface.co/datasets/mlabonne/open-perfectblend) dataset, which contains **1.4M samples** spanning a much broader set of domains‚Äîparticularly in coding and mathematics.\n\nAs a result, SpecBundle not only supports a wide range of mainstream instruct-tuned models, but also delivers strong and consistent speedup across diverse benchmarks, achieving **up to 4√ó end-to-end inference speedup** over standard decoding baselines.\n\n- **Comparison with the existing open-sourced weights**\n\n\u003cdiv style=\"display:flex; gap:16px;\"\u003e\n  \u003cimg src=\"/images/blog/specbundle-phase1/llama4-perf.png\" alt=\"Llama-4 scout\" style=\"width:50%;\"\u003e\n  \u003cimg src=\"/images/blog/specbundle-phase1/qwen-235b-perf.png\" alt=\"Qwen-235B\" style=\"width:50%;\"\u003e\n\u003c/div\u003e\n\n- **SpecBundle for models with more than 100B parameters**\n\n\u003cdiv style=\"display:flex; gap:16px;\"\u003e\n  \u003cimg src=\"/images/blog/specbundle-phase1/ling-perf.png\" alt=\"ling-flash-v2\" style=\"width:50%;\"\u003e\n  \u003cimg src=\"/images/blog/specbundle-phase1/kimi-perf.png\" alt=\"kimi-k2\" style=\"width:50%;\"\u003e\n\u003c/div\u003e\n\n\u003cdiv style=\"border-left: 4px solid #3b82f6; padding: 10px 12px; margin: 12px 0; background: #eff6ff; border-radius: 8px;\"\u003e\n  \u003cstrong\u003eüëâ Check out the \u003ca href=\"https://huggingface.co/collections/lmsys/specbundle\"\u003efull model collection\u003c/a\u003e.\u003c/strong\u003e\n\u003c/div\u003e\n\nWe have published comprehensive benchmark results on the [SpecBundle website](https://docs.sglang.io/SpecForge/SpecBundle/index.html). Please visit the site for more detailed evaluation results.\n\n## Roadmap\n\nLast but not least, the SpecForge team will continue building and expanding the LLM ecosystem throughout 2026. We have just published the 2026 Q1 roadmap, and we would love to hear your feedback and have you join us on this journey.\n\nOur upcoming efforts will primarily focus on:\n\n- Long-context training\n- Vision‚ÄìLanguage Model (VLM) support\n- System-level performance enhancements\n- MTP finetuning\n- SpecBundle Phase 2 (reasoning models) and Phase 3 (VLMs)\n\nWhether you are a researcher, practitioner, or industry partner, we warmly welcome your ideas and contributions as we work together to push the boundaries of scalable and efficient LLM systems.\n\n\u003cdiv style=\"border-left: 4px solid #3b82f6; padding: 10px 12px; margin: 12px 0; background: #eff6ff; border-radius: 8px;\"\u003e\n  \u003cstrong\u003eüëâ Check out the \u003ca href=\"https://github.com/sgl-project/SpecForge/issues/374\"\u003efull roadmap\u003c/a\u003e.\u003c/strong\u003e\n\u003c/div\u003e\n\n## Acknowledgement\n\nWe sincerely appreciate the collective efforts from both the developers in the open-source community and our industrial partners, especially **Ant Group AQ Team**, **Meituan**, **Nex-AGI (Qiji Zhifeng)**, and **EigenAI** for their invaluable contributions to the development of SpecBundle and SpecForge.\n\n- SpecForge Team: Shenggui Li, Chao Wang, Yubo Wang, Yefei Chen, Yikai Zhu, Jiaping Wang, Jin Pan, Tao Liu, Fan Yin, Shuai Shi, Yineng Zhang\n- Ant Group AQ Team: Ji Li, Yanan Gao, Zhiling Ye\n- Meituan Search Team: Laixin Xie\n- Nex-AGI (Qiji Zhifeng) Team: Qiaoling Chen, Guoteng Wang, Peng Sun\n- EigenAI Team: Xiaomin Dong, Jinglei Cheng\n","slug":"2025-12-23-spec-bundle-phase-1"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2025-12-23-spec-bundle-phase-1"},"buildId":"pk4Ah5DI2m-wiqjA2l-6I","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>