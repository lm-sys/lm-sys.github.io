---
title: "How Long Can Open-source LLMs truly Promise on Context Length?"
author: "The LongChat Team"
date: "June 29, 2023"
previewImg: /images/blog/longchat/topic_retrieval_preview.png
---

In this blogpost, we introduce our latest series of chatbot models, [LongChat](https://github.com/DachengLi1/LongChat), featuring a new level of extended context length. 

LongChat is currently available in two sizes: 7B and 13B. 
They can not only handle a context length of up to 16K tokens but also precisely follow human instructions in dialogues and demonstrate strong human preferences in our recent benchmark  [MT-Bench](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge). 
Their preview versions are available at HuggingFace [lmsys/longchat-13b-16k](https://huggingface.co/lmsys/longchat-13b-16k) and [lmsys/longchat-7b-16k](https://huggingface.co/lmsys/longchat-7b-16k).
You can try them immediately in CLI or web interface using FastChat:
```python
python3 -m fastchat.serve.cli --model-path lmsys/longchat-7b-16k
```

There has been a significant surge of interest within the open-source community in developing language models with longer context or extending the context length of existing models like LLaMA. 
This trend has led to interesting observations and extensive discussions in various sources, such as [Kaiokendev’s blog](https://kaiokendev.github.io/context) and this [arXiv manuscript](https://arxiv.org/pdf/2306.15595.pdf); 
meanwhile, several notable models have been released claiming to support much longer context than LLaMA, notable ones include:
- [MPT-7B-storywriter](https://huggingface.co/mosaicml/mpt-7b-storywriter) supports 65K context length and extrapolates to 80K. 
- [MPT-30B-chat](https://huggingface.co/spaces/mosaicml/mpt-30b-chat) supports 8K context length.
- [ChatGLM2-6B](https://huggingface.co/THUDM/chatglm2-6b) supports 32K context.

At LMSYS Org, we have been concurrently exploring various techniques to lengthen the context of our models like [Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.3). 
In this blogpost, alongside the release of the LongChat series, we share our [evaluation tools](https://github.com/DachengLi1/LongChat.git) to verify the long-context capability of LLMs. 

Using our evaluation tools in combination with various academic long-context evaluation benchmarks, we conduct a thorough comparison of several open-source and commercial models that claim to support long context. 
Through this analysis, we examine how well these models deliver on their promised context length and assess their performance in multiple dimensions.

The data and code used to reproduce the results in the blog post are available in our LongChat [repo](https://github.com/DachengLi1/LongChat/tree/longeval). 
We provide a visualization in this [notebook](https://github.com/DachengLi1/LongChat/blob/longeval/longeval/topics_lines_demo.ipynb).

## LongChat Training Recipe

LongChat is finetuned from LLaMA models, which were originally pretrained with 2048 context length. 
The training recipe can be conceptually described in two-steps:

### Step 1: Condensing rotary embeddings
[Rotary position embedding](https://arxiv.org/abs/2104.09864v4) is a type of positional embedding that injects the information of position in Transformer. 
It is implemented in Hugging Face transformer by:
```python
query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
```
Where position_ids are indices such as 1, 2, 3, ... that denote the position of a token in the sentence. 
For instance, the token "today" in the sentence "today is a good day" has position_ids 1. 
The `apply_rotary_pos_emb()` function then applies a [transformation](https://arxiv.org/pdf/2104.09864.pdf) based on the provided position_ids.

The LLaMA model is pre-trained with rotary embedding on sequence length 2048, which means that it has not observed scenarios where position_ids > 2048 during the pre-training phase. 
Instead of forcing the LLaMA model to adapt to position_ids > 2048, we condense position_ids > 2048 to be within 0 to 2048. 
Intuitively, we conjecture this condensation can maximally reuse the model weights learned in the pre-training stage. See more insights from [Kaiokendev’s blog](https://kaiokendev.github.io/context).

We define a term `condensation ratio` by dividing the target new context length `y` by 2048. We then divide every position_ids by this ratio and feed it into the `apply_rotary_pos_emb()` function.
```python
query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids / ratio)
```
In this release, we fine-tune the model to a context length of 16384, and thus the condensation ratio is 8. For instance, a token with position_ids = 10000 becomes position_ids = 10000 / 8 = 1250, and the neighboring token 10001 becomes 10001 / 8 = 1250.125. 
This step requires no training.

### Step 2: Finetuning on Curated Conversation Data
After condensing the embedding, we perform the finetuning procedure on our curated conversation dataset. 
We reuse our collected user-shared conversations previously used for training Vicuna. 
We clean the data using FastChat data pipeline, and truncate these conversations so they are no longer than 16K. 
We finetune the model using standard next-token prediction loss. We fine-tune the 7B and 13B model with 80k and 18k conversations, respectively. 
To save memory, we use Pytorch FSDP and Flash Attention. Assume A100 is $3/hour on Cloud, the 7B model costs ~$300, and the 13B model costs ~$700. 

## Evaluation toolkits - LongEval
Recently, commercial and open-source models continue to tout their abilities to support expanded context length (from 8K, 32K, to 100K) in their latest releases, but how can we verify these claims?
The term "long-context capability" can mean different things for different model providers. For instance, does [MPT-StoryWriter's](https://huggingface.co/mosaicml/mpt-7b-storywriter) advertised 65K context length operate at the same capacity as OpenAI’s ChatGPT at 16K? This issue is also prevalent in our LongChat models development: how do we swiftly and effectively confirm if a freshly trained model can handle the intended context length?

To address this, we can base our evaluations on tasks that necessitate LLMs to process lengthy contexts, such as text generation, retrieval, summarization, and information association in long text sequences. 
Inspired by [recent discussions](https://twitter.com/DimitrisPapail/status/1658091355632189440), we've devised a long context test suite. This suite incorporates two tasks of varying degrees of difficulty, providing a simple and swift way to measure and compare long-context performance.

### Task 1: Coarse-grained Topic Retrieval
In real-world long conversations, users usually talk about and jump between several topics with the chatbot. The Topic Retrieval task mimics this scenario by asking the chatbot to retrieve the first topic in a long conversation consisting of multiple topics. An example task is:
```python
… (instruction of the task)
USER: I would like to discuss <TOPIC-1>
ASSISTANT: Sure! What about xxx of <TOPIC-1>?
… (a multi-turn conversation of <TOPIC-1>)
USER: I would like to discuss  <TOPIC-2>
…
USER: I would like to discuss <TOPIC-k>
… 
USER: What is the first topic we discussed?
ASSISTANT: 
```
This task is testing whether the model can locate a chunk of texts and associate it with the right topic name. We design a conversation to be 400 ~ 600 tokens long. Thus, this task is considered coarse-grained because the model may give correct predictions when it locates positions not too far away (<500 token distance) from the right ones.

### Task 2: Fine-grained Line Retrieval
To further test the model ability to locate and associate texts from a long conversation, we introduce a finer-grained Line Retrieval test. In this test, the chatbot needs to precisely retrieve a number from a long document, instead of a topic from long multi-round conversations. Below is a sample:
```python
line torpid-kid: REGISTER_CONTENT is <24169>
line moaning-conversation: REGISTER_CONTENT is <10310>
…
line tacit-colonial: REGISTER_CONTENT is <14564>
What is the <REGISTER_CONTENT> in line moaning-conversation?
```

The task was originally proposed in [Little Retrieval Test](https://github.com/anadim/the-little-retrieval-test). 
The original testcase uses numbers to denote a line, which we found smaller LLMs usually cannot comprehend well. 
To disentangle these factors and make it more suitable for testing open-source chatbots at various sizes by using random natural language (e.g., torpid-kid) instead.

We found these two tasks behave with expected characteristics:
1. The task can effectively capture the abilities of text generation, retrieval, and information association at long context, reflected by the retrieving accuracy.
2. It is easy to extend the tests to arbitrary lengths to test models’ capacity under different context lengths.
3. We have run sensitivity tests of both tasks and observed expected results. For example, the vanilla LLaMA models, pretrained with a 2K context length, can achieve perfect accuracy on both tasks when the test inputs length are <2K, but will immediately fail (nearly 0 accuracy) on any test inputs beyond 2K.

More details and example usage of LongEval can be found in this [notebook](https://github.com/DachengLi1/LongChat/blob/longeval/longeval/topics_lines_demo.ipynb).


## Results and findings
In this section, we share our evaluation and findings.
<br>
<p style="color:gray; text-align: center;">Table 1. Model Specifications.</p>
<table id="Table1" style="display: flex; justify-content: center;" align="left" >
<tbody>

<tr> <th>Model</th> <th>Size</th> <th>Instruction-tuned?</th> <th>Pretrained Context Length</th> <th>Finetune Context Length</th> <th>Claimed Context Length</th> <th>Open Source?</th></tr>

<tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-30b-chat">MPT-30-chat</a></td>  <td>30B</td>  <td>Yes</td>  <td>8K</td>  <td>-</td> <td>8K</td> <td>Yes</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-7b-storywriter">MPT-7b-storywriter</a></td>  <td>7B</td> <td>Yes</td>  <td>2K</td>  <td>65K</td>  <td>84K</td> <td>Yes</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/THUDM/chatglm2-6b">ChatGLM2-6b</a></td>  <td>6B</td>  <td>Yes</td>  <td>8K</td>  <td>?</td> <td>32K</td> <td>Yes</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/lmsys/longchat-13b-16k">LongChat-13b-16k (ours)</a></td>  <td>13B</td>  <td>Yes</td> <td>2K</td>  <td>16K</td>  <td>16K</td> <td>Yes</td> </tr>
<tr> <td><a target="_blank" href="https://chat.openai.com/">GPT-3.5-turbo</a></td>  <td>-</td>  <td>-</td>  <td>-</td> <td>-</td> <td>16K</td>  <td>No</td> </tr>
<tr> <td><a target="_blank" href="https://www.anthropic.com/index/introducing-claude">Anthropic Claude-1.3</a></td>  <td>-</td>  <td>-</td>  <td>-</td> <td>-</td> <td>100K</td>  <td>No</td> </tr>
</tbody>
</table>
In particular, we consider four open-sourced models and two proprietary models.

&shy;

### LongEval results

<img src="/images/blog/longchat/topic_retrieval.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 1000px;"></img>
<p style="color:gray; text-align: center;">Figure 2: Accuracy on the long range topic retrieval task.</p>

From the coarse-grained topic retrieval test results (Figure 2), we already observe questionable performance of open-source long-context models. For instance, Mpt-7b-storywriter claims to have a context length of 84K, but barely achieves 50% accuracy even at one fourth of its claimed context length (16K). 
chatglm2-6B cannot reliably retrieve the first topic even at the length of 6K (46% accuracy). Its accuracy falls to almost 0% when tested on > 10K context length. On the other hand, we observed that our LongChat-13B-16K model reliably retrieves the first topic, with comparable accuracy to gpt-3.5-turbo.

<img src="/images/blog/longchat/line_retrieval.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 1000px;"></img>
<p style="color:gray; text-align: center;">Figure 2: Accuracy on the long range line retrieval task.</p>

In the finer-grained line retrieval test, Mpt-7b-storywriter performs even worse than in the coarse-grained cases, dropping accuracy from ~50% to ~30%. Chatglm2-6B also observes degradation, and does not perform well at the shortest length we test (5K context length). In contrast, we observe that LongChat-13B-16K perform reliably, achieving near gpt-3.5/Anthropic-claude ability within 12K context length (we also find the preview version is not perfect at 12K-16K, see discussion section).


***Disentangle irrelevant LLM abilities in LongEval***
In topics and line retrieval tests, we observe mistakes caused by factors irrelevant to long-context ability, such as instruction-following ability. For instance, in the Line Retrieval test, the model may simply respond “sure, I will tell you the number” instead of returning an actual number. To give a fair comparison, we took two actions to avoid factors irrelevant to long-context capability: prompt engineering and compute accuracy based only on the cases where the model follows our instruction. Check our codes for details.

### Human preference benchmark (MT-bench)
In the previous section, we have observed that LongChat models perform well on long-range retrieval tasks, but does this come with a significant drop of human preference? To test whether it still follows human preferences, we use GPT-4 graded [MT-bench](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge), a set of challenging multi-turn conversation questions.

<br>
<p style="color:gray; text-align: center;">Table 2. Human preference scores comparing LongChat-13B to other models of similar sizes.</p>
<table id="Table1" style="display: flex; justify-content: center;" align="left" >
<tbody>

<tr> <th>Model</th> <th>MT-bench (score) </th> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/lmsys/longchat-13b-16k">LongChat-13B-16K </a></td>  <td>5.95</td>> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/lmsys/vicuna-13b-v1.3"> Vicuna-13B </a></td>  <td>6.39</td>  </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/WizardLM/WizardCoder-15B-V1.0"> WizardLM-15B</a></td>  <td>6.35</td>  </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/project-baize/baize-v2-13b"> Baize-v2-13B </a></td>  <td>5.75</td>  </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/NousResearch/Nous-Hermes-13b"> Nous-Hermes-13B </a></td>  <td>5.51</td>   </tr>
<tr> <td><a target="_blank" href="https://crfm.stanford.edu/2023/03/13/alpaca.html"> Alpaca-13B</a></td>  <td>4.53</td>  </tr>
</tbody>
</table>

&shy;
We find that LongChat-13B-16K is comparable to its closest alternatives - Vicuna-13B, which indicates that this long-range ability does not come with a significant sacrifice of its short-range ability. At the same time, LongChat-13B-16K is competitive compared to other models of the same size.

### Long sequence question answer benchmark 
In the previous sections, we have tested models on our (simple) long-range retrieval tasks and human preference tasks. But how do these models perform on more complex academic long-range reasoning tasks?  In this section, we study this by running the Qasper question answering dataset. We use the validation set selection and prompts from the [ZeroScrolls](https://www.zero.scrolls-benchmark.com/) long sequence benchmark.

<br>
<p style="color:gray; text-align: center;">Table 3. ZeroScrolls Benchmark (validation set)</p>
<table id="Table1" style="display: flex; justify-content: center;" align="left" >
<tbody>

<tr> <th>Benchmark</th> <th>LongChat-13B-16K</th> <th>LongChat-7B-16k</th> <th>Vicuna-13B-v1.3</th> <th>Vicuna-7B-v1.3</th> <th>GPT-4-8k</th></tr>
<tr> <td>Qasper (F1)</td>  <td>0.286</td> <td>0.275</td> <td>0.220</td> <td>0.190</td> <td>0.356</td> </tr>

</tbody>
</table>

We find that LongChat significantly outperforms Vicuna due to its longer context length. We leave more rigorous analysis on academic benchmarks for future work.
&shy;

## Discussion
We find that LongChat-13B-16K experiences an accuracy drop when the context length is near 16K on the fine-grained line retrieval task. In our preliminary attempts, we conjecture that this is because it is near the maximal fine-tuning length. For instance, training on even larger (e.g. 32K) can alleviate this problem. We are actively working on solving this in a near-future release.

## Conclusion
In our evaluations, commercial long-context models always fulfill its promise: gpt-3.5-16k and Anthropic Claude (almost) achieve perfect performance in both our benchmarks. However, existing open-sourced models often do not perform well in the claimed context-length - in fact, usually a much smaller context length.

<br>
<p style="color:gray; text-align: center;">Table 4. Ability levels of open source models supporting long context</p>
<table id="Table1" style="display: flex; justify-content: center;" align="left" >
<tbody>

<tr> <th></th> <th>Claimed Context Length</th> <th>Text generation</th> <th>Coarse Retrieval</th> <th>Fine-grained Retrieval</th></tr>
<tr> <td>Ability Description at claimed context length</td> <td>-</td> <td>Faithfully generate natural languages</td> <td>Retrieve information in a coarse granularity</td> <td>Retrieve information precisely in a fine-grained granularity</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/lmsys/longchat-13b-16k">LongChat-13B-16K </a> <td>16K</td> <td>⭐⭐⭐</td> <td>⭐⭐⭐</td> <td>⭐⭐</td></tr>
<tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-30b-chat">MPT-30B-chat</a></td> <td>8K</td> <td>⭐⭐⭐</td> <td>⭐⭐⭐</td> <td>⭐⭐</td></tr>
<tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-7b-storywriter">MPT-7B-storywriter</a></td> <td>80K</td> <td>⭐⭐⭐</td> <td>⭐⭐⭐</td> <td>⭐⭐</td></tr>
<tr> <td><a target="_blank" href="https://huggingface.co/THUDM/chatglm2-6b">ChatGLM2-6B</a></td> <td>32K</td>  <td>⭐⭐⭐</td> <td>⭐</td> <td>⭐</td></tr>
<tr> <td><a target="_blank" href="https://chat.openai.com/">GPT-3.5-turbo</a></td> <td>16K</td> <td>⭐⭐⭐</td> <td>⭐⭐⭐</td> <td>⭐⭐⭐</td></tr>
<tr> <td><a target="_blank" href="https://www.anthropic.com/index/introducing-claude">Anthropic Claude-1.3</a></td> <td>100K</td> <td>⭐⭐⭐</td> <td>⭐⭐⭐</td> <td>⭐⭐⭐</td></tr>
</tbody>
</table>

We qualatatively illustrate the level of performance in the table, and we would like to make our final thoughs - There is a gap between being able to perform inference on long contexts and an actual good long-context models. For instance, techniques such as Flash Attention enables model to be served in a much longer sequence length. However, the model provider needs to train the model well(e.g. with high-quality longer sequence data, or perform condensing as we explored) to perform good long-context text-generation, retrieval, and resoning ability. We call for the community to contribute to more evaluation benchmarks of long-context chatbots and further understand/bridge the gap. 

## Next Steps
Inspired by the promising performance and the simple training recipe of our 16K models, we would like to explore even longer chatbot models. We have observed many efficiency issues (e.g. memory and throughput) during training/inferencing chatbots of much longer context length. We plan to develop system for long-context chatbot training/inference

## Disclaimer
The benchmark longeval introduced in this blog post is not yet a rigorous academic benchmark. We are actively working on more systematic benchmarking.

## The Team
The LongChat models and this blogpost are developed, evaluated, and maintained by the following members:
Dacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianmin Zheng, Ion Stoica, Xuezhe Ma, Hao Zhang

(* Joint first-coauthor)

## Citation
```
@misc{longchat2023,
    title = {LongChat: How Long Can Context Length of Open-Source LLMs truly Promise?},
    url = {https://lmsys.org/blog/2023-06-29-longchat},
    author = {Dacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianmin Zheng, Ion Stoica, Xuezhe Ma, and Hao Zhang},
    month = {June},
    year = {2023}
}
```













