<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>Squeezing 1TB Model Rollout into a Single H200: INT4 QAT RL End-to-End Practice | LMSYS Org</title><meta name="title" content="Squeezing 1TB Model Rollout into a Single H200: INT4 QAT RL End-to-End Practice | LMSYS Org"/><meta property="og:title" content="Squeezing 1TB Model Rollout into a Single H200: INT4 QAT RL End-to-End Practice | LMSYS Org"/><meta name="twitter:title" content="Squeezing 1TB Model Rollout into a Single H200: INT4 QAT RL End-to-End Practice | LMSYS Org"/><meta name="description" content="&lt;blockquote&gt;
&lt;p&gt;üí° &lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Inspired by the Kimi K2 team, the SGLang RL team successfully landed an INT4 &lt;strong&gt;Quantization-Aware Tra..."/><meta property="og:description" content="&lt;blockquote&gt;
&lt;p&gt;üí° &lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Inspired by the Kimi K2 team, the SGLang RL team successfully landed an INT4 &lt;strong&gt;Quantization-Aware Tra..."/><meta name="twitter:description" content="&lt;blockquote&gt;
&lt;p&gt;üí° &lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Inspired by the Kimi K2 team, the SGLang RL team successfully landed an INT4 &lt;strong&gt;Quantization-Aware Tra..."/><meta property="og:image" content="https://lmsys.orghttps://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/fake-quantize-STE.png"/><meta name="twitter:image" content="https://lmsys.orghttps://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/fake-quantize-STE.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2026-01-26-int4-qat"/><meta name="twitter:url" content="https://lmsys.org/blog/2026-01-26-int4-qat"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d62cc293bc63f5ee.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/yZXPcMyfUCcN8SYdOM9NB/_buildManifest.js" defer=""></script><script src="/_next/static/yZXPcMyfUCcN8SYdOM9NB/_ssgManifest.js" defer=""></script><script src="/_next/static/yZXPcMyfUCcN8SYdOM9NB/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.io" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">Squeezing 1TB Model Rollout into a Single H200: INT4 QAT RL End-to-End Practice</h1><p class="text-xl pt-2 pb-2">by: <!-- -->SGLang RL Team, InfiXAI Team, Ant Group Asystem &amp; AQ Infra Team, slime Team, RadixArk Team<!-- -->,<!-- --> <!-- -->Jan 26, 2026<!-- --></p><hr/><div class="pt-2 article"><blockquote>
<p>üí° <strong>TL;DR:</strong></p>
<p>Inspired by the Kimi K2 team, the SGLang RL team successfully landed an INT4 <strong>Quantization-Aware Training (QAT)</strong> pipeline. By combining <strong>fake quantization during training</strong> with <strong>real quantization at inference (W4A16)</strong>, we achieved stability and train‚Äìinfer consistency comparable to BF16 full-precision training. Meanwhile, extreme INT4 compression allows single-node rollout for ~1TB-scale models, eliminating cross-node communication bottlenecks and significantly improving rollout efficiency‚Äîan open-source reference that balances high performance and low cost.</p>
</blockquote>
<h2><a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h2>
<p>Recently, the SGLang RL team has made significant progress in RL training stability, efficiency, and application scenarios, including:</p>
<ul>
<li><strong>INT4 QAT End-to-End Training</strong>: We implemented a complete QAT INT4 closed-loop solution from training to inference and provided a detailed <a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/slime/int4/readme-en.md">technical recipe</a>, significantly improving rollout efficiency and stability.</li>
<li><strong>Unified Multi-Turn VLM/LLM Training</strong>: We provided an implementation for the VLM multi-turn sampling paradigm <a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/slime/vlm-multi-turn/readme.md">blog</a>. Developers only need to write a customized <code>rollout</code> function to easily start multi-turn RL for VLM, just like training LLM.</li>
<li><strong>Rollout Router Replay</strong>: We implemented the <strong><a href="https://github.com/THUDM/slime/blob/58525eb986c66a271aa31077e17b8afebe704b4f/tests/test_qwen3_30B_A3B_r3.py#L79">Rollout Router Replay</a></strong> mechanism, significantly improving RL stability for MoE models during RL training.</li>
<li><strong>FP8 End-to-End Training</strong>: We successfully implemented <strong><a href="https://lmsys.org/blog/2025-11-25-fp8-rl/">end-to-end FP8 training and sampling</a></strong> in RL scenarios, further unlocking hardware performance.</li>
<li><strong>Speculative Decoding in RL</strong>: We successfully practiced <strong><a href="https://thudm.github.io/slime/advanced/speculative-decoding.html">speculative sampling</a></strong> in RL scenarios, achieving lossless acceleration for large-scale training.</li>
</ul>
<p>Building on top of these, we went one step further: on the slime framework, we reproduced and deployed an <strong>end-to-end INT4 QAT</strong> solution: <strong><a href="https://github.com/THUDM/slime/blob/58525eb986c66a271aa31077e17b8afebe704b4f/scripts/low_precision/run-kimi-k2-Thinking-int4.sh">INT4 Quantization-Aware Training (QAT)</a></strong>. This solution is deeply inspired by the Kimi team‚Äôs K2-Thinking technical report and its <strong>W4A16 QAT (Quantization-Aware Training)</strong> practice: <a href="https://www.zhihu.com/question/1969558404759544488/answer/1970539327902679960"><strong>W4A16 QAT (Quantization-Aware Training)</strong></a>. To pay tribute to pioneers and give back to the community, this article <strong>dissects</strong> the technical details of building the full pipeline in an open-source ecosystem, aiming to provide a practical reference that balances stability and performance.</p>
<p><strong>Key benefits at a glance:</strong></p>
<ul>
<li><strong>Break the VRAM bottleneck</strong>: With weight compression and low-bit quantization, ~1TB-scale K2-like models can be shrunk to fit on a single H200 (141GB) GPU, avoiding cross-node communication bottlenecks.</li>
<li><strong>Train‚Äìinfer consistency</strong>: Training uses QAT to shape weights into an INT4-friendly distribution; inference uses W4A16 (INT4 weights, BF16 activations). Both rely on BF16 Tensor Cores, achieving train‚Äìinfer consistency comparable to BF16 full precision.</li>
<li><strong>Single-node efficiency doubling</strong>: For very large models, INT4 greatly reduces VRAM and bandwidth pressure, delivering rollout efficiency significantly higher than W8A8 (FP8 weights, FP8 activations).</li>
</ul>
<p>This project is jointly completed by the <strong>SGLang RL team, InfiXAI team, Ant Group Asystem &amp; AQ Infra team, slime team, and RadixArk team</strong>. Related features and recipes have been synced to the <a href="https://github.com/THUDM/slime">slime</a> and <a href="https://github.com/radixark/miles">Miles</a> communities. We welcome everyone to try them out and contribute. We are also further challenging ourselves with MXFP8 and NVFP4. We also gratefully acknowledge <a href="https://www.linkedin.com/company/verda-cloud/">Verda Cloud</a> for compute resource sponsorship.</p>
<h2><a id="technical-overview" class="anchor" href="#technical-overview" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Technical Overview</h2>
<h3><a id="overall-pipeline" class="anchor" href="#overall-pipeline" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Overall Pipeline</h3>
<p>We implemented a complete INT4 QAT closed loop from training to inference, as illustrated below:</p>
<div align="center">
  <img src="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/QAT-INT4-e2e.png" alt="End-to-end QAT INT4 pipeline" width="80%"  />
</div>
<p>During the <strong>QAT training stage</strong>, the training side maintains BF16 master weights, while the forward pass introduces quantization noise via <strong>fake quantization</strong>. ‚ÄúFake‚Äù means we do not truly convert BF16 tensors to low-precision INT4 storage; instead, we keep the floating-point compute path and insert <strong>QDQ (Quantize-DeQuantize)</strong> operations to emulate low-precision arithmetic. Concretely, high-precision weights are first ‚Äúdiscretized‚Äù into INT4 and then immediately restored. Although the physical dtype remains floating point, the value precision is effectively reduced. The discrepancy between the original and restored values introduces quantization error, which is mathematically equivalent to injecting noise into the network, forcing the model to adapt to this precision loss via gradient updates.</p>
<p>For the backward pass, we use <strong>STE (Straight-Through Estimator)</strong> to bypass the non-differentiability of quantization. The core quantization operator is <strong>rounding</strong>, which is a step function whose derivative is 0 almost everywhere. This would completely block gradient flow and prevent updates to the underlying master weights. STE uses a <strong>‚Äústraight-through gradient estimator‚Äù</strong> strategy: during backprop it defines the derivative of rounding as 1 (i.e., treats it as an identity mapping). This is like building a bridge over a cliff, enabling gradients to pass through the rounding layer and update the high-precision floating-point weights, thus closing the QAT training loop.</p>
<p>In the <strong>weight conversion stage</strong>, we export the converged BF16 weights and perform <strong>real quantization</strong>, converting them into INT4 formats suitable for inference engines (e.g., Marlin).</p>
<p>In the <strong>RL rollout stage</strong>, SGLang loads INT4 weights and runs efficient W4A16 inference (INT4 weights √ó BF16 activations). The generated experience data flows back to the first stage for the next RL training iteration, forming a self-consistent closed loop.</p>
<h3><a id="key-strategy-choices" class="anchor" href="#key-strategy-choices" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Key Strategy Choices</strong></h3>
<p>For quantization format, we follow <a href="https://huggingface.co/moonshotai/Kimi-K2-Thinking">Kimi-K2-Thinking</a> and choose <strong>INT4 (W4A16)</strong>. Compared to FP4, INT4 has broader support on existing hardware (pre-Blackwell), and the ecosystem already has mature high-performance Marlin kernels. Experiments show that with a 1√ó32 scale granularity, INT4 provides sufficient dynamic range and stable accuracy, and its performance and tooling are well-optimized. As an industry ‚Äúgood enough‚Äù quantization standard, INT4 strikes a rational balance across performance, risk, and maintenance cost. That said, we also plan to explore FP4 RL on NVIDIA Blackwell GPUs in the future.</p>
<p>For training, we use the classic combination of <strong>fake quantization + STE</strong>. By maintaining BF16 master weights, simulating quantization noise in the forward pass, and passing gradients straight through in the backward pass, we maximize convergence and stability in low-precision training.</p>
<h2><a id="training-side-retrofitting-fake-quantization-into-megatron-lm" class="anchor" href="#training-side-retrofitting-fake-quantization-into-megatron-lm" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Training Side: Retrofitting Fake Quantization into Megatron-LM</h2>
<h3><a id="implementing-fake-quantization-and-ste-straight-through-estimator" class="anchor" href="#implementing-fake-quantization-and-ste-straight-through-estimator" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Implementing Fake Quantization and STE (Straight-Through Estimator)</h3>
<div align="center">
  <img src="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/fake-quantize-STE.png" alt="Training-side Fake Quantization & STE" width="80%"  />
</div>
<p>The core goal of this stage is to simulate quantization error on-the-fly during training, forcing the model to ‚Äúlearn‚Äù to adapt to low-precision representations. We therefore adopt <strong>fake quantization</strong>: while weights are stored and updated in BF16, they are temporarily mapped into the INT4 precision range in the forward pass.</p>
<p>Implementation-wise, the core logic lives in the <code>_FakeInt4QuantizationSTE</code> class in <code>megatron/core/extensions/transformer_engine.py</code>. It performs dynamic quantization based on per-group max absolute value, emulating INT4‚Äôs <code>[-7, 7]</code> range and clipping, but still computes in BF16 and only injects quantization error. For the crucial backward pass, we introduce <strong>STE</strong>, ensuring gradients pass through the quantization layer unchanged to update master weights, keeping training continuous.</p>
<h3><a id="fake-quantization-ablations" class="anchor" href="#fake-quantization-ablations" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fake Quantization Ablations</h3>
<p>To validate the necessity of QAT and study the impact of train‚Äìinfer precision mismatch, we designed ablations for two asymmetric scenarios:</p>
<ul>
<li><strong>QAT INT4 training enabled, BF16 rollout</strong></li>
<li><strong>QAT training disabled, direct INT4 rollout</strong></li>
</ul>
<p>We measure train‚Äìinfer inconsistency using the absolute difference in log probabilities (Logprob Abs Diff).</p>
<p><img src="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/moonlight-1.png" alt="Rollout BF16, training-side comparison of QAT INT4 effect" width="45%" /> <img src="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/moonlight-2.png" alt="Rollout INT4 weight-only, training-side comparison of QAT INT4 effect" width="45%" /></p>
<p><strong>The left plot shows ‚ÄúQAT INT4 training + BF16 rollout‚Äù</strong> (the <strong>red curve</strong>). Interestingly, even with high-precision BF16 inference, the error remains significantly higher. This is because QAT has already adapted weights to INT4 quantization noise via ‚Äúcompensation‚Äù; if we remove quantization at inference, that compensation becomes a perturbation, causing <strong>distribution shift</strong>.</p>
<p><strong>The right plot shows ‚Äúno QAT training + direct INT4 rollout‚Äù</strong> (the <strong>red curve</strong>), corresponding to a conventional post-training quantization (PTQ) setup. Since the model never saw quantization noise during training, compressing weights to INT4 causes severe information loss and shifts feature distributions relative to training, resulting in errors that oscillate and increase with training steps.</p>
<p><strong>Conclusion:</strong> These results strongly indicate that <strong>training-side fake quantization and inference-side real quantization must be enabled together</strong>. Only when the simulated noise during training is <strong>strictly aligned</strong> with the true quantization during inference can we suppress train‚Äìinfer mismatch, avoid distribution shift, and keep errors near baseline‚Äîthus truly closing the loop for end-to-end low-precision RL training.</p>
<h2><a id="weight-update-stage" class="anchor" href="#weight-update-stage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Weight Update Stage</h2>
<h3><a id="weight-flow-and-dynamic-format-adaptation" class="anchor" href="#weight-flow-and-dynamic-format-adaptation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Weight Flow and Dynamic Format Adaptation</h3>
<div align="center">
  <img src="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/marlin_optimization.jpg" alt="SGLang-side weight handling pipeline" width="80%"  />
</div>
<p>To reuse existing inference-side optimizations in SGLang, we directly adopted its built-in <strong>Marlin kernel</strong> as the INT4 inference backend. However, in practice we encountered a notable ‚Äúformat gap‚Äù: QAT training outputs weights in standard formats (similar to Hugging Face), while SGLang‚Äôs Marlin kernel requires weights to be specially <strong>packed</strong> and <strong>permuted</strong> so that the kernel can read them efficiently.</p>
<p>Given that RL training requires frequent weight updates, we must solve format compatibility. We therefore designed a reverse <code>restore_weights_before_loading</code> <strong>safety mechanism</strong>. Using cached <code>_original_shapes</code> metadata, it restores (resizes) the in-memory Marlin weight format back to original shapes <strong>before</strong> any weight update happens. This prevents runtime errors due to shape mismatches and enables smooth switching between standard weight formats and Marlin formats. We also added a system-level <code>post_process_weights</code> API to allow the control plane to explicitly trigger this process according to the training schedule.</p>
<p>To address post-load format adaptation, we implemented a <strong>dynamic weight management mechanism</strong> in <code>compressed_tensors_moe.py</code>. After weight loading finishes, the system automatically runs <code>process_weights_after_loading</code>, calling operators like <code>gptq_marlin_moe_repack</code> and <code>marlin_moe_permute_scales</code> to convert standard weights into highly optimized Marlin formats in memory, maximizing memory-access and compute efficiency for inference.</p>
<h3><a id="quantization-during-weight-updates" class="anchor" href="#quantization-during-weight-updates" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Quantization During Weight Updates</h3>
<div align="center">
  <img src="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/weights-update.jpg" alt="Weight update" width="80%"  />
</div>
<p>Now comes the core <strong>real quantization</strong> step. Unlike training-time fake quantization, this step irreversibly compresses precision via <code>int4_block_quantize</code>: with a configured group size, we compute per-group scales and map high-precision floats into the INT4 integer domain <code>[-7, 7]</code>.</p>
<p>To maximize VRAM efficiency, we then do <strong>bit packing</strong>. Since PyTorch lacks a native INT4 dtype, we implement <code>pack_int4_to_int32</code> using bitwise tricks to tightly pack 8 INT4 values into one INT32 integer (i.e., <code>8 √ó 4 bits = 32 bits</code>). Finally, these packed weights together with scales are passed to the inference engine, completing the conversion from ‚Äútraining format‚Äù to ‚Äúinference format‚Äù.</p>
<h2><a id="inference-stage" class="anchor" href="#inference-stage" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Inference Stage</h2>
<div align="center">
  <img src="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/sglang-w4a16.png" alt="SGLang W4A16 inference" width="80%"  />
</div>
<p><strong>Minimal packing and near-zero-overhead unpacking</strong></p>
<p>During RL rollout, we directly reuse SGLang‚Äôs mature W4A16 quantization solution. SGLang stores weights in a compact INT4 format by packing two 4-bit values into one byte, saving <strong>75%</strong> memory compared to BF16. At inference time, Triton kernels unpack efficiently using bit operations (<code>&gt;&gt; 4</code> and <code>&amp; 0xF</code>). Thanks to overlap between compute and IO, this unpacking is almost zero-overhead.</p>
<p><strong>Deep fusion for MoE operators</strong></p>
<ul>
<li><strong>Memory optimization</strong>: SGLang introduces a dynamic <code>moe_align_block_size</code> that chooses <code>block_size</code> based on current token counts and expert distribution, grouping and aligning tokens for the same expert to improve bandwidth utilization.</li>
<li><strong>Compute fusion</strong>: Besides integrating a high-performance <a href="https://github.com/IST-DASLab/marlin"><strong>Marlin INT4</strong></a> implementation, SGLang also fuses the gating part into a single high-performance kernel to avoid repeated kernel launches and intermediate reads/writes. This INT4 inference scheme is compatible with mainstream formats such as GPTQ and AWQ, and supports both symmetric and asymmetric modes.</li>
</ul>
<h2><a id="int4-qat-rl-results" class="anchor" href="#int4-qat-rl-results" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>INT4 QAT RL Results</h2>
<h3><a id="training-results" class="anchor" href="#training-results" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Training Results</h3>
<ul>
<li><strong>Training side</strong></li>
</ul>
<div align="center">
  <img src="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/qwen3-235b-raw-reward.png" alt="Qwen3-235B-A22B Raw-Reward comparison" width="45%"  /> 
  <img src="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/kimi-k2-raw-reward.png" alt="Kimi-K2-Thinking Raw-Reward comparison" width="45%"  />
</div>
<p>The plots above show training performance on the dapo-math-17k dataset for Qwen3-235B-A22B and Kimi-K2-Thinking under the slime framework. Compared with <strong>‚ÄúBF16 train‚ÄìBF16 infer‚Äù</strong> and <strong>‚ÄúBF16 train‚ÄìFP8 infer‚Äù</strong>, the <strong>‚ÄúBF16 train‚ÄìINT4 infer‚Äù</strong> setup still achieves steady Raw-Reward growth with a trend largely consistent with the former two, demonstrating the effectiveness of this approach.</p>
<ul>
<li><strong>Evaluation side</strong></li>
</ul>
<div align="center">
  <img src="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/qwen3-235b-AIME.png" alt="Qwen3-235B-A22B AIME evaluation comparison" width="45%"  /> 
  <img src="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/kimi-k2-AIME.png" alt="Kimi-K2-Thinking AIME evaluation comparison" width="45%"  />
</div>
<p>To evaluate model capability more rigorously, we run an evaluation on the aime-2024 benchmark every 10 training steps. The plots show the scoring trajectories of Qwen3-235B-A22B and Kimi-K2-Thinking under different RL training configurations.</p>
<p>The experiments indicate that the <strong>‚ÄúBF16 train‚ÄìINT4 infer‚Äù</strong> scheme not only exhibits a stable upward trend in evaluation scores, but also closely overlaps with <strong>‚ÄúBF16 train‚ÄìBF16 infer‚Äù</strong> and <strong>‚ÄúBF16 train‚ÄìFP8 infer‚Äù</strong> in both slope and peak score. This strong alignment suggests that low-bit quantization does not harm core representational capacity, enabling large compute savings while preserving (or matching) full-precision generalization performance.</p>
<h3><a id="traininfer-gap" class="anchor" href="#traininfer-gap" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Train‚ÄìInfer Gap</h3>
<div align="center">
  <img src="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/qwen3-30b-train-infer-gap.png" alt="Qwen3-30B-A3B train‚Äìinfer gap comparison" width="45%"  /> 
  <img src="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/qwen3-235b-train-infer-gap.png" alt="Qwen3-235B-A22B train‚Äìinfer gap comparison" width="45%"  />
</div>
<p>To visualize effectiveness, we validated QAT RL training on Qwen3-30B and Qwen3-235B. The Y-axis shows the absolute logprob difference between training-side and inference-side outputs; lower values mean stronger consistency. Results show that INT4 (<strong>green dashed</strong>) almost overlaps with the BF16 baseline (<strong>red solid</strong>), and is significantly lower than FP8 (<strong>blue dashed</strong>). This confirms that INT4 QAT can effectively avoid the accuracy loss in the <strong>‚ÄúBF16 train‚ÄìFP8 infer‚Äù</strong> mode and achieve train‚Äìinfer behavior indistinguishable from full precision.</p>
<p><strong>We hypothesize two reasons behind this consistency:</strong></p>
<ul>
<li><strong>Truncation error suppression</strong>: Training-side fake quantization constrains weights to the INT4 range. This constraint <a href="https://www.zhihu.com/question/1969558404759544488/answer/1970539327902679960">can effectively reduce floating-point rounding errors caused by non-deterministic accumulation orders in parallel matmul (i.e., the ‚Äúadding a small number to a large number‚Äù precision loss).</a></li>
<li><strong>High-precision compute</strong>: Inference uses W4A16 and relies on <strong>BF16 Tensor Cores</strong> throughout, keeping compute precision highly aligned with training.</li>
</ul>
<h3><a id="rollout-speedup" class="anchor" href="#rollout-speedup" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Rollout Speedup</h3>
<div align="center">
  <img src="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/qwen3-235b-rollout-performance.png" alt="Qwen3-235B-A22B rollout performance comparison" width="45%"  />
  <img src="https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/kimi-k2-rollout-performance.png" alt="Kimi-K2-Thinking rollout performance comparison" width="45%"  />
</div>
<p>From the Qwen3-235B rollout performance plot, we can see that INT4 (<strong>green dash-dot</strong>) and FP8 (<strong>blue dashed</strong>) both significantly speed up compared to the BF16 baseline (<strong>red solid</strong>), but the gap between INT4 and FP8 is not huge. This is largely limited by current hardware: NVIDIA H-series GPUs do not have native INT4 Tensor Cores. W4A16 essentially still uses BF16 Tensor Cores for compute; while it greatly reduces memory bandwidth pressure, it cannot gain the compute uplift of native FP8 Tensor Cores as W8A8 does. Therefore, INT4 only shows a slight advantage in per-step latency and remains in roughly the same performance tier as FP8.</p>
<p>For Kimi-K2-Thinking rollout performance, first look at the <strong>communication bottleneck</strong> in the two-node scenario: FP8 (<strong>red line</strong>) and INT4 (<strong>blue line</strong>) are similar, because H-series GPUs lack native INT4 compute units and INT4 cannot speed up compute, so overall performance is still limited by cross-node bandwidth.</p>
<p>However, the single-node result (the <strong>green line</strong>) reveals INT4‚Äôs <strong>true value‚ÄîVRAM compression</strong>. By halving model size, we can load ~1TB-scale models fully into a single machine‚Äôs VRAM, eliminating expensive cross-node communication and greatly reducing rollout time. This strongly demonstrates that under current hardware, the main benefit of INT4 QAT is enabling efficient single-node rollouts via VRAM compression.</p>
<h2><a id="summary-and-future-work" class="anchor" href="#summary-and-future-work" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Summary and Future Work</h2>
<p>By reproducing the approach in an open-source framework, we validated the effectiveness of the INT4 QAT scheme proposed by the Kimi team:</p>
<ul>
<li><strong>Accuracy reproduction</strong>: In slime reproductions, we observed the same INT4 QAT accuracy advantages, matching the BF16 baseline.</li>
<li><strong>Efficiency improvement</strong>: Rollout throughput improved significantly, validating the value of low-bit quantization in RL.</li>
</ul>
<p>Future work:</p>
<ul>
<li><strong>Training-side efficiency optimization</strong>: Today, adding QAT fake quantization introduces extra compute overhead during training, making it noticeably slower than BF16. This partially offsets the end-to-end gains from faster rollout. We plan to propose a new optimization to address this training-side bottleneck and accelerate the full pipeline.</li>
<li><strong>Inference-side FP4</strong>: As NVIDIA Blackwell becomes more widely available, we will actively explore FP4 precision for RL training and inference to further tap into hardware potential.</li>
</ul>
<p>slime‚Äôs attempt at INT4 QAT not only demonstrates the feasibility of reproducing industrial state-of-the-art techniques in an open-source ecosystem, but also opens a new path for low-cost training at extreme scale. We hope this solution helps more developers deeply understand QAT and promote its practical adoption in RL.</p>
<h2><a id="acknowledgements" class="anchor" href="#acknowledgements" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgements</h2>
<p>SGLang RL Team: Ji Li, Yefei Chen, Xi Chen, BBuf, Chenyang Zhao</p>
<p>InfiXAI Team: Mingfa Feng, Congkai Xie, Shuo Cai</p>
<p>Ant Group Asystem &amp; AQ Infra Team: Yanan Gao, Zhiling Ye, Yuan Wang, Xingliang Shi</p>
<p>slime Team: Zilin Zhu, Lei Li, Haisha Zhao</p>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Squeezing 1TB Model Rollout into a Single H200: INT4 QAT RL End-to-End Practice","author":"SGLang RL Team, InfiXAI Team, Ant Group Asystem \u0026 AQ Infra Team, slime Team, RadixArk Team","date":"January 26, 2026","previewImg":"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/fake-quantize-STE.png"},"content":"\n\u003e üí° **TL;DR:**\n\u003e\n\u003e Inspired by the Kimi K2 team, the SGLang RL team successfully landed an INT4 **Quantization-Aware Training (QAT)** pipeline. By combining **fake quantization during training** with **real quantization at inference (W4A16)**, we achieved stability and train‚Äìinfer consistency comparable to BF16 full-precision training. Meanwhile, extreme INT4 compression allows single-node rollout for ~1TB-scale models, eliminating cross-node communication bottlenecks and significantly improving rollout efficiency‚Äîan open-source reference that balances high performance and low cost.\n\n## Introduction\n\nRecently, the SGLang RL team has made significant progress in RL training stability, efficiency, and application scenarios, including:\n\n- **INT4 QAT End-to-End Training**: We implemented a complete QAT INT4 closed-loop solution from training to inference and provided a detailed [technical recipe](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/slime/int4/readme-en.md), significantly improving rollout efficiency and stability.\n- **Unified Multi-Turn VLM/LLM Training**: We provided an implementation for the VLM multi-turn sampling paradigm [blog](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/slime/vlm-multi-turn/readme.md). Developers only need to write a customized `rollout` function to easily start multi-turn RL for VLM, just like training LLM.\n- **Rollout Router Replay**: We implemented the **[Rollout Router Replay](https://github.com/THUDM/slime/blob/58525eb986c66a271aa31077e17b8afebe704b4f/tests/test_qwen3_30B_A3B_r3.py#L79)** mechanism, significantly improving RL stability for MoE models during RL training.\n- **FP8 End-to-End Training**: We successfully implemented **[end-to-end FP8 training and sampling](https://lmsys.org/blog/2025-11-25-fp8-rl/)** in RL scenarios, further unlocking hardware performance.\n- **Speculative Decoding in RL**: We successfully practiced **[speculative sampling](https://thudm.github.io/slime/advanced/speculative-decoding.html)** in RL scenarios, achieving lossless acceleration for large-scale training.\n\nBuilding on top of these, we went one step further: on the slime framework, we reproduced and deployed an **end-to-end INT4 QAT** solution: **[INT4 Quantization-Aware Training (QAT)](https://github.com/THUDM/slime/blob/58525eb986c66a271aa31077e17b8afebe704b4f/scripts/low_precision/run-kimi-k2-Thinking-int4.sh)**. This solution is deeply inspired by the Kimi team‚Äôs K2-Thinking technical report and its **W4A16 QAT (Quantization-Aware Training)** practice: [**W4A16 QAT (Quantization-Aware Training)**](https://www.zhihu.com/question/1969558404759544488/answer/1970539327902679960). To pay tribute to pioneers and give back to the community, this article **dissects** the technical details of building the full pipeline in an open-source ecosystem, aiming to provide a practical reference that balances stability and performance.\n\n**Key benefits at a glance:**\n\n- **Break the VRAM bottleneck**: With weight compression and low-bit quantization, ~1TB-scale K2-like models can be shrunk to fit on a single H200 (141GB) GPU, avoiding cross-node communication bottlenecks.\n- **Train‚Äìinfer consistency**: Training uses QAT to shape weights into an INT4-friendly distribution; inference uses W4A16 (INT4 weights, BF16 activations). Both rely on BF16 Tensor Cores, achieving train‚Äìinfer consistency comparable to BF16 full precision.\n- **Single-node efficiency doubling**: For very large models, INT4 greatly reduces VRAM and bandwidth pressure, delivering rollout efficiency significantly higher than W8A8 (FP8 weights, FP8 activations).\n\nThis project is jointly completed by the **SGLang RL team, InfiXAI team, Ant Group Asystem \u0026 AQ Infra team, slime team, and RadixArk team**. Related features and recipes have been synced to the [slime](https://github.com/THUDM/slime) and [Miles](https://github.com/radixark/miles) communities. We welcome everyone to try them out and contribute. We are also further challenging ourselves with MXFP8 and NVFP4. We also gratefully acknowledge [Verda Cloud](https://www.linkedin.com/company/verda-cloud/) for compute resource sponsorship.\n\n## Technical Overview\n\n### Overall Pipeline\n\nWe implemented a complete INT4 QAT closed loop from training to inference, as illustrated below:\n\n\u003cdiv align=\"center\"\u003e\n  \u003cimg src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/QAT-INT4-e2e.png\" alt=\"End-to-end QAT INT4 pipeline\" width=\"80%\"  /\u003e\n\u003c/div\u003e\n\nDuring the **QAT training stage**, the training side maintains BF16 master weights, while the forward pass introduces quantization noise via **fake quantization**. ‚ÄúFake‚Äù means we do not truly convert BF16 tensors to low-precision INT4 storage; instead, we keep the floating-point compute path and insert **QDQ (Quantize-DeQuantize)** operations to emulate low-precision arithmetic. Concretely, high-precision weights are first ‚Äúdiscretized‚Äù into INT4 and then immediately restored. Although the physical dtype remains floating point, the value precision is effectively reduced. The discrepancy between the original and restored values introduces quantization error, which is mathematically equivalent to injecting noise into the network, forcing the model to adapt to this precision loss via gradient updates.\n\nFor the backward pass, we use **STE (Straight-Through Estimator)** to bypass the non-differentiability of quantization. The core quantization operator is **rounding**, which is a step function whose derivative is 0 almost everywhere. This would completely block gradient flow and prevent updates to the underlying master weights. STE uses a **‚Äústraight-through gradient estimator‚Äù** strategy: during backprop it defines the derivative of rounding as 1 (i.e., treats it as an identity mapping). This is like building a bridge over a cliff, enabling gradients to pass through the rounding layer and update the high-precision floating-point weights, thus closing the QAT training loop.\n\nIn the **weight conversion stage**, we export the converged BF16 weights and perform **real quantization**, converting them into INT4 formats suitable for inference engines (e.g., Marlin).\n\nIn the **RL rollout stage**, SGLang loads INT4 weights and runs efficient W4A16 inference (INT4 weights √ó BF16 activations). The generated experience data flows back to the first stage for the next RL training iteration, forming a self-consistent closed loop.\n\n### **Key Strategy Choices**\n\nFor quantization format, we follow [Kimi-K2-Thinking](https://huggingface.co/moonshotai/Kimi-K2-Thinking) and choose **INT4 (W4A16)**. Compared to FP4, INT4 has broader support on existing hardware (pre-Blackwell), and the ecosystem already has mature high-performance Marlin kernels. Experiments show that with a 1√ó32 scale granularity, INT4 provides sufficient dynamic range and stable accuracy, and its performance and tooling are well-optimized. As an industry ‚Äúgood enough‚Äù quantization standard, INT4 strikes a rational balance across performance, risk, and maintenance cost. That said, we also plan to explore FP4 RL on NVIDIA Blackwell GPUs in the future.\n\nFor training, we use the classic combination of **fake quantization + STE**. By maintaining BF16 master weights, simulating quantization noise in the forward pass, and passing gradients straight through in the backward pass, we maximize convergence and stability in low-precision training.\n\n## Training Side: Retrofitting Fake Quantization into Megatron-LM\n\n### Implementing Fake Quantization and STE (Straight-Through Estimator)\n\n\u003cdiv align=\"center\"\u003e\n  \u003cimg src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/fake-quantize-STE.png\" alt=\"Training-side Fake Quantization \u0026 STE\" width=\"80%\"  /\u003e\n\u003c/div\u003e\n\nThe core goal of this stage is to simulate quantization error on-the-fly during training, forcing the model to ‚Äúlearn‚Äù to adapt to low-precision representations. We therefore adopt **fake quantization**: while weights are stored and updated in BF16, they are temporarily mapped into the INT4 precision range in the forward pass.\n\nImplementation-wise, the core logic lives in the `_FakeInt4QuantizationSTE` class in `megatron/core/extensions/transformer_engine.py`. It performs dynamic quantization based on per-group max absolute value, emulating INT4‚Äôs `[-7, 7]` range and clipping, but still computes in BF16 and only injects quantization error. For the crucial backward pass, we introduce **STE**, ensuring gradients pass through the quantization layer unchanged to update master weights, keeping training continuous.\n\n### Fake Quantization Ablations\n\nTo validate the necessity of QAT and study the impact of train‚Äìinfer precision mismatch, we designed ablations for two asymmetric scenarios:\n\n- **QAT INT4 training enabled, BF16 rollout**\n- **QAT training disabled, direct INT4 rollout**\n\nWe measure train‚Äìinfer inconsistency using the absolute difference in log probabilities (Logprob Abs Diff).\n\n\u003cimg src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/moonlight-1.png\" alt=\"Rollout BF16, training-side comparison of QAT INT4 effect\" width=\"45%\" /\u003e \u003cimg src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/moonlight-2.png\" alt=\"Rollout INT4 weight-only, training-side comparison of QAT INT4 effect\" width=\"45%\" /\u003e\n\n**The left plot shows ‚ÄúQAT INT4 training + BF16 rollout‚Äù** (the **red curve**). Interestingly, even with high-precision BF16 inference, the error remains significantly higher. This is because QAT has already adapted weights to INT4 quantization noise via ‚Äúcompensation‚Äù; if we remove quantization at inference, that compensation becomes a perturbation, causing **distribution shift**.\n\n**The right plot shows ‚Äúno QAT training + direct INT4 rollout‚Äù** (the **red curve**), corresponding to a conventional post-training quantization (PTQ) setup. Since the model never saw quantization noise during training, compressing weights to INT4 causes severe information loss and shifts feature distributions relative to training, resulting in errors that oscillate and increase with training steps.\n\n**Conclusion:** These results strongly indicate that **training-side fake quantization and inference-side real quantization must be enabled together**. Only when the simulated noise during training is **strictly aligned** with the true quantization during inference can we suppress train‚Äìinfer mismatch, avoid distribution shift, and keep errors near baseline‚Äîthus truly closing the loop for end-to-end low-precision RL training.\n\n## Weight Update Stage\n\n### Weight Flow and Dynamic Format Adaptation\n\n\u003cdiv align=\"center\"\u003e\n  \u003cimg src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/marlin_optimization.jpg\" alt=\"SGLang-side weight handling pipeline\" width=\"80%\"  /\u003e\n\u003c/div\u003e\n\nTo reuse existing inference-side optimizations in SGLang, we directly adopted its built-in **Marlin kernel** as the INT4 inference backend. However, in practice we encountered a notable ‚Äúformat gap‚Äù: QAT training outputs weights in standard formats (similar to Hugging Face), while SGLang‚Äôs Marlin kernel requires weights to be specially **packed** and **permuted** so that the kernel can read them efficiently.\n\nGiven that RL training requires frequent weight updates, we must solve format compatibility. We therefore designed a reverse `restore_weights_before_loading` **safety mechanism**. Using cached `_original_shapes` metadata, it restores (resizes) the in-memory Marlin weight format back to original shapes **before** any weight update happens. This prevents runtime errors due to shape mismatches and enables smooth switching between standard weight formats and Marlin formats. We also added a system-level `post_process_weights` API to allow the control plane to explicitly trigger this process according to the training schedule.\n\nTo address post-load format adaptation, we implemented a **dynamic weight management mechanism** in `compressed_tensors_moe.py`. After weight loading finishes, the system automatically runs `process_weights_after_loading`, calling operators like `gptq_marlin_moe_repack` and `marlin_moe_permute_scales` to convert standard weights into highly optimized Marlin formats in memory, maximizing memory-access and compute efficiency for inference.\n\n### Quantization During Weight Updates\n\n\u003cdiv align=\"center\"\u003e\n  \u003cimg src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/weights-update.jpg\" alt=\"Weight update\" width=\"80%\"  /\u003e\n\u003c/div\u003e\n\nNow comes the core **real quantization** step. Unlike training-time fake quantization, this step irreversibly compresses precision via `int4_block_quantize`: with a configured group size, we compute per-group scales and map high-precision floats into the INT4 integer domain `[-7, 7]`.\n\nTo maximize VRAM efficiency, we then do **bit packing**. Since PyTorch lacks a native INT4 dtype, we implement `pack_int4_to_int32` using bitwise tricks to tightly pack 8 INT4 values into one INT32 integer (i.e., `8 √ó 4 bits = 32 bits`). Finally, these packed weights together with scales are passed to the inference engine, completing the conversion from ‚Äútraining format‚Äù to ‚Äúinference format‚Äù.\n\n## Inference Stage\n\n\u003cdiv align=\"center\"\u003e\n  \u003cimg src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/sglang-w4a16.png\" alt=\"SGLang W4A16 inference\" width=\"80%\"  /\u003e\n\u003c/div\u003e\n\n**Minimal packing and near-zero-overhead unpacking**\n\nDuring RL rollout, we directly reuse SGLang‚Äôs mature W4A16 quantization solution. SGLang stores weights in a compact INT4 format by packing two 4-bit values into one byte, saving **75%** memory compared to BF16. At inference time, Triton kernels unpack efficiently using bit operations (`\u003e\u003e 4` and `\u0026 0xF`). Thanks to overlap between compute and IO, this unpacking is almost zero-overhead.\n\n**Deep fusion for MoE operators**\n\n- **Memory optimization**: SGLang introduces a dynamic `moe_align_block_size` that chooses `block_size` based on current token counts and expert distribution, grouping and aligning tokens for the same expert to improve bandwidth utilization.\n- **Compute fusion**: Besides integrating a high-performance [**Marlin INT4**](https://github.com/IST-DASLab/marlin) implementation, SGLang also fuses the gating part into a single high-performance kernel to avoid repeated kernel launches and intermediate reads/writes. This INT4 inference scheme is compatible with mainstream formats such as GPTQ and AWQ, and supports both symmetric and asymmetric modes.\n\n## INT4 QAT RL Results\n\n### Training Results\n\n- **Training side**\n\n\u003cdiv align=\"center\"\u003e\n  \u003cimg src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/qwen3-235b-raw-reward.png\" alt=\"Qwen3-235B-A22B Raw-Reward comparison\" width=\"45%\"  /\u003e \n  \u003cimg src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/kimi-k2-raw-reward.png\" alt=\"Kimi-K2-Thinking Raw-Reward comparison\" width=\"45%\"  /\u003e\n\u003c/div\u003e\n\nThe plots above show training performance on the dapo-math-17k dataset for Qwen3-235B-A22B and Kimi-K2-Thinking under the slime framework. Compared with **‚ÄúBF16 train‚ÄìBF16 infer‚Äù** and **‚ÄúBF16 train‚ÄìFP8 infer‚Äù**, the **‚ÄúBF16 train‚ÄìINT4 infer‚Äù** setup still achieves steady Raw-Reward growth with a trend largely consistent with the former two, demonstrating the effectiveness of this approach.\n\n- **Evaluation side**\n\n\u003cdiv align=\"center\"\u003e\n  \u003cimg src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/qwen3-235b-AIME.png\" alt=\"Qwen3-235B-A22B AIME evaluation comparison\" width=\"45%\"  /\u003e \n  \u003cimg src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/kimi-k2-AIME.png\" alt=\"Kimi-K2-Thinking AIME evaluation comparison\" width=\"45%\"  /\u003e\n\u003c/div\u003e\n\nTo evaluate model capability more rigorously, we run an evaluation on the aime-2024 benchmark every 10 training steps. The plots show the scoring trajectories of Qwen3-235B-A22B and Kimi-K2-Thinking under different RL training configurations.\n\nThe experiments indicate that the **‚ÄúBF16 train‚ÄìINT4 infer‚Äù** scheme not only exhibits a stable upward trend in evaluation scores, but also closely overlaps with **‚ÄúBF16 train‚ÄìBF16 infer‚Äù** and **‚ÄúBF16 train‚ÄìFP8 infer‚Äù** in both slope and peak score. This strong alignment suggests that low-bit quantization does not harm core representational capacity, enabling large compute savings while preserving (or matching) full-precision generalization performance.\n\n### Train‚ÄìInfer Gap\n\n\u003cdiv align=\"center\"\u003e\n  \u003cimg src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/qwen3-30b-train-infer-gap.png\" alt=\"Qwen3-30B-A3B train‚Äìinfer gap comparison\" width=\"45%\"  /\u003e \n  \u003cimg src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/qwen3-235b-train-infer-gap.png\" alt=\"Qwen3-235B-A22B train‚Äìinfer gap comparison\" width=\"45%\"  /\u003e\n\u003c/div\u003e\n\nTo visualize effectiveness, we validated QAT RL training on Qwen3-30B and Qwen3-235B. The Y-axis shows the absolute logprob difference between training-side and inference-side outputs; lower values mean stronger consistency. Results show that INT4 (**green dashed**) almost overlaps with the BF16 baseline (**red solid**), and is significantly lower than FP8 (**blue dashed**). This confirms that INT4 QAT can effectively avoid the accuracy loss in the **‚ÄúBF16 train‚ÄìFP8 infer‚Äù** mode and achieve train‚Äìinfer behavior indistinguishable from full precision.\n\n**We hypothesize two reasons behind this consistency:**\n\n- **Truncation error suppression**: Training-side fake quantization constrains weights to the INT4 range. This constraint [can effectively reduce floating-point rounding errors caused by non-deterministic accumulation orders in parallel matmul (i.e., the ‚Äúadding a small number to a large number‚Äù precision loss).](https://www.zhihu.com/question/1969558404759544488/answer/1970539327902679960)\n- **High-precision compute**: Inference uses W4A16 and relies on **BF16 Tensor Cores** throughout, keeping compute precision highly aligned with training.\n\n### Rollout Speedup\n\n\u003cdiv align=\"center\"\u003e\n  \u003cimg src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/qwen3-235b-rollout-performance.png\" alt=\"Qwen3-235B-A22B rollout performance comparison\" width=\"45%\"  /\u003e\n  \u003cimg src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/int4/figs/kimi-k2-rollout-performance.png\" alt=\"Kimi-K2-Thinking rollout performance comparison\" width=\"45%\"  /\u003e\n\u003c/div\u003e\n\nFrom the Qwen3-235B rollout performance plot, we can see that INT4 (**green dash-dot**) and FP8 (**blue dashed**) both significantly speed up compared to the BF16 baseline (**red solid**), but the gap between INT4 and FP8 is not huge. This is largely limited by current hardware: NVIDIA H-series GPUs do not have native INT4 Tensor Cores. W4A16 essentially still uses BF16 Tensor Cores for compute; while it greatly reduces memory bandwidth pressure, it cannot gain the compute uplift of native FP8 Tensor Cores as W8A8 does. Therefore, INT4 only shows a slight advantage in per-step latency and remains in roughly the same performance tier as FP8.\n\nFor Kimi-K2-Thinking rollout performance, first look at the **communication bottleneck** in the two-node scenario: FP8 (**red line**) and INT4 (**blue line**) are similar, because H-series GPUs lack native INT4 compute units and INT4 cannot speed up compute, so overall performance is still limited by cross-node bandwidth.\n\nHowever, the single-node result (the **green line**) reveals INT4‚Äôs **true value‚ÄîVRAM compression**. By halving model size, we can load ~1TB-scale models fully into a single machine‚Äôs VRAM, eliminating expensive cross-node communication and greatly reducing rollout time. This strongly demonstrates that under current hardware, the main benefit of INT4 QAT is enabling efficient single-node rollouts via VRAM compression.\n\n## Summary and Future Work\n\nBy reproducing the approach in an open-source framework, we validated the effectiveness of the INT4 QAT scheme proposed by the Kimi team:\n\n- **Accuracy reproduction**: In slime reproductions, we observed the same INT4 QAT accuracy advantages, matching the BF16 baseline.\n- **Efficiency improvement**: Rollout throughput improved significantly, validating the value of low-bit quantization in RL.\n\nFuture work:\n\n- **Training-side efficiency optimization**: Today, adding QAT fake quantization introduces extra compute overhead during training, making it noticeably slower than BF16. This partially offsets the end-to-end gains from faster rollout. We plan to propose a new optimization to address this training-side bottleneck and accelerate the full pipeline.\n- **Inference-side FP4**: As NVIDIA Blackwell becomes more widely available, we will actively explore FP4 precision for RL training and inference to further tap into hardware potential.\n\nslime‚Äôs attempt at INT4 QAT not only demonstrates the feasibility of reproducing industrial state-of-the-art techniques in an open-source ecosystem, but also opens a new path for low-cost training at extreme scale. We hope this solution helps more developers deeply understand QAT and promote its practical adoption in RL.\n\n## Acknowledgements\n\nSGLang RL Team: Ji Li, Yefei Chen, Xi Chen, BBuf, Chenyang Zhao\n\nInfiXAI Team: Mingfa Feng, Congkai Xie, Shuo Cai\n\nAnt Group Asystem \u0026 AQ Infra Team: Yanan Gao, Zhiling Ye, Yuan Wang, Xingliang Shi\n\nslime Team: Zilin Zhu, Lei Li, Haisha Zhao","slug":"2026-01-26-int4-qat"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2026-01-26-int4-qat"},"buildId":"yZXPcMyfUCcN8SYdOM9NB","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>