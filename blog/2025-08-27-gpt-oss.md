---
title: "SGLang for gpt-oss: From Day 0 Support to Enhanced Performance"
author: "SGLang Team"
date: "August 27, 2025"
previewImg: /images/blog/gpt_oss/gpt_oss_preview.png
---

<span style="color: red; font-weight: bold;">
All the red text should be re-checked carefully.
</span>

We are excited to announce a major update for SGLang, focusing on deep performance optimizations and new features for the recently released openai/gpt-oss-120b model. **While we had support from day zero, we took the last few weeks to enhance our engine to ensure you get the best possible performance.**

This post highlights our latest achievements: a significant performance improvement for gpt-oss with up to **2.1x** higher throughput on prefill and **2.25x** higher throughput on decode, out-of-the-box support for NVIDIA Blackwell & Hopper and AMD MI350 GPUs, and enhanced APIs to power complex agentic applicationsâ€”all while maintaining the model's high accuracy.

All changes are now available in our main branch.

### Get Started with SGLang

```bash
pip install "sglang[all]>=0.5.2"
python3 -m sglang.launch_server --model-path openai/gpt-oss-120b --tp 4
```

<div style="color: red; font-weight: bold;">
Add all necessary commands into the document.
</div>

For detailed instructions on environment setup and how to gain the best performance, please see our guide [here](https://github.com/sgl-project/awesome-sglang/tree/main/gpt-oss).

## By the Numbers: Comprehensive Benchmark Results ðŸ“Š

To show the impact of our optimizations, we benchmarked SGLang across a range of hardware configurations. For all the results, the reproduction command can be found [here](https://github.com/sgl-project/sglang/tree/main/benchmark/gpt_oss).

##### Low-Latency Performance (Batch Size = 1)

For latency-sensitive applications, we measured single-batch decode throughput across NVIDIA and AMD GPUs, showcasing excellent performance.

| Hardware / Precision | NVIDIA B200  | NVIDIA H100  | AMD MI350    |
| -------------------- | ------------ | ------------ | ------------ |
| MXFP4                | 416.02 tok/s | 318.53 tok/s | 200.84 tok/s |
| BF16                 | 315.63 tok/s | 293.12 tok/s | 220.06 tok/s |

<span style="color: grey; font-size: 12px;">
B200 was tested with TP=4, H100 with TP=8 and triton attention, and MI350 with TP=8 and triton backend.
</span>

##### High-Throughput Performance (Batch Size = 32)

For high-throughput applications, SGLang delivers significant performance gains over our initial Day 0 support and have shown great performance on both prefill and decode on different hardwares.

![combined_prefill_performance.png](/images/blog/gpt_oss/combined_prefill_performance.png)

![combined_decode_performance.png](/images/blog/gpt_oss/combined_decode_performance.png)

## Performance Deep Dive ðŸš€

Our performance gains come from several key optimizations at the kernel level:

- **FlashInfer Kernels for Blackwell**: To unlock peak performance for gpt-oss on Blackwell GPUs, we integrated highly optimized kernels from FlashInfer. This accelerates core components, including multi-head attention and Mixture of Experts (MoE) layers, on the new hardware.
- **FlashAttention-3 for Hopper**: We modified the FlashAttention-3 kernels to support attention sinks, providing a significant speedup for inference on Hopper GPUs.
- **Kernel Fusion and Reduction**: We performed several low-level fusions to reduce overhead. This includes fusing the RMS norm with all-reduce, merging the set KV buffer operation into RoPE, and fusing hidden states padding into quantization. We also removed unnecessary kernels, enabled [PDL](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programmatic-dependent-launch-and-synchronization) for some kernels, and reduced CPU overhead for greater efficiency.

## Accuracy Alignment with Official Report ðŸŽ¯

We validated our optimized gpt-oss implementation against the GPQA benchmark and confirmed that our results align closely with the official model card, ensuring that these speedups do not compromise the model's reasoning capabilities.

| Reasoning Effort | SGLang | [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html#accuracy-evaluation-panels) | [Official](https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf) |
| ---------------- | ------ | ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------ |
| Low              | 65.6   | 65.3                                                                                                   | 67.1                                                                                                   |
| Medium           | 71.3   | 72.4                                                                                                   | 73.1                                                                                                   |
| High             | 79.8   | 79.4                                                                                                   | 80.1                                                                                                   |

## Speculative Decoding Support ðŸ¦…

<div style="color: red; font-weight: bold;">
Ke write this
Put eagle here
</div>

## Powering Agentic Applications ðŸ¤–

<div style="color: red; font-weight: bold;">
Need more check from @Chang@Lianmin@Xingyuan
</div>

To better enable agentic workflows, SGLang now offers [OpenAI Reponse API support](https://docs.sglang.ai/basic_usage/gpt_oss.html#responses-api) and [native chat completion support](https://docs.sglang.ai/advanced_features/function_calling.html#). Here is an example of how to build a simple web search agent with SGLang.

Launch the server:

```bash
export EXA_API_KEY=YOUR_EXA_KEY
python3 -m sglang.launch_server --port 30000 --model-path openai/gpt-oss-120b --tp 4 --tool-server demo 
```

Use Response API to build a web search agent:

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:30000/v1",
    api_key="EMPTY"
)
response = client.responses.create(
    model="openai/gpt-oss-120b",
    tools=[{"type": "web_search_preview"}],
    input="What does SGLang update today?"
)

print(response.output_text)
```

## What's Next? ðŸ”®

<div style="color: red; font-weight: bold;">
Need more check from @Lianmin
</div>

We are continuously working to push the boundaries of LLM inference. Our future roadmap includes exploring SWA (Sliding Window Attention) optimizations and speculative decoding to further enhance performance.

We invite you to try out the latest version of SGLang and share your feedback. Thank you for being part of our community!