<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Blog | LMSYS Org</title><meta name="title" content="Blog | LMSYS Org"/><meta property="og:title" content="Blog | LMSYS Org"/><meta name="twitter:title" content="Blog | LMSYS Org"/><meta name="description" content="LMSYS Org, Large Model Systems Organization, is an organization missioned to democratize the technologies underlying large models and their system infrastructures."/><meta property="og:description" content="LMSYS Org, Large Model Systems Organization, is an organization missioned to democratize the technologies underlying large models and their system infrastructures."/><meta name="twitter:description" content="LMSYS Org, Large Model Systems Organization, is an organization missioned to democratize the technologies underlying large models and their system infrastructures."/><meta property="og:image" content="https://lmsys.org/social.png"/><meta name="twitter:image" content="https://lmsys.org/social.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog"/><meta name="twitter:url" content="https://lmsys.org/blog"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="19"/><link rel="stylesheet" href="/fonts/load.css"/><link rel="preload" href="/_next/static/css/a846616179a41020.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a846616179a41020.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-1e08157da55264e1.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/136-86bea74fb8aba3d0.js" defer=""></script><script src="/_next/static/chunks/pages/blog-67b668fa93f0d877.js" defer=""></script><script src="/_next/static/P243ogEyw3hjk50Uz-0Nu/_buildManifest.js" defer=""></script><script src="/_next/static/P243ogEyw3hjk50Uz-0Nu/_ssgManifest.js" defer=""></script><script src="/_next/static/P243ogEyw3hjk50Uz-0Nu/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://arena.lmsys.org" target="_blank" rel="noopener noreferrer">Chatbot Arena</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/h6kCZb72G7" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://arena.lmsys.org" target="_blank" rel="noopener noreferrer">Chatbot Arena</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/h6kCZb72G7" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center pt-16 md:pt-5"><div class="container px-5"><h1 class="text-8xl md:text-8xl font-bold pb-2">BLOG</h1><div class="text-2xl pb-4">Latest updates and releases by LMSYS Org are announced through our blogpost series.</div><hr class="mb-5 md:hidden"/><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky text-paper border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Chatbot Arena Leaderboard Updates (Week 4)</p><p class="text-base pt-2 pb-2">by: <!-- -->LMSYS Org<!-- -->, <!-- -->May 25, 2023<!-- --></p><hr/><p class="text-base pt-2 pb-1">A new Elo rating leaderboard based on the 28.7K anonymous voting data collected in the wild between April 24 and May 22, 2023 is released in Table 1 below. In this update, we are excited to welcome the following chatbots joining the Arena:

Google PaLM 2, chat-tuned with the code name chat-bison@001...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky text-paper border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Chatbot Arena Leaderboard Updates (Week 2)</p><p class="text-base pt-2 pb-2">by: <!-- -->LMSYS Org<!-- -->, <!-- -->May 10, 2023<!-- --></p><hr/><p class="text-base pt-2 pb-1">We release an updated leaderboard with more models and new data we collected last week, after the announcement of the anonymous Chatbot Arena. We are actively iterating on the design of the arena and leaderboard scores.
In this update, we have added 4 new yet strong players into the Arena, including...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky text-paper border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings</p><p class="text-base pt-2 pb-2">by: <!-- -->Lianmin Zheng*, Ying Sheng*, Wei-Lin Chiang, Hao Zhang, Joseph E. Gonzalez, Ion Stoica<!-- -->, <!-- -->May 3, 2023<!-- --></p><hr/><p class="text-base pt-2 pb-1">We present Chatbot Arena, a benchmark platform for large language models (LLMs) that features anonymous, randomized battles in a crowdsourced manner. In this blog post, we are releasing our initial results and a leaderboard based on the Elo rating system, which is a widely-used rating system in ches...</p></div></div><div class="border mb-5 hover:bg-paper hover:text-sky transition-colors cursor-pointer bg-sky text-paper border-paper flex flex-col lg:flex-row items-stretch shadow-lg shadow-neutral-800/20"><div class="basis-2/5 team-wrap"></div><div class="p-5 basis-3/5"><p class="text-3xl">Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality</p><p class="text-base pt-2 pb-2">by: <!-- -->The Vicuna Team<!-- -->, <!-- -->March 30, 2023<!-- --></p><hr/><p class="text-base pt-2 pb-1">We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%* quality of OpenAI ChatGPT and Google Bard while outperforming other models like LL...</p></div></div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"slug":"2023-05-25-leaderboard","frontmatter":{"title":"Chatbot Arena Leaderboard Updates (Week 4)","author":"LMSYS Org","date":"May 25, 2023","previewImg":"/images/blog/leaderboard_week4/leaderboard_cover.png"},"content":"\nA new Elo rating leaderboard based on the 28.7K anonymous voting data collected **in the wild** between April 24 and May 22, 2023 is released in Table 1 below. In this update, we are excited to welcome the following chatbots joining the Arena:\n\n1. Google PaLM 2, chat-tuned with the code name [chat-bison@001](https://cloud.google.com/vertex-ai/docs/release-notes#May_10_2023) on Google Cloud Vertex AI\n2. Anthropic Claude-instant-v1\n3. MosaicML MPT-7B-chat\n4. Vicuna-7B\n\nWe provide the [voting data](https://drive.google.com/file/d/1HPlwuwm3ptQWlx4fio-psIkIETTbtqHQ/view?usp=share_link) and the [Google Colab notebook](https://colab.research.google.com/drive/17L9uCiAivzWfzOxo2Tb9RMauT7vS6nVU?usp=sharing) to analyze this data, including the computation of the Elo ratings.\n\n\n\u003cstyle\u003e\nth {text-align: left}\ntd {text-align: left}\n\u003c/style\u003e\n\n\u003cbr\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 1. Elo ratings of LLMs (Timeframe: April 24 - May 22, 2023)\u003c/p\u003e\n\u003ctable style=\"display: flex; justify-content: center;\" align=\"left\" \u003e\n\u003ctbody\u003e\n\u003ctr\u003e \u003cth\u003eRank\u003c/th\u003e \u003cth\u003eModel\u003c/th\u003e \u003cth\u003eElo Rating\u003c/th\u003e \u003cth\u003eDescription\u003c/th\u003e \u003cth\u003eLicense\u003c/th\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e1\u003c/td\u003e \u003ctd\u003eü•á \u003ca href=\"https://chat.openai.com/\" target=\"_blank\"\u003eGPT-4\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1225\u003c/td\u003e \u003ctd\u003eChatGPT-4 by OpenAI\u003c/td\u003e \u003ctd\u003eProprietary\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e2\u003c/td\u003e \u003ctd\u003eü•à \u003ca href=\"https://www.anthropic.com/index/introducing-claude\" target=\"_blank\"\u003eClaude-v1\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1195\u003c/td\u003e \u003ctd\u003eClaude by Anthropic\u003c/td\u003e \u003ctd\u003eProprietary\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e3\u003c/td\u003e \u003ctd\u003eü•â \u003ca href=\"https://www.anthropic.com/index/introducing-claude\" target=\"_blank\"\u003eClaude-instant-v1\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1153\u003c/td\u003e \u003ctd\u003eLighter, less expensive, and much faster version of Claude\u003c/td\u003e \u003ctd\u003eProprietary\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e4\u003c/td\u003e \u003ctd\u003e \u003ca href=\"https://chat.openai.com/\" target=\"_blank\"\u003eGPT-3.5-turbo\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1143\u003c/td\u003e \u003ctd\u003eChatGPT-3.5 by OpenAI\u003c/td\u003e  \u003ctd\u003eProprietary\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e5\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://lmsys.org/blog/2023-03-30-vicuna/\" target=\"_blank\"\u003eVicuna-13B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1054\u003c/td\u003e \u003ctd\u003ea chat assistant fine-tuned from LLaMA on user-shared conversations by LMSYS\u003c/td\u003e \u003ctd\u003eWeights available; Non-commercial\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e6\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://cloud.google.com/vertex-ai/docs/release-notes#May_10_2023\" target=\"_blank\"\u003ePaLM 2\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1042\u003c/td\u003e \u003ctd\u003ePaLM 2 tuned for chat (chat-bison@001 on Google Vertex AI). The PaLM 2 model family is powering Bard.\u003c/td\u003e \u003ctd\u003eProprietary\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e7\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://huggingface.co/lmsys/vicuna-7b-delta-v1.1\" target=\"_blank\"\u003eVicuna-7B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1007\u003c/td\u003e \u003ctd\u003ea chat assistant fine-tuned from LLaMA on user-shared conversations by LMSYS\u003c/td\u003e \u003ctd\u003eWeights available; Non-commercial\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e8\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://bair.berkeley.edu/blog/2023/04/03/koala\" target=\"_blank\"\u003eKoala-13B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e980\u003c/td\u003e \u003ctd\u003ea dialogue model for academic research by BAIR\u003c/td\u003e \u003ctd\u003eWeights available; Non-commercial\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e9\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://www.mosaicml.com/blog/mpt-7b\" target=\"_blank\"\u003empt-7b-chat\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e952\u003c/td\u003e \u003ctd\u003ea chatbot fine-tuned from MPT-7B by MosaicML\u003c/td\u003e \u003ctd\u003eApache 2.0\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e10\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://huggingface.co/lmsys/fastchat-t5-3b-v1.0\" target=\"_blank\"\u003eFastChat-T5-3B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e941\u003c/td\u003e \u003ctd\u003ea chat assistant fine-tuned from FLAN-T5 by LMSYS\u003c/td\u003e \u003ctd\u003eApache 2.0\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e11\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\" target=\"_blank\"\u003eAlpaca-13B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e937\u003c/td\u003e \u003ctd\u003ea model fine-tuned from LLaMA on instruction-following demonstrations by Stanford\u003c/td\u003e  \u003ctd\u003eWeights available; Non-commercial\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e12\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://huggingface.co/BlinkDL/rwkv-4-raven\" target=\"_blank\"\u003eRWKV-4-Raven-14B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e928\u003c/td\u003e \u003ctd\u003ean RNN with transformer-level LLM performance\u003c/td\u003e \u003ctd\u003eApache 2.0\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e13\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://open-assistant.io\" target=\"_blank\"\u003eOasst-Pythia-12B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e921\u003c/td\u003e \u003ctd\u003ean Open Assistant for everyone by LAION\u003c/td\u003e \u003ctd\u003eApache 2.0\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e14\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://chatglm.cn/blog\" target=\"_blank\"\u003eChatGLM-6B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e921\u003c/td\u003e \u003ctd\u003ean open bilingual dialogue language model by Tsinghua University\u003c/td\u003e \u003ctd\u003eWeights available; Non-commercial\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e15\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://github.com/stability-AI/stableLM\" target=\"_blank\"\u003eStableLM-Tuned-Alpha-7B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e882\u003c/td\u003e \u003ctd\u003eStability AI language models\u003c/td\u003e  \u003ctd\u003eCC-BY-NC-SA-4.0\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e16\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\" target=\"_blank\"\u003eDolly-V2-12B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e866\u003c/td\u003e \u003ctd\u003ean instruction-tuned open large language model by Databricks\u003c/td\u003e \u003ctd\u003eMIT\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e17\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://arxiv.org/abs/2302.13971\" target=\"_blank\"\u003eLLaMA-13B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e854\u003c/td\u003e \u003ctd\u003eopen and efficient foundation language models by Meta\u003c/td\u003e \u003ctd\u003eWeights available; Non-commercial\u003c/td\u003e \u003c/tr\u003e\n\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u0026shy;\n\n\n**Win Fraction Matrix**  \nThe win fraction matrix of all model pairs is shown in Figure 1.\n\u003cimg src=\"/images/blog/leaderboard_week4/win_fraction_matrix.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 1: Fraction of Model A Wins for All Non-tied A vs. B Battles.\u003c/p\u003e\n\n\n## Overview\n\n### Google PaLM 2\n\nGoogle's PaLM 2 is one of the most significant models announced since our last leaderboard update. We added the PaLM 2 Chat to the Chatbot Arena via the [Google Cloud Vertex AI API](https://cloud.google.com/vertex-ai/docs/release-notes#May_10_2023). The model is chat-tuned under the code name *chat-bison@001*.\n\nIn the past two weeks, PaLM 2 has competed for nearly 1400 non-tie anonymous battles with the other 16 chatbots, currently ranked 6th on the leaderboard. It ranks above all other open-source chatbots, except for Vicuna-13B, whose Elo is 12 scores higher than PaLM 2 (Vicuna 1054 vs. PaLM 2 1042) which in terms of ELO rating is nearly a virtual tie. We noted the following interesting results from PaLM 2's Arena data.\n\nPaLM 2 is better when playing against the top 4 players, i.e., GPT-4, Claude-v1, ChatGPT, Claude-instant-v1, and it also wins 53% of the plays with Vicuna, but worse when playing against weaker players. This can be seen in Figure 1 which shows the win fraction matrix. Among all battles PaLM 2 has participated in, 21.64% were lost to a chatbot that is not one of GPT-4, Claude-v1, GPT-3.5-turbo, Claude-instant-v1. For reference, another proprietary model GPT-3.5-turbo only loses 12.8% of battles to those chatbots.\n\nIn short, we find that the current PaLM 2 version available at Google Cloud Vertex API have the following deficiencies when compared to other models we have evaluated:\n\n1. PaLM 2 seems more strongly regulated than other models which impacts its ability to answer some questions.\n2. The currently offered PaLM 2 has limited multilingual abilities.\n3. The currently offered PaLM 2 has unsatisfied reasoning capabilities.\n\n**PaLM 2 is more strongly regulated**\n\nPaLM 2 seems to be more strongly regulated than other models. In many user conversations, when the users ask questions that PaLM 2 is uncertain or uncomfortable giving an answer to, PaLM 2 is more likely to abstain from responding than other models. \n\nBased on a rough estimate, among all pairwise battles, PaLM 2 has lost 20.92% of the battles by refusing to answer, and it has lost 30.82% of the battles to chatbots not belonging to one of the top four (GPT-4, Claude-v1, ChatGPT, Claude-instant-v1) by refusing to answer. \n\nThis partially explains why PaLM 2 frequently loses plays to weaker chatbots on the leaderboard. This also highlights a flaw in the chatbot arena methodology, as casual users are more likely to penalize abstention over subtly inaccurate responses. Below we provide several failure cases illustrating how PaLM loses plays to weaker chatbots because it refuses to answer the question.\n\n\nWe also noticed that, sometimes, it is hard to clearly specify the boundary for LLM regulation. In the offered PaLM 2 versions, we see several undesired tendencies: \n - PaLM 2 refuses many roleplay questions, even if the users asked it to emulate a Linux terminal or a programming language interpreter.\n - Sometimes PaLM 2 refuses to answer easy and non-controversial factual questions. \n\nSeveral examples are shown below:\n\n\u003cimg src=\"/images/blog/leaderboard_week4/PaLM2_refusal_1.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\n\u003cimg src=\"/images/blog/leaderboard_week4/PaLM2_refusal_2.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 2: Example questions that PaLM 2 refuses to answer.\u003c/p\u003e\n\n\n**Limited multilingual abilities**\n\nWe do not see strong multilingual abilities from PaLM 2 with the currently offered public API chat-bison@001 at Google Vertex API. PaLM 2 tends to not answer non-English questions, including questions written in popular languages such as Chinese, Spanish, and Hebrew. We were unable to reproduce several multilingual examples demonstrated in the PaLM 2 technical report using the current PaLM 2 versions. We are waiting for Google to gradually release the latest version of PaLM 2. \n\nWe also calculate the Elo ratings of all models when only considering English and only considering non-English conversations, respectively, illustrated in Figure 3. The results confirm the observations ‚Äì on the non-English leaderboard, PaLM 2 ranks 16th.\n\n\u003cimg src=\"/images/blog/leaderboard_week4/language_leaderboard.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 3: The English-only and non-English leaderboards.\u003c/p\u003e\n\n\n**PaLM 2's reasoning ability is unsatisfied**\n\nWe also observe the offered PaLM 2 version do not demonstrate strong reasoning capabilities. On one hand, it seems to detect if the question is in plain text, and tends to refuse many questions not in plain text, such as those in programming languages, debugging, and code interpretation. On the other hand, we see PaLM 2 didn‚Äôt perform well on some entry-level reasoning tasks when compared against other chatbots. See several examples in Figure 4.\n\n\u003cimg src=\"/images/blog/leaderboard_week4/PaLM2_reasoning_1.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\n\u003cimg src=\"/images/blog/leaderboard_week4/PaLM2_reasoning_2.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 4: Examples where PaLM 2 fails on simple reasoning tasks.\u003c/p\u003e\n\n\n**Elo ratings after removing non-English and refusal conversations**\n\nWe remove all non-English conversations and all conversations for which PaLM 2 didn‚Äôt provide an answer and calculate the Elo ratings of each model with the filtered data. This rating represents a hypothetical upper bound of PaLM 2's Elo in the Arena. See Figure 5 below.\n\n\u003cimg src=\"/images/blog/leaderboard_week4/english_non_refusal_leaderboard.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 500px;\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 5: The leaderboard after removing PaLM 2's non-English and refusal conversations.\u003c/p\u003e\n\n### Smaller Models Are Competitive\n\nWe observe several smaller models, including vicuna-7B and mpt-7b-chat, have achieved high ratings on the leaderboard. These smaller models perform favorably when compared against larger models with doubled parameters. \n\nWe speculate that high-quality pre-training and fine-tuning datasets are more critical than model size. However, it is possible that larger models would still perform better with more complex reasoning tasks or answering more subtle questions (e.g., Trivia).\nHence, curating high-quality datasets in both pretraining and finetuning stages seems to be a key approach to reducing model sizes while keeping model quality high.\n\n\n### Claude-v1 and Claude-instant-v1\nClaude-instant-v1 is a low-cost, faster alternative to Claude-v1 offered by Anthropic. If benchmarked in the wild in the arena, we observe that Claude-instant is close to GPT-3.5-turbo (1153 vs. 1143). The rating gap between Claude and Claude-instant seems smaller than that between GPT-4 and GPT-3.5-turbo. Claude-instant has a context length of 9K, is charged at a price of 0.00163/1K prompt token and 0.00551/1K completion token, compared to its OpenAI opponent product ‚Äì GPT-3.5-turbo ‚Äì with a context length of 4K and a uniform price of 0.002/1K token (regardless of prompt or completion).\n\n### Limitations of the ‚ÄúIn-the-wild‚Äù Evaluation\nHowever, we want to point out a few facts about the current chatbot Arena and leaderboard. The current Arena is designed to benchmark LLM-based chatbots **\"in the wild\"**. That means, the voting data provided by our Arena users and the prompts-answers generated during the voting process reflect how the chatbots perform in normal human-chatbot interactions. This might not align with many benchmarking results in the LLM research literature, which tends to characterize long-tail abilities like zero-shot, complex reasoning, etc. Hence, the current chatbot arena has limitations in clearly reflecting the long-tail capability difference between chatbots. See the later section for more details and our plan.\n\n\n## Next Step\n\n**Rotating chatbot players**\n\nThe Chatbot Arena has been live for 4 weeks. Based on the feedback we have gathered from the community, we will adjust the probabilities of some players appearing in the anonymous arena and try to cover as many open-source chatbots as possible. Thanks to the [unique ordering and Incrementality characteristics](https://lmsys.org/blog/2023-05-03-arena/) of the Elo algorithm, even if some players are less active in the Arena, we could still compare their past Elo scores to those active ones and get a sense of their relative ability levels.\n\nAfter the update of this leaderboard, we plan to substantially lower the participation rate of the following 3 chatbots in the Arena, to make space (and GPUs) for newer players.\n\n- stablelm-tuned-alpha-7b, Elo 882\n- dolly-v2-12b, Elo 866\n- llama-13b, Elo 854\n\n**Evaluating long-tail capability of LLMs**\n\nMost conversations happening in the Chatbot Arena are ‚Äúin the wild‚Äù natural conversations. Hence we can think of the Arena as a unique way to benchmark LLMs in the wild for such types of human-chatbot interactions. However, this also means it has several significant limitations.\n\nAs pointed out by the community in [thread 1](https://twitter.com/tinkerteller/status/1656914923316998144?s=20) and [thread 2](https://twitter.com/LechMazur/status/1659915936919347202?s=20), the current Arena and leaderboard design has one major limitation: Performing user studies on a small scale often cannot generate many hard or medium prompts that are necessary to tell the long-tail capability difference between LLMs. Moreover, for difficult questions, it is also very hard for regular Arena users to judge which LLM has generated a better answer -- some domain-specific questions are considered very difficult, even for 99% of non-expert humans.\n\nHowever, long-tail capability, such as complex reasoning, can be crucial for LLMs to complete real-world tasks. Building long-tail capability into LLMs is the holy-grail problem and is the most actively studied and invested area in LLM development.\n\nOn usual prompts, a strong and mediocre chatbot may answer similarly more or less. Consequently, the current leaderboard Elo scores may be unable to reflect such capability differences between two LLMs (e.g., GPT-4 vs. Vicuna-13B).\n\nWe listen carefully to the community feedback and are thinking about how to improve the leaderboard to overcome these limitations and capture the long-tail capability different in LLMs. On top of the Chatbot Arena, we are actively designing a new tournament mechanism to examine the chatbots using presets of expert-designed questions and expert judges. We will have more updates soon.\n","date":1684972800000},{"slug":"2023-05-10-leaderboard","frontmatter":{"title":"Chatbot Arena Leaderboard Updates (Week 2)","author":"LMSYS Org","date":"May 10, 2023","previewImg":"/images/blog/leaderboard_week2/leaderboard_cover.png"},"content":"\nWe release an updated leaderboard with more models and new data we collected last week, after the announcement of the anonymous [Chatbot Arena](https://lmsys.org/blog/2023-05-03-arena/). We are actively iterating on the design of the arena and leaderboard scores.\n\nIn this update, we have added 4 new yet strong players into the Arena, including three **proprietary models** and one open-source model. They are:\n\n- OpenAI GPT-4\n- OpenAI GPT-3.5-turbo\n- Anthropic Claude-v1\n- RWKV-4-Raven-14B \n\nTable 1 displays the Elo ratings of all 13 models, which are based on the 13K voting data and calculations shared in this [notebook](https://colab.research.google.com/drive/1iI_IszGAwSMkdfUrIDI6NfTG7tGDDRxZ?usp=sharing). You can also try the voting [demo](https://arena.lmsys.org) and see more about the [leaderboard](https://leaderboard.lmsys.org).\n\n\u003cstyle\u003e\nth {text-align: left}\ntd {text-align: left}\n\u003c/style\u003e\n\n\u003cbr\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 1. Elo ratings of LLMs (Timeframe: April 24 - May 8, 2023)\u003c/p\u003e\n\u003ctable style=\"display: flex; justify-content: center;\" align=\"left\" \u003e\n\u003ctbody\u003e\n\u003ctr\u003e \u003cth\u003eRank\u003c/th\u003e \u003cth\u003eModel\u003c/th\u003e \u003cth\u003eElo Rating\u003c/th\u003e \u003cth\u003eDescription\u003c/th\u003e \u003cth\u003eLicense\u003c/th\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e1\u003c/td\u003e \u003ctd\u003eü•á \u003ca href=\"https://chat.openai.com/\" target=\"_blank\"\u003eGPT-4\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1274\u003c/td\u003e \u003ctd\u003eChatGPT-4 by OpenAI\u003c/td\u003e \u003ctd\u003eProprietary\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e2\u003c/td\u003e \u003ctd\u003eü•à \u003ca href=\"https://www.anthropic.com/index/introducing-claude\" target=\"_blank\"\u003eClaude-v1\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1224\u003c/td\u003e \u003ctd\u003eClaude by Anthropic\u003c/td\u003e \u003ctd\u003eProprietary\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e3\u003c/td\u003e \u003ctd\u003eü•â \u003ca href=\"https://chat.openai.com/\" target=\"_blank\"\u003eGPT-3.5-turbo\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1155\u003c/td\u003e \u003ctd\u003eChatGPT-3.5 by OpenAI\u003c/td\u003e  \u003ctd\u003eProprietary\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e4\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://lmsys.org/blog/2023-03-30-vicuna/\" target=\"_blank\"\u003eVicuna-13B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1083\u003c/td\u003e \u003ctd\u003ea chat assistant fine-tuned from LLaMA on user-shared conversations by LMSYS\u003c/td\u003e \u003ctd\u003eWeights available; Non-commercial\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e5\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://bair.berkeley.edu/blog/2023/04/03/koala\" target=\"_blank\"\u003eKoala-13B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1022\u003c/td\u003e \u003ctd\u003ea dialogue model for academic research by BAIR\u003c/td\u003e \u003ctd\u003eWeights available; Non-commercial\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e6\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://huggingface.co/BlinkDL/rwkv-4-raven\" target=\"_blank\"\u003eRWKV-4-Raven-14B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e989\u003c/td\u003e \u003ctd\u003ean RNN with transformer-level LLM performance\u003c/td\u003e \u003ctd\u003eApache 2.0\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e7\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://open-assistant.io\" target=\"_blank\"\u003eOasst-Pythia-12B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e928\u003c/td\u003e \u003ctd\u003ean Open Assistant for everyone by LAION\u003c/td\u003e \u003ctd\u003eApache 2.0\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e8\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://chatglm.cn/blog\" target=\"_blank\"\u003eChatGLM-6B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e918\u003c/td\u003e \u003ctd\u003ean open bilingual dialogue language model by Tsinghua University\u003c/td\u003e \u003ctd\u003eWeights available; Non-commercial\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e9\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://github.com/stability-AI/stableLM\" target=\"_blank\"\u003eStableLM-Tuned-Alpha-7B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e906\u003c/td\u003e \u003ctd\u003eStability AI language models\u003c/td\u003e  \u003ctd\u003eCC-BY-NC-SA-4.0\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e10\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\" target=\"_blank\"\u003eAlpaca-13B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e904\u003c/td\u003e \u003ctd\u003ea model fine-tuned from LLaMA on instruction-following demonstrations by Stanford\u003c/td\u003e  \u003ctd\u003eWeights available; Non-commercial\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e11\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://huggingface.co/lmsys/fastchat-t5-3b-v1.0\" target=\"_blank\"\u003eFastChat-T5-3B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e902\u003c/td\u003e \u003ctd\u003ea chat assistant fine-tuned from FLAN-T5 by LMSYS\u003c/td\u003e \u003ctd\u003eApache 2.0\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e12\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\" target=\"_blank\"\u003eDolly-V2-12B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e863\u003c/td\u003e \u003ctd\u003ean instruction-tuned open large language model by Databricks\u003c/td\u003e \u003ctd\u003eMIT\u003c/td\u003e \u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e13\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://arxiv.org/abs/2302.13971\" target=\"_blank\"\u003eLLaMA-13B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e826\u003c/td\u003e \u003ctd\u003eopen and efficient foundation language models by Meta\u003c/td\u003e \u003ctd\u003eWeights available; Non-commercial\u003c/td\u003e \u003c/tr\u003e\n\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u0026shy;\n\nIf you want to see more models, please help us [add them](https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model) or [contact us](mailto:lmsysorg@gmail.com) by giving us API access.\n\n## Overview\nThanks to the community's help, we have gathered 13k anonymous votes. Looking at the rankings and data collected from this leaderboard update, we have a few interesting findings.\n\n**Gaps between proprietary and open-source models**  \nWe do observe a substantial gap between the three proprietary models and all other open-source models. \nIn particular, GPT-4 is leading the board, achieving an Elo score of 1274. It is almost 200 scores higher than the best open-source alternative on this board -- our Vicuna-13B.\nAfter dropping ties, GPT-4 wins 82% of the matches when it is against Vicuna-13B, and it even wins 79% of the matches when it is against its previous generation GPT-3.5-turbo.\n\nHowever, it is important to note that these open-source models on the leaderboard generally have fewer parameters, in the range of 3B - 14B, than proprietary models.\nIn fact, recent advancements in LLMs and data curation have allowed for significant improvements in performance with smaller models. \n[Google's latest PaLM 2](https://ai.google/discover/palm2) is a great example of this: knowing that PaLM 2 achieves even better performance than its previous generation using smaller model sizes, \nwe remain very optimistic about the potential for open-source language models to catch up. Through our [FastChat-based Chatbot Arena](https://github.com/lm-sys/FastChat) and this leaderboard effort, \nwe hope to contribute a trusted evaluation platform for evaluating LLMs, and help advance this field and create better language models for everyone.\n \n\n**Comparing proprietary models**  \nHowever, among the three proprietary models, we do observe, based on our collected voting results, \nthat Anthropic's Claude model is preferred by our users over GPT-3.5-turbo, which is often discussed as its opponent.\nIn fact, Claude is highly competitive even when competing against the most powerful model -- OpenAI's GPT-4. \nLooking at the win rate plots (Figure 3 below), among the 66 non-tied matches between GPT-4 and Claude, Claude indeed wins over GPT-4 in 32 (48%) matches. Great job Anthropic team!\n\n**Comparing open-source chatbots**  \nIn this update, we have added RWKV-4-Raven-14B model into the Arena thanks to the community [contribution](https://github.com/lm-sys/FastChat/issues/633). Unlike all other models, RWKV model is an RNN instead of a transformer-based model; but it performs surprisingly well!\nIt soon uptrends on the leaderboard and is positioned #6 on the overall leaderboard. It wins more than 50% of non-tied matches against all other open-source models except Vicuna. You are welcome to check out its [repo](https://github.com/BlinkDL/RWKV-LM) to learn more about other features like memory saving and fast inference.\nKudos to the RWKV developers.\n\n**Fluctuations of Elo scores**  \nThe Elo scores of existing models can go up and down depending on the results of the new games played. This is similar to the way the Elo scores of chess players vary over time (see [here](https://en.chessbase.com/post/historical-chess-ratings-dynamically-presented)).\nSince the participation of the three strong proprietary models, the Chatbot Arena has never been more competitive than ever before!\nAs a consequence, we observe the Elo scores of all open source models have decreased a bit. This is because open source models lose lots of pairwise matches when they are against the proprietary models.\n\n## Detailed Results\n\n**When does GPT-4 fail?**  \nWe present a few examples in which GPT-4 is not preferred by users.\n\n\u003cimg src=\"/images/blog/leaderboard_week2/claude_vs_gpt4.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 1: One example where Claude is preferred over GPT-4.\u003c/p\u003e\n\nIn Figure 1, the user posed a tricky question that demanded careful reasoning and planning. Although both Claude and GPT-4 provided similar answers, Claude's response was marginally better as the needle was positioned on top. \nHowever, we observed that the outcome of this example cannot always be replicated due to the randomness of sampling.\nSometimes GPT-4 can also give the same order as Claude, but it fails at this generation trial.\nAdditionally, we noted that the behavior of GPT-4 differed slightly when using the OpenAI API versus the ChatGPT interface, which could be attributed to different prompts, sampling parameters, or other unknown factors.\n\n\u003cimg src=\"/images/blog/leaderboard_week2/claude_vs_gpt4_fail.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 2: One example where a user thinks both Claude and GPT-4 are wrong.\u003c/p\u003e\n\nIn Figure 2, both Claude and GPT-4 are still struggling with this kind of tricky reasoning questions despite their amazing capabilities.\n\nBesides these tricky cases, there are also a lot of easy questions that do not require complex reasoning or knowledge. In this case, open source models like Vicuna can perform on par with GPT-4, so we might be able to use a slightly weaker (but smaller or cheaper) LLM in place of the more powerful one like GPT-4.\n\n**Win Fraction Matrix**  \nWe present the win fraction of all model pairs in Figure 3.\n\u003cimg src=\"/images/blog/leaderboard_week2/win_fraction_matrix.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 3: Fraction of Model A Wins for All Non-tied A vs. B Battles.\u003c/p\u003e\n\n**Language-specific leaderboards**  \nLastly, we present two language-specific leaderboards, by isolating the conversation data into two subsets based on the language: (1) English-only and (2) non-English. From Figure 4, we can tell that Koala is worse at non-English languages and ChatGLM-6B is better at non-English languages. This is because of the different compositions of their training data.\n\n\u003cimg src=\"/images/blog/leaderboard_week2/english_vs_non_english.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 4: The English-only and non-English leaderboards.\u003c/p\u003e\n\nMore figures, analyses, and calculations can be found in this [notebook](https://colab.research.google.com/drive/1iI_IszGAwSMkdfUrIDI6NfTG7tGDDRxZ?usp=sharing).\n\n## Next Steps\n\n**Help us add more models**  \nSince the launch of Chatbot Arena, we have seen growing interest from the community. Many model developers are eager to put their chatbots into the Arena and see how they perform against others.\nPlease help us add more models by following [this guide](https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model). \n\n**Bring your own self-hosted chatbot (BYOC)**  \nWe also plan to open some APIs to allow competitors to register their self-hosted chatbots and participate in the Arena.\n\n**Area-specific Arena**  \nSimilar to the language-specific Arena, we will extend a single, monolithic leaderboard to more areas, and publish more functionality-specific leaderboards, \nsuch as writing, coding, and reasoning. In which specific area or ability do you want to see the LLMs evaluated?\nPlease give us feedback on [Discord](https://discord.gg/h6kCZb72G7) or [Twitter](https://twitter.com/lmsysorg).\n\n## Acknowledgement\nThis blog post is primarily contributed by Lianmin Zheng, Ying Sheng, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nWe thank other members of LMSYS team (Wei-Lin Chiang, Siyuan Zhuang, and more) for valuable feedback and MBZUAI for donating compute resources.\nAdditionally, we extend our thanks to community contributors for their votes and model support.\n","date":1683676800000},{"slug":"2023-05-03-arena","frontmatter":{"title":"Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings","author":"Lianmin Zheng*, Ying Sheng*, Wei-Lin Chiang, Hao Zhang, Joseph E. Gonzalez, Ion Stoica","date":"May 3, 2023","previewImg":"/images/blog/arena/cover.png"},"content":"\r\nWe present Chatbot Arena, a benchmark platform for large language models (LLMs) that features anonymous, randomized battles in a crowdsourced manner. In this blog post, we are releasing our initial results and a leaderboard based on the Elo rating system, which is a widely-used rating system in chess and other competitive games. We invite the entire community to join this effort by contributing new models and evaluating them by asking questions and voting for your favorite answer.\r\n\r\n\u003cstyle\u003e\r\nth {text-align: left}\r\ntd {text-align: left}\r\n\u003c/style\u003e\r\n\r\n\u003cbr\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 1. Elo ratings of popular open-source large language models. (Timeframe: April 24 - May 1, 2023)\u003c/p\u003e\r\n\u003ctable style=\"display: flex; justify-content: center;\" align=\"left\" \u003e\r\n\u003ctbody\u003e\r\n\u003ctr\u003e\r\n\u003cth\u003eRank\u003c/th\u003e \u003cth\u003eModel\u003c/th\u003e \u003cth\u003eElo Rating\u003c/th\u003e \u003cth\u003eDescription\u003c/th\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e1\u003c/td\u003e \u003ctd\u003eü•á \u003ca href=\"https://lmsys.org/blog/2023-03-30-vicuna/\" target=\"_blank\"\u003evicuna-13b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1169\u003c/td\u003e \u003ctd\u003ea chat assistant fine-tuned from LLaMA on user-shared conversations by LMSYS\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e2\u003c/td\u003e \u003ctd\u003eü•à \u003ca href=\"https://bair.berkeley.edu/blog/2023/04/03/koala\" target=\"_blank\"\u003ekoala-13b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1082\u003c/td\u003e \u003ctd\u003ea dialogue model for academic research by BAIR\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e3\u003c/td\u003e \u003ctd\u003eü•â \u003ca href=\"https://open-assistant.io\" target=\"_blank\"\u003eoasst-pythia-12b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1065\u003c/td\u003e \u003ctd\u003ean Open Assistant for everyone by LAION\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e4\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\" target=\"_blank\"\u003ealpaca-13b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e1008\u003c/td\u003e \u003ctd\u003ea model fine-tuned from LLaMA on instruction-following demonstrations by Stanford\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e5\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://chatglm.cn/blog\" target=\"_blank\"\u003echatglm-6b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e985\u003c/td\u003e \u003ctd\u003ean open bilingual dialogue language model by Tsinghua University\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e6\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://huggingface.co/lmsys/fastchat-t5-3b-v1.0\" target=\"_blank\"\u003efastchat-t5-3b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e951\u003c/td\u003e \u003ctd\u003ea chat assistant fine-tuned from FLAN-T5 by LMSYS\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e7\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\" target=\"_blank\"\u003edolly-v2-12b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e944\u003c/td\u003e \u003ctd\u003ean instruction-tuned open large language model by Databricks\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e8\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://arxiv.org/abs/2302.13971\" target=\"_blank\"\u003ellama-13b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e932\u003c/td\u003e \u003ctd\u003eopen and efficient foundation language models by Meta\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003e9\u003c/td\u003e \u003ctd\u003e\u003ca href=\"https://github.com/stability-AI/stableLM\" target=\"_blank\"\u003establelm-tuned-alpha-7b\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e858\u003c/td\u003e \u003ctd\u003eStability AI language models\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\r\n\u0026shy;\r\n\r\nTable 1 displays the Elo ratings of nine popular models, which are based on the 4.7K voting data and calculations shared in this [notebook](https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5?usp=sharing). You can also try the voting [demo](https://arena.lmsys.org) and see more about the [leaderboard](https://leaderboard.lmsys.org).\r\n\r\n\u003cimg src=\"/images/blog/arena/chat_demo.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 1. The side-by-side chatting and voting interface.\u003c/p\u003e\r\n\r\n## Introduction\r\nFollowing the great success of ChatGPT, there has been a proliferation of open-source large language models that are finetuned to follow instructions. These models are capable of providing valuable assistance in response to users‚Äô questions/prompts. Notable examples include Alpaca and Vicuna, based on LLaMA, and OpenAssistant and Dolly, based on Pythia.\r\n\r\nDespite the constant release of new models every week, the community faces a challenge in benchmarking these models effectively. Benchmarking LLM assistants is extremely challenging because the problems can be open-ended, and it is very difficult to write a program to automatically evaluate the response quality.\r\nIn this case, we typically have to resort to human evaluation based on pairwise comparison.\r\n\r\nThere are some desired properties for a good benchmark system based on pairwise comparison.\r\n- **Scalability**. The system should scale to a large number of models when it is not feasible to collect sufficient data for all possible model pairs.\r\n- **Incrementality**. The system should be able to evaluate a new model using a relatively small number of trials.\r\n- **Unique order**. The system should provide a unique order for all models. Given any two models, we should be able to tell which ranks higher or whether they are tied.\r\n\r\nExisting LLM benchmark systems rarely satisfy all of these properties. Classical LLM benchmark frameworks, such as [HELM](https://crfm.stanford.edu/helm/latest/) and [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), provide multi-metric measurements for tasks commonly used in academic research. However, they are not based on pairwise comparison and are not effective at evaluating open-ended questions. OpenAI also launched the [evals](https://github.com/openai/evals) project to collect better questions, but this project does not provide ranking mechanisms for all participating models. When we launched our [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) model, we utilized a GPT-4-based evaluation pipeline, but it does not provide a solution for scalable and incremental ratings.\r\n\r\nIn this blog post, we introduce Chatbot Arena, an LLM benchmark platform featuring anonymous randomized battles in a crowdsourced manner. Chatbot Arena adopts the [Elo rating system](https://en.wikipedia.org/wiki/Elo_rating_system), which is a widely-used rating system in chess and other competitive games. The Elo rating system is promising to provide the desired property mentioned above. We noticed that the [Anthropic LLM paper](https://arxiv.org/pdf/2204.05862.pdf) also adopted the Elo rating system.\r\n\r\nTo collect data, we launched the arena with several popular open-source LLMs one week ago. In the arena, a user can chat with two anonymous models side-by-side and vote for which one is better. This crowdsourcing way of data collection represents some use cases of LLMs in the wild. A comparison between several evaluation methods is shown in Table 2.\r\n\r\n\u003cbr\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 2: Comparison between different evaluation methods.\u003c/p\u003e\r\n\u003ctable style=\"display: flex; justify-content: center;\"\u003e\r\n\u003ctbody\u003e\r\n\u003ctr\u003e\r\n\u003cth\u003e\u003c/th\u003e \u003cth\u003eHELM / lm-evaluation-harness\u003c/th\u003e \u003cth\u003eOpenAI/eval\u003c/th\u003e \u003cth\u003eAlpaca Evaluation\u003c/th\u003e \u003cth\u003eVicuna Evaluation\u003c/th\u003e \u003cth\u003eChatbot Arena\u003c/th\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003eQuestion Source\u003c/td\u003e \u003ctd\u003eAcademic datasets\u003c/td\u003e \u003ctd\u003eMixed\u003c/td\u003e \u003ctd\u003eSelf-instruct evaluation set\u003c/td\u003e \u003ctd\u003eGPT-4 generated\u003c/td\u003e \u003ctd\u003eUser prompts\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003eEvaluator\u003c/td\u003e \u003ctd\u003eProgram\u003c/td\u003e \u003ctd\u003eProgram/Model\u003c/td\u003e \u003ctd\u003eHuman\u003c/td\u003e \u003ctd\u003eGPT-4\u003c/td\u003e \u003ctd\u003eUser\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr\u003e\r\n\u003ctd\u003eMetrics\u003c/td\u003e \u003ctd\u003eBasic metrics \u003c/td\u003e \u003ctd\u003eBasic metrics\u003c/td\u003e \u003ctd\u003eWin rate\u003c/td\u003e \u003ctd\u003eWin rate\u003c/td\u003e \u003ctd\u003eElo ratings\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\r\n## Data Collection\r\nWe hosted the arena at [https://arena.lmsys.org](https://arena.lmsys.org) with our multi-model serving system, [FastChat](https://github.com/lm-sys/FastChat). When a user enters the arena, they can chat with two anonymous models side-by-side, as shown in Figure 1.\r\nAfter getting responses from the two models, users can continue chatting or vote for the model they think is better. Once a vote is submitted, the model names will be revealed. Users can continue chatting or restart a new battle with two new randomly chosen anonymous models. The platform logs all user interactions. In our analysis, we only use the votes when the model names are hidden.\r\n\r\nThe arena was launched about one week ago and we have collected 4.7k valid anonymous votes since then.  We share some exploratory analysis in this [notebook](https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5?usp=sharing) and present a short summary here.\r\n\r\n\u003cimg src=\"/images/blog/arena/battle_counts.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 60%\"\u003e\u003c/img\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 2: Battle count of each combination of models\u003c/p\u003e\r\n\r\nFigure 2 shows the battles count of each combination of models. When we initially launched the tournament, we had prior information on the likely ranking based on our benchmarks and chose to pair models according to this ranking. We gave preference to what we believed would be strong pairings based on this ranking. However, we later switched to uniform sampling to get better overall coverage of the rankings. Towards the end of the tournament, we also introduced a new model `fastchat-t5-3b`. All of these result in non-uniform model frequency.\r\n\r\n\u003cimg src=\"/images/blog/arena/lang_counts.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 80%\"\u003e\u003c/img\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 3: Battle counts for the top-15 languages.\u003c/p\u003e\r\n\r\nFigure 3 plots the language distribution and shows most user prompts are in English.\r\n\r\n## Elo Rating System\r\nThe [Elo rating system](https://en.wikipedia.org/wiki/Elo_rating_system) is a method for calculating the relative skill levels of players, which has been widely adopted in competitive games and sports. The difference in the ratings between two players serves as a predictor of the outcome of a match. The Elo rating system works well for our case because we have multiple models and we run pairwise battles between them.\r\n\r\nIf player A has a rating of `Ra` and player B a rating of `Rb`, the exact formula (using the logistic curve with base 10) for the probability of player A winning is\r\n\r\n\u003cimg src=\" https://wikimedia.org/api/rest_v1/media/math/render/svg/7c80282e9c95e92d6b210467aab48a8c4c81ef10\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\r\n\r\nThe ratings of players can be linearly updated after each battle. Suppose player A (with Rating `Ra`) was expected to score `Ea` points but actucally scored `Sa` points. The formula for updating that player's rating is \r\n\r\n\u003cimg src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1cad9fb1cfc6a8e845493ac9a40eb98541a4641a\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\r\n\r\nUsing the collected data, we compute the Elo ratings of the models in this [notebook](https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5?usp=sharing) and put the main results in Table 1. You are welcome to try the notebook and play with the voting data by yourself. The data only contains voting results without conversation histories because releasing the conversation history will raise concerns such as privacy and toxicity.\r\n\r\n## Pairwise Win Rates\r\nAs a basis for calibration, we also present here the pairwise win rates for each model in the tournament (Figure 4) as well as the predicted pairwise win rate estimated using Elo ratings (Figure 5).\r\nBy comparing the figures, we find the elo ratings can predict win rates relatively well.\r\n\r\n\u003cimg src=\"/images/blog/arena/win_fraction.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 4: Fraction of Model A wins for all non-tied A vs. B battles.\u003c/p\u003e\r\n\r\n\u003cimg src=\"/images/blog/arena/predicted_win_fraction.png\" style=\"display:block; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 5: Predicted win rate using Elo ratings for Model A in an A vs. B battle\u003c/p\u003e\r\n\r\n## Future Plans\r\nWe plan to work on the following items:\r\n- Add more closed-source models (ChatGPT-3.5, ChatGPT-4, and Claude-v1 are avaiable now in the anonymous Arena)\r\n- Add more open-source models\r\n- Release periodically updated leaderboards (e.g., monthly)\r\n- Implement better sampling algorithms, tournament mechanisms, and serving systems to support a much larger number of models\r\n- Provide fine-grained rankings on different task types.\r\n\r\nWe appreciate any feedback from you to make the arena better.\r\n\r\n## Join Us\r\nWe invite the entire community to join this benchmarking effort by contributing your models and votes for the anonymous models you think provide better answers. You can visit [https://arena.lmsys.org](https://arena.lmsys.org) to vote for better models. If you want to see a specific model in the arena, you can follow this [guide](https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model) to help us add it.\r\n\r\n## Acknowledgment\r\nWe thank other members of the Vicuna team for valuable feedback and MBZUAI for donating compute resources. Additionally, we extend our thanks to Tianjun Zhang and Eric Wallace for their insightful discussions.\r\n\r\n## Links\r\n- Demo: [https://arena.lmsys.org](https://arena.lmsys.org)\r\n- Leaderboard: [https://leaderboard.lmsys.org](https://leaderboard.lmsys.org)\r\n- GitHub: [https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat)\r\n- Colab notebook: [https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5?usp=sharing](https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5?usp=sharing)\r\n","date":1683072000000},{"slug":"2023-03-30-vicuna","frontmatter":{"title":"Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality","author":"The Vicuna Team","date":"March 30, 2023","previewImg":"/images/blog/vicuna/vicuna.jpeg"},"content":"\r\nWe introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%* quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90%\u003csup\u003e*\u003c/sup\u003e of cases. The cost of training Vicuna-13B is around $300. The [code](https://github.com/lm-sys/FastChat) and [weights](https://github.com/lm-sys/FastChat#vicuna-weights), along with an online [demo](https://chat.lmsys.org), are publicly available for non-commercial use.\r\n\r\n\u003cimg src=\"/images/blog/vicuna/vicuna.jpeg\" style=\"width: 30%; margin-left: auto; margin-right: auto; margin-bottom: auto\"\u003e\u003c/img\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eVicuna (generated by stable diffusion 2.1) \u003c/p\u003e\r\n\r\n\u003cp style=\"color:gray;\"\u003e*According to a fun and non-scientific evaluation with GPT-4. Further rigorous evaluation is needed.\u003c/p\u003e\r\n\r\n## How Good is Vicuna?\r\nAfter fine-tuning Vicuna with 70K user-shared ChatGPT conversations, we discover that Vicuna becomes capable of generating more detailed and well-structured answers compared to Alpaca (see examples below), with the quality on par with ChatGPT.\r\n\r\n\u003cstyle\u003e\r\n.tg  {border-collapse:collapse;border-spacing:0;margin:0px auto;}\r\n.tg td{border-color:#ccc;border-style:solid;border-width:1px;\r\n  overflow:hidden;padding:10px 5px;word-break:normal;}\r\n.tg .tg-head{background-color:#c0c0c0;border-color:#ccc;text-align:left;vertical-align:top;}\r\n.tg .tg-body{text-align:left;vertical-align:top;}\r\n\u003c/style\u003e\r\n\r\n\u003cstyle\u003e\r\n  iframe {\r\n    display: block;\r\n    width: 100%;\r\n    height: 950px;\r\n    border: none;\r\n    overflow: hidden;\r\n  }\r\n\u003c/style\u003e\r\n\u003ciframe src=\"/images/blog/vicuna/gpt4eval/index.html\"\u003e\u003c/iframe\u003e\r\n\u003chr\u003e\r\n\r\nHowever, evaluating chatbots is never a simple task. \r\nWith recent advancements in GPT-4, we are curious whether its capabilities have reached a human-like level that could enable an automated evaluation framework for benchmark generation and performance assessments. \r\nOur initial finding indicates that GPT-4 can produce highly consistent ranks and detailed assessment when comparing chatbots‚Äô answers (see above example of GPT-4 judgment).\r\nPreliminary evaluations based on GPT-4, summarized in Figure 1, show that Vicuna achieves 90%\u003csup\u003e*\u003c/sup\u003e capability of Bard/ChatGPT. \r\nWhile this proposed framework shows a potential to automate chatbot assessment, **it is not yet a rigorous approach**. \r\nBuilding an evaluation system for chatbots remains an open question requiring further research. More details are provided in the evaluation section.\r\n\r\n\u003cimg src=\"/images/blog/vicuna/chart.svg\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 60%\"\u003e\u003c/img\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 1. Relative Response Quality Assessed by GPT-4*\u003c/p\u003e\r\n\r\n## Online Demo\r\nTry the Vicuna-13B demo [here](https://chat.lmsys.org)!\r\n\r\n\u003c!-- Add a video that automatically play --\u003e\r\n\u003cdiv\u003e\r\n  \u003ca href=\"https://chat.lmsys.org\"  style=\"display: flex; justify-content: center; margin-top: 1em; margin-bottom: 1em;\"\u003e\r\n  \u003cvideo autoplay muted loop src=\"/images/blog/vicuna/demo-narrow.mp4\" type=\"video/mp4\" style=\"width: 70%;\" id=\"demo\"\u003e\r\n  \u003c/video\u003e\r\n  \u003c/a\u003e\r\n\u003c/div\u003e\r\n\r\n## Overview\r\nThe rapid advancement of large language models (LLMs) has revolutionized chatbot systems, resulting in unprecedented levels of intelligence as seen in OpenAI's ChatGPT. However, despite its impressive performance, the training and architecture details of ChatGPT remain unclear, hindering research and open-source innovation in this field. Inspired by the Meta LLaMA and Stanford Alpaca project, we introduce Vicuna-13B, an open-source chatbot backed by an enhanced dataset and an easy-to-use, scalable infrastructure. By fine-tuning a LLaMA base model on user-shared conversations collected from ShareGPT.com, Vicuna-13B has demonstrated competitive performance compared to other open-source models like Stanford Alpaca. This blog post provides a preliminary evaluation of Vicuna-13B's performance and describes its training and serving infrastructure. We also invite the community to interact with our online demo to test the capabilities of this chatbot.\r\n\r\n\u003cimg src=\"/images/blog/vicuna/overview.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 70%\"\u003e\u003c/img\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 2. Workflow Overview\u003c/p\u003e\r\n\r\nFigure 2 provides an overview of our work. To begin, we collected around 70K conversations from ShareGPT.com, a website where users can share their ChatGPT conversations. Next, we enhanced the training scripts provided by Alpaca to better handle multi-round conversations and long sequences. The training was done with PyTorch FSDP on 8 A100 GPUs in one day. For serving the demo, we implemented a lightweight distributed serving system. We conducted a preliminary evaluation of the model quality by creating a set of 80 diverse questions and utilizing GPT-4 to judge the model outputs. To compare two different models, we combine the outputs from each model into a single prompt for each question. The prompts are then sent to GPT-4, which assesses which model provides better responses. A detailed comparison of LLaMA, Alpaca, ChatGPT, and Vicuna is shown in Table 1 below.\r\n\r\n\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 1. Comparison between several notable models\u003c/p\u003e\r\n\r\n\u003ctable class=\"tg\" style=\"display: flex;justify-content: center;\"\u003e\r\n\u003ctbody\u003e\r\n  \u003ctr\u003e\r\n    \u003ctd class=\"tg-head\"\u003e\u003cspan style=\"font-weight:bold;\"\u003eModel Name\u003c/span\u003e\u003c/td\u003e\r\n    \u003ctd class=\"tg-head\"\u003e\u003cspan style=\"font-weight:bold;\"\u003eLLaMA\u003c/span\u003e\u003c/td\u003e\r\n    \u003ctd class=\"tg-head\"\u003e\u003cspan style=\"font-weight:bold;\"\u003eAlpaca\u003c/span\u003e\u003c/td\u003e\r\n    \u003ctd class=\"tg-head\"\u003e\u003cspan style=\"font-weight:bold;\"\u003eVicuna\u003c/span\u003e\u003c/td\u003e\r\n    \u003ctd class=\"tg-head\"\u003e\u003cspan style=\"font-weight:bold;\"\u003eBard/ChatGPT\u003c/span\u003e\u003c/td\u003e\r\n  \u003c/tr\u003e\r\n  \u003ctr\u003e\r\n    \u003ctd class=\"tg-body\"\u003eDataset\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003ePublicly available datasets\u003cbr\u003e(1T token)\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003eSelf-instruct from davinci-003 API\u003cbr\u003e(52K samples)\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003eUser-shared conversations\u003cbr\u003e(70K samples)\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003eN/A\u003c/td\u003e\r\n  \u003c/tr\u003e\r\n  \u003ctr\u003e\r\n    \u003ctd class=\"tg-body\"\u003eTraining code\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003eN/A\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003eAvailable\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003eAvailable\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003eN/A\u003c/td\u003e\r\n  \u003c/tr\u003e\r\n  \u003ctr\u003e\r\n    \u003ctd class=\"tg-body\"\u003eEvaluation metrics\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003eAcademic benchmark\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003eAuthor evaluation\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003eGPT-4 assessment\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003eMixed\u003c/td\u003e\r\n  \u003c/tr\u003e\r\n  \u003ctr\u003e\r\n    \u003ctd class=\"tg-body\"\u003eTraining cost\u003cbr\u003e(7B)\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003e82K GPU-hours\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003e$500 (data) + $100 (training)\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003e$140 (training)\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003eN/A\u003c/td\u003e\r\n  \u003c/tr\u003e\r\n  \u003ctr\u003e\r\n    \u003ctd class=\"tg-body\"\u003eTraining cost\u003cbr\u003e(13B)\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003e135K GPU-hours\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003eN/A\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003e$300 (training)\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\"\u003eN/A\u003c/td\u003e\r\n  \u003c/tr\u003e\r\n\u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\r\n## Training\r\nVicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model's maximum context length.\r\n\r\nOur training recipe builds on top of [Stanford‚Äôs alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) with the following improvements.\r\n- **Memory Optimizations:** To enable Vicuna's understanding of long context, we expand the max context length from 512 in alpaca to 2048, which substantially increases GPU memory requirements. We tackle the memory pressure by utilizing [gradient checkpointing](https://arxiv.org/abs/1604.06174) and [flash attention](https://arxiv.org/abs/2205.14135).\r\n- **Multi-round conversations:** We adjust the training loss to account for multi-round conversations and compute the fine-tuning loss solely on the chatbot's output.\r\n- **Cost Reduction via Spot Instance:** The 40x larger dataset and 4x sequence length for training poses a considerable challenge in training expenses. We employ [SkyPilot](https://github.com/skypilot-org/skypilot) [managed spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html) to reduce the cost by leveraging the cheaper spot instances with auto-recovery for preemptions and auto zone switch. This solution slashes costs for training the 7B model from $500 to around $140 and the 13B model from around $1K to $300.\r\n\r\n\r\n## Serving\r\nWe build a serving system that is capable of serving multiple models with distributed workers. It supports flexible plug-in of GPU workers from both on-premise clusters and the cloud. By utilizing a fault-tolerant controller and managed spot feature in SkyPilot, this serving system can work well with cheaper spot instances from multiple clouds to reduce the serving costs. It is currently a lightweight implementation and we are working on integrating more of our latest [research](https://arxiv.org/abs/2302.11665) into it.\r\n\r\n## How To Evaluate a Chatbot?\r\nEvaluating AI chatbots is a challenging task, as it requires examining language understanding, reasoning, and context awareness. With AI chatbots becoming more advanced, current open benchmarks may no longer suffice. For instance, the evaluation dataset used in Stanford‚Äôs Alpaca, [self-instruct](https://github.com/yizhongw/self-instruct/tree/main/human_eval), can be effectively answered by SOTA chatbots, making it difficult for humans to discern differences in performance. More limitations include training/test data contamination and the potentially high cost of creating new benchmarks. To tackle these issues, we propose an evaluation framework based on GPT-4 to automate chatbot performance assessment.\r\n\r\nFirst, we devised eight question categories, such as Fermi problems, roleplay scenarios, and coding/math tasks, to test various aspects of a chatbot's performance. Through careful prompt engineering, GPT-4 is able to generate diverse, challenging questions that baseline models struggle with. We select ten questions per category and collect answers from five chatbots: LLaMA, Alpaca, ChatGPT, Bard, and Vicuna. We then ask GPT-4 to rate the quality of their answers based on helpfulness, relevance, accuracy, and detail. We discover that GPT-4 can produce not only relatively consistent scores but also detailed explanations on why such scores are given (detailed examples [link](https://lmsys.org/vicuna_eval/)). However, we also notice that GPT-4 is not very good at judging coding/math tasks.\r\n\r\n\u003cimg src=\"/images/blog/vicuna/response-compare.png\" style=\"display: flex; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 50%;\"\u003e\u003c/img\u003e\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 3. Response Comparison Assessed by GPT-4\u003c/p\u003e\r\n\r\nFigure 3 displays the comparison results between all baselines and Vicuna. GPT-4 prefers Vicuna over state-of-the-art open-source models (LLaMA, Alpaca) in more than 90% of the questions, and it achieves competitive performance against proprietary models (ChatGPT, Bard). In 45% of the questions, GPT-4 rates Vicuna's response as better or equal to ChatGPT's.\r\nAs GPT-4 assigns a quantitative score to each response on a scale of 10, we calculate the total score for each (baseline, Vicuna) comparison pair by adding up the scores obtained by each model on 80 questions. As shown in Table 2, Vicuna‚Äôs total score is 92% of ChatGPT‚Äôs. Despite recent advancements, these chatbots still face limitations, such as struggling with basic math problems or having limited coding ability.\r\n\r\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 2. Total Scores Assessed by GPT-4. \u003c/p\u003e\r\n\r\n\u003ctable class=\"tg\" style=\"display: flex;justify-content: center;\"\u003e\r\n\u003ctbody\u003e\r\n  \u003ctr\u003e\r\n    \u003ctd class=\"tg-head\"\u003e\u003cspan style=\"font-weight:bold;\"\u003eBaseline\u003c/span\u003e\u003c/td\u003e\r\n    \u003ctd class=\"tg-head\"\u003e\u003cspan style=\"font-weight:bold;\"\u003eBaseline Score\u003c/span\u003e\u003c/td\u003e\r\n    \u003ctd class=\"tg-head\"\u003e\u003cspan style=\"font-weight:bold;\"\u003eVicuna Score\u003c/span\u003e\u003c/td\u003e\r\n  \u003c/tr\u003e\r\n  \u003ctr\u003e\r\n    \u003ctd class=\"tg-body\"\u003eLLaMA-13B\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\" style=\"text-align: right\"\u003e513.0\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\" style=\"text-align: right\"\u003e\u003cspan style=\"font-weight:bold;\"\u003e694.0\u003c/span\u003e\u003c/td\u003e\r\n  \u003c/tr\u003e\r\n  \u003ctr\u003e\r\n    \u003ctd class=\"tg-body\"\u003eAlpaca-13B\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\" style=\"text-align: right\"\u003e583.0\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\" style=\"text-align: right\"\u003e\u003cspan style=\"font-weight:bold;\"\u003e704.0\u003c/span\u003e\u003c/td\u003e\r\n  \u003c/tr\u003e\r\n  \u003ctr\u003e\r\n    \u003ctd class=\"tg-body\"\u003eBard\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\" style=\"text-align: right\"\u003e\u003cspan style=\"font-weight:bold;\"\u003e664.0\u003c/span\u003e\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\" style=\"text-align: right\"\u003e655.5\u003c/td\u003e\r\n  \u003c/tr\u003e\r\n  \u003ctr\u003e\r\n    \u003ctd class=\"tg-body\"\u003eChatGPT\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\" style=\"text-align: right\"\u003e\u003cspan style=\"font-weight:bold;\"\u003e693.0\u003c/span\u003e\u003c/td\u003e\r\n    \u003ctd class=\"tg-body\" style=\"text-align: right\"\u003e638.0\u003c/td\u003e\r\n  \u003c/tr\u003e\r\n\u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\u003cbr\u003e\r\n\r\nWhile this proposed evaluation framework demonstrates the potential for assessing chatbots, it is not yet a rigorous or mature approach, as large language models are prone to hallucinate. Developing a comprehensive, standardized evaluation system for chatbots remains an open question requiring further research.\r\n\r\n## Limitations\r\nWe have noticed that, similar to other large language models, Vicuna has certain limitations. For instance, it is not good at tasks involving reasoning or mathematics, and it may have limitations in accurately identifying itself or ensuring the factual accuracy of its outputs. Additionally, it has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias. To address the safety concerns, we use the OpenAI [moderation](https://platform.openai.com/docs/guides/moderation/overview) API to filter out inappropriate user inputs in our online demo. Nonetheless, we anticipate that Vicuna can serve as an open starting point for future research to tackle these limitations.\r\n\r\n## Release\r\nIn our first release, we will share the training, serving, and evaluation code on a GitHub repo: [https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat).\r\nWe also released the Vicuna-13B model weights, please find the instructions [here](https://github.com/lm-sys/FastChat#vicuna-weights).\r\nThere is no plan to release the dataset.\r\nJoin our [Discord](https://discord.gg/h6kCZb72G7) server and follow our [Twitter](https://twitter.com/lmsysorg) to get the latest updates.\r\n\r\n## License\r\nThe online demo is a research preview intended for non-commercial use only, subject to the model [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us If you find any potential violation.\\\\\r\nThe code is released under the Apache License 2.0.\r\n\r\n## The Team\r\nThis is a joint effort with collaborators from multiple institutions, including UC Berkeley, CMU, Stanford, UC San Diego, and MBZUAI.\r\n\r\n**Students (alphabetical order):**\\\r\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang\r\n\r\n**Advisors (alphabetical order):**\\\r\nJoseph E. Gonzalez, Ion Stoica, Eric P. Xing\r\n\r\n\r\n## Acknowledgment\r\nWe would like to thank Xinyang Geng, Hao Liu, and Eric Wallace from BAIR; Xuecheng Li, and Tianyi Zhang from Stanford Alpaca team for their insightful discussion and feedback; Qirong Ho from MBZUAI for providing support on the serving cluster. Please check out a blog post from BAIR about a concurrent effort on their chatbot, [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/).\r\n\r\n\r\n## Citation\r\n```\r\n@misc{vicuna2023,\r\n    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\\%* ChatGPT Quality},\r\n    url = {https://lmsys.org/blog/2023-03-30-vicuna/},\r\n    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},\r\n    month = {March},\r\n    year = {2023}\r\n}\r\n```\r\n","date":1680134400000}]},"__N_SSG":true},"page":"/blog","query":{},"buildId":"P243ogEyw3hjk50Uz-0Nu","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>