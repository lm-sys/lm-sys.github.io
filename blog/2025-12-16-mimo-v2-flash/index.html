<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>SGLang Day-0 Support for MiMo-V2-Flash Model | LMSYS Org</title><meta name="title" content="SGLang Day-0 Support for MiMo-V2-Flash Model | LMSYS Org"/><meta property="og:title" content="SGLang Day-0 Support for MiMo-V2-Flash Model | LMSYS Org"/><meta name="twitter:title" content="SGLang Day-0 Support for MiMo-V2-Flash Model | LMSYS Org"/><meta name="description" content="&lt;h2&gt;&lt;a id=&quot;introduction&quot; class=&quot;anchor&quot; href=&quot;#introduction&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1..."/><meta property="og:description" content="&lt;h2&gt;&lt;a id=&quot;introduction&quot; class=&quot;anchor&quot; href=&quot;#introduction&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1..."/><meta name="twitter:description" content="&lt;h2&gt;&lt;a id=&quot;introduction&quot; class=&quot;anchor&quot; href=&quot;#introduction&quot; aria-hidden=&quot;true&quot;&gt;&lt;svg aria-hidden=&quot;true&quot; class=&quot;octicon octicon-link&quot; height=&quot;16&quot; version=&quot;1.1..."/><meta property="og:image" content="https://lmsys.org/images/blog/mimo-v2-flash/decode_1.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/mimo-v2-flash/decode_1.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2025-12-16-mimo-v2-flash"/><meta name="twitter:url" content="https://lmsys.org/blog/2025-12-16-mimo-v2-flash"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d62cc293bc63f5ee.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/QRPdtedtwlzVI-v5jUlfR/_buildManifest.js" defer=""></script><script src="/_next/static/QRPdtedtwlzVI-v5jUlfR/_ssgManifest.js" defer=""></script><script src="/_next/static/QRPdtedtwlzVI-v5jUlfR/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.io" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">SGLang Day-0 Support for MiMo-V2-Flash Model</h1><p class="text-xl pt-2 pb-2">by: <!-- -->SGLang Team and Xiaomi LLM Core Team<!-- -->,<!-- --> <!-- -->Dec 16, 2025<!-- --></p><hr/><div class="pt-2 article"><h2><a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h2>
<p><a href="https://huggingface.co/XiaomiMiMo/MiMo-V2-Flash">XiaomiMiMo/MiMo-V2-Flash</a>, with 309B total parameters and 15B activated parameters, is a new inference-centric model designed to maximize decoding efficiency. It is based on two key designs: <strong>sliding window attention</strong> and <strong>multi-layer MTP</strong>. MiMo-V2-Flash is explicitly co-designed for real-world serving workloads, enabling flexible tradeoffs between throughput and latency on different hardware. Combined with SGLang’s optimized Spec v2 runtime, which provides near-zero-overhead support for multi-layer MTP and efficient SWA execution, MiMo-V2-Flash delivers balanced TPOT and throughput on H200. In this blog, we will introduce the model and SGLang's efficient support.</p>
<h2><a id="inference-efficient-modeling" class="anchor" href="#inference-efficient-modeling" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Inference-Efficient Modeling</h2>
<p>The design of MiMo-V2-Flash follows an inference-efficiency principle. The MiMo-V2-Flash adopted two critical designs:</p>
<ol>
<li>Sliding Window Attention (SWA): In SWA, each token’s receptive field is limited to a fixed, constant-sized window, to reduce the attention's complexity on the sequence dimension.</li>
<li>MTP: MiMo-V2-Flash's multi-layer MTP uses a chain of prediction heads, where each head sequentially predicts the next token. The resulting draft tokens are then verified in parallel in the following step using an extended query.
The overview of MiMo-V2-Flash is shown in the figure below:</li>
</ol>
<p><img src="/images/blog/mimo-v2-flash/overview.PNG" alt="figure1"><small><center>MiMo-V2-Flash Overview</center></small></p>
<p>Now let's see how those designs lead to a cost-efficient inference.</p>
<h3><a id="swa" class="anchor" href="#swa" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>SWA</h3>
<p>In MiMo-V2-Flash, every five attention layers with a sliding window pattern are alternated with one dense GQA. The wide use of SWA can benefit the inference from multiple perspectives. First, during the prefilling stage, compute dominates the cost. Especially when the sequence is long, $O(N^2)$ attention computation is the bottleneck. SWA reduces the $O(N^2)$ complexity to a linear level to sequence length, $O(Nw)$, where $w$ is the window size. In a long context scenario, this design can significantly reduce the TTFT. SWA also reduces KV cache complexity to a constant level - releasing more resources for a larger batch size, and allows a better TPOT through fewer KV cache loading operations.</p>
<p>The figure below shows the prefill benchmarking results for MiMo-V2-Flash.</p>
<p><img src="/images/blog/mimo-v2-flash/prefill.PNG" alt="figure2"><small><center>MiMo-V2-Flash Prefill Benchmark (Radix Cache Disabled)</center></small></p>
<h3><a id="mtp" class="anchor" href="#mtp" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>MTP</h3>
<p>One of the most important designs in MiMo-V2-Flash is the multi-layer MTP, with 3 MTP layers.</p>
<p>In decoding scenarios, most of the kernels are memory-bound. Since the query length is always 1, using a larger number of parallel decoding tokens is the most intuitive way to achieve higher throughput.</p>
<p>However, as the batch size increases to a certain level, this effect will be restricted - the KV cache memory access also grows linearly with the batch size, and it performs as the memory-bounded bottleneck. At this time, the device's computing potential is still not fulfilled, but it's hard to increase throughput by increasing batch size.</p>
<p>MTP can still leverage this underexploited compute to reduce the TPOT. In MTP, multiple tokens are generated at the same time by sequential prediction heads, and the tokens will be verified in parallel in the same query, increasing the query length. This will not trigger more KV cache access; it will always increase arithmetic intensity. When the inference is still heavily memory-bound, and the batch size's effect has been marginal, an aggressive MTP strategy with a satisfying acceptance rate can theoretically leverage the rest of the device's potential and achieve a better TPOT.</p>
<h2><a id="hardware-aware-mtp-configuration" class="anchor" href="#hardware-aware-mtp-configuration" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hardware-Aware MTP Configuration</h2>
<p>Since MTP benefits from an unsaturated arithmetic intensity, and GQA's arithmetic computation is low - MiMo-V2-Flash attention design is natively well-suited for multi-layer MTP. However, when deploying MiMo-V2-Flash, choosing the right combination of batch size and MTP depth is still essential for achieving the optimal compute–memory balance and maximizing performance across different hardware platforms. Theoretically, we want to choose the best tradeoff, which achieves a satisfying throughput and TPOT simultaneously. The sweet spot of this trade-off depends on the hardware, because each hardware platform has its own roofline model.</p>
<p>Generally speaking, devices with a higher roofline benefit more from aggressive MTP because they have abundant compute capacity that is harder to saturate in memory-bound decoding. In contrast, inference-oriented accelerators, e.g., H20, have comparatively limited FLOPs, and the usage of MTP should be more careful: aggressive MTP depth can push the workload to compute-bound and degrade the throughput.</p>
<p>Here, we provide the benchmarking results on H200. MiMo-V2-Flash achieves balanced performance in both throughput and per request TPS. Thanks to SWA and MTP, the per request decoding throughput remains at 150 TPS even under long-context settings of up to 64K input tokens with per DP rank batch size 16.</p>
<p><img src="/images/blog/mimo-v2-flash/decode_1.png" alt="figure3"><small><center>MiMo-V2-Flash Decode Benchmark (DP 2, TP 4, EP 8, MTP Accept Length 3.6, Input Token Length 16k, Varying Batch Size)</center></small></p>
<p><img src="/images/blog/mimo-v2-flash/decode_2.png" alt="figure4"><small><center>MiMo-V2-Flash Decode Benchmark (DP 2, TP 4, EP 8, MTP Accept Length 3.6, Per DP Rank Batch Size 16, Varying Input Token Length)</center></small></p>
<h2><a id="fast-mtp-serving-with-sglang-spec-v2" class="anchor" href="#fast-mtp-serving-with-sglang-spec-v2" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fast MTP Serving with SGLang Spec v2</h2>
<p>MiMo’s multi-layer MTP is implemented natively on SGLang’s spec v2. We apply the fully overlapped MTP feature to improve throughput and latency, delivering faster MTP serving. In spec v2, the overlap scheduler is fused with speculative decoding: output sync/processing is delayed while the next batch’s kernels launch early, so CPU overhead for batching and syncing is hidden in GPU forward. This cuts GPU bubbles and improves throughput and latency.</p>
<p>The figure below is a screenshot of the profiling, showing the overlapped decoding process with spec v2.</p>
<p><img src="/images/blog/mimo-v2-flash/profile.png" alt="figure4"><small><center>Overlapped Speculative Decoding Profile</center></small></p>
<h2><a id="more-discussions" class="anchor" href="#more-discussions" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>More Discussions</h2>
<p>In most LLM-serving workloads, the decoding stage is memory-bounded, leaving substantial compute underutilized, particularly on the mainstream training-oriented GPUs. While inference-specific accelerators with high bandwidth and lower FLOPs offer a cost-efficient choice, their speed is limited. MiMo-V2-Flash attempts to take another perspective to make the model itself inference-efficient. The multi-layer MTP model may be a generalizable solution - if the acceptance rate can be further optimized, it allows people to leverage their GPU's computation to achieve faster decoding. With a more adaptable architecture, hardware selection becomes more flexible: each device can operate at its own optimal compute–memory balance point. This opens the possibility of using the same class of hardware for both training and inference, simplifying deployment and reducing overall system cost.</p>
<p>MiMo-V2-Flash support is already available in SGLang via PR (<a href="https://github.com/sgl-project/sglang/pull/15207">#15207</a>, <a href="https://github.com/sgl-project/sglang/pull/15208">#15208</a>) and will be merged into the main branch shortly. The benchmarks in this blog were conducted on MiMo’s optimized branch, and the corresponding optimizations will be upstreamed into SGLang main in the near future.</p>
<h2><a id="getting-started" class="anchor" href="#getting-started" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Getting Started</h2>
<p>MiMo-V2-Flash is currently available in SGLang via Docker image and pip install. Please see the instructions below to launch the SGLang server and start using MiMo-V2-Flash.</p>
<p>See Instructions below:</p>
<br>
<details>
<summary><span style="font-size: 1.3em; font-weight: bold;">Docker</span></summary>
<pre><code class="hljs language-bash"><span class="hljs-comment"># Pull the docker image</span>
docker pull lmsysorg/sglang:dev-pr-15207

<span class="hljs-comment"># Launch the container</span>
docker run -it --gpus all \
  --shm-size=32g \
  --ipc=host \
  --network=host \
  lmsysorg/sglang:dev-pr-15207 bash

<span class="hljs-comment"># Start the server</span>
SGLANG_ENABLE_SPEC_V2=1 python3 -m sglang.launch_server \
        --model-path XiaomiMiMo/MiMo-V2-Flash \
        --dp-size 2 \
        --enable-dp-attention \
        --tp-size 8 \
        --trust-remote-code \
        --mem-fraction-static 0.75 \
        --max-running-requests 128 \
        --chunked-prefill-size 16384 \
        --reasoning-parser qwen3 \
        --tool-call-parser mimo \
        --model-loader-extra-config <span class="hljs-string">&#x27;{&quot;enable_multithread_load&quot;: &quot;true&quot;,&quot;num_threads&quot;: 64}&#x27;</span> \
        --attention-backend fa3 \
        --speculative-algorithm EAGLE \
        --speculative-num-steps=3 \
        --speculative-eagle-topk=1 \
        --speculative-num-draft-tokens=4 \
        --enable-mtp
</code></pre>
</details>
<br>
<details>
<summary><span style="font-size: 1.3em; font-weight: bold;">Pip Installation</span></summary>
<pre><code class="hljs language-bash"><span class="hljs-comment"># On a machine with SGLang dependencies installed or inside a SGLang nightly container</span>
<span class="hljs-comment"># Start an SGLang nightly container</span>
docker run -it --gpus all \
  --shm-size=32g \
  --ipc=host \
  --network=host \
  lmsysorg/sglang:nightly-dev-20251215-4449c170 bash

<span class="hljs-comment"># If you already have SGLang installed, uninstall the current SGLang version</span>
pip uninstall sglang -y

<span class="hljs-comment"># Install the PyPI Package</span>
pip install sglang==0.5.6.post2.dev8005+pr.15207.g39d5bd57a \
  --index-url https://sgl-project.github.io/whl/pr/ \
  --extra-index-url https://pypi.org/simple

<span class="hljs-comment">#Launch the server</span>
SGLANG_ENABLE_SPEC_V2=1 python3 -m sglang.launch_server \
        --model-path XiaomiMiMo/MiMo-V2-Flash \
        --dp-size 2 \
        --enable-dp-attention \
        --tp-size 8 \
        --trust-remote-code \
        --mem-fraction-static 0.75 \
        --max-running-requests 128 \
        --chunked-prefill-size 16384 \
        --reasoning-parser qwen3 \
        --tool-call-parser mimo \
        --model-loader-extra-config <span class="hljs-string">&#x27;{&quot;enable_multithread_load&quot;: &quot;true&quot;,&quot;num_threads&quot;: 64}&#x27;</span> \
        --attention-backend fa3 \
        --speculative-algorithm EAGLE \
        --speculative-num-steps=3 \
        --speculative-eagle-topk=1 \
        --speculative-num-draft-tokens=4 \
        --enable-mtp
</code></pre>
</details>
<br>
<details>
<summary><span style="font-size: 1.3em; font-weight: bold;">Testing the deployment</span></summary>
<p>Once the server is running, test it with a chat completion request in another terminal:</p>
<pre><code class="hljs language-bash">curl http://localhost:30000/v1/chat/completions \
  -H <span class="hljs-string">&quot;Content-Type: application/json&quot;</span> \
  -d <span class="hljs-string">&#x27;{
    &quot;model&quot;: &quot;XiaomiMiMo/MiMo-V2-Flash&quot;,
    &quot;messages&quot;: [
      {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello! What can you help me with?&quot;}
    ],
    &quot;temperature&quot;: 0.7,
    &quot;max_tokens&quot;: 100
  }&#x27;</span>

</code></pre>
<p><strong>Expected response:</strong></p>
<pre><code class="hljs language-json"><span class="hljs-punctuation">{</span>
  <span class="hljs-attr">&quot;id&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;...&quot;</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;object&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;chat.completion&quot;</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;model&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;XiaomiMiMo/MiMo-V2-Flash&quot;</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;choices&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">{</span>
    <span class="hljs-attr">&quot;message&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
      <span class="hljs-attr">&quot;role&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;assistant&quot;</span><span class="hljs-punctuation">,</span>
      <span class="hljs-attr">&quot;content&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Hello! I can help you with...&quot;</span>
    <span class="hljs-punctuation">}</span>
  <span class="hljs-punctuation">}</span><span class="hljs-punctuation">]</span>
<span class="hljs-punctuation">}</span>
</code></pre>
</details>
<br>
<details>
<summary><span style="font-size: 1.3em; font-weight: bold;">Troubleshooting</span></summary>
<p><strong>DeepGEMM Timeout Error</strong>
Occasionally DeepGEMM timeout errors occur during first launch. Simply rerun the server command in the same container - the compiled kernels are cached and subsequent launches will be fast.</p>
</details></div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"SGLang Day-0 Support for MiMo-V2-Flash Model","author":"SGLang Team and Xiaomi LLM Core Team","date":"December 16, 2025","previewImg":"/images/blog/mimo-v2-flash/decode_1.png"},"content":"\n## Introduction\n[XiaomiMiMo/MiMo-V2-Flash](https://huggingface.co/XiaomiMiMo/MiMo-V2-Flash), with 309B total parameters and 15B activated parameters, is a new inference-centric model designed to maximize decoding efficiency. It is based on two key designs: **sliding window attention** and **multi-layer MTP**. MiMo-V2-Flash is explicitly co-designed for real-world serving workloads, enabling flexible tradeoffs between throughput and latency on different hardware. Combined with SGLang’s optimized Spec v2 runtime, which provides near-zero-overhead support for multi-layer MTP and efficient SWA execution, MiMo-V2-Flash delivers balanced TPOT and throughput on H200. In this blog, we will introduce the model and SGLang's efficient support.\n\n## Inference-Efficient Modeling\nThe design of MiMo-V2-Flash follows an inference-efficiency principle. The MiMo-V2-Flash adopted two critical designs:\n\n1. Sliding Window Attention (SWA): In SWA, each token’s receptive field is limited to a fixed, constant-sized window, to reduce the attention's complexity on the sequence dimension.\n2. MTP: MiMo-V2-Flash's multi-layer MTP uses a chain of prediction heads, where each head sequentially predicts the next token. The resulting draft tokens are then verified in parallel in the following step using an extended query.\nThe overview of MiMo-V2-Flash is shown in the figure below:\n\n![figure1](/images/blog/mimo-v2-flash/overview.PNG)\u003csmall\u003e\u003ccenter\u003eMiMo-V2-Flash Overview\u003c/center\u003e\u003c/small\u003e\n\nNow let's see how those designs lead to a cost-efficient inference.\n\n### SWA\nIn MiMo-V2-Flash, every five attention layers with a sliding window pattern are alternated with one dense GQA. The wide use of SWA can benefit the inference from multiple perspectives. First, during the prefilling stage, compute dominates the cost. Especially when the sequence is long, $O(N^2)$ attention computation is the bottleneck. SWA reduces the $O(N^2)$ complexity to a linear level to sequence length, $O(Nw)$, where $w$ is the window size. In a long context scenario, this design can significantly reduce the TTFT. SWA also reduces KV cache complexity to a constant level - releasing more resources for a larger batch size, and allows a better TPOT through fewer KV cache loading operations. \n\nThe figure below shows the prefill benchmarking results for MiMo-V2-Flash. \n\n![figure2](/images/blog/mimo-v2-flash/prefill.PNG)\u003csmall\u003e\u003ccenter\u003eMiMo-V2-Flash Prefill Benchmark (Radix Cache Disabled)\u003c/center\u003e\u003c/small\u003e\n\n### MTP\nOne of the most important designs in MiMo-V2-Flash is the multi-layer MTP, with 3 MTP layers. \n\nIn decoding scenarios, most of the kernels are memory-bound. Since the query length is always 1, using a larger number of parallel decoding tokens is the most intuitive way to achieve higher throughput. \n\nHowever, as the batch size increases to a certain level, this effect will be restricted - the KV cache memory access also grows linearly with the batch size, and it performs as the memory-bounded bottleneck. At this time, the device's computing potential is still not fulfilled, but it's hard to increase throughput by increasing batch size. \n\nMTP can still leverage this underexploited compute to reduce the TPOT. In MTP, multiple tokens are generated at the same time by sequential prediction heads, and the tokens will be verified in parallel in the same query, increasing the query length. This will not trigger more KV cache access; it will always increase arithmetic intensity. When the inference is still heavily memory-bound, and the batch size's effect has been marginal, an aggressive MTP strategy with a satisfying acceptance rate can theoretically leverage the rest of the device's potential and achieve a better TPOT.\n\n## Hardware-Aware MTP Configuration\nSince MTP benefits from an unsaturated arithmetic intensity, and GQA's arithmetic computation is low - MiMo-V2-Flash attention design is natively well-suited for multi-layer MTP. However, when deploying MiMo-V2-Flash, choosing the right combination of batch size and MTP depth is still essential for achieving the optimal compute–memory balance and maximizing performance across different hardware platforms. Theoretically, we want to choose the best tradeoff, which achieves a satisfying throughput and TPOT simultaneously. The sweet spot of this trade-off depends on the hardware, because each hardware platform has its own roofline model. \n\nGenerally speaking, devices with a higher roofline benefit more from aggressive MTP because they have abundant compute capacity that is harder to saturate in memory-bound decoding. In contrast, inference-oriented accelerators, e.g., H20, have comparatively limited FLOPs, and the usage of MTP should be more careful: aggressive MTP depth can push the workload to compute-bound and degrade the throughput.\n\nHere, we provide the benchmarking results on H200. MiMo-V2-Flash achieves balanced performance in both throughput and per request TPS. Thanks to SWA and MTP, the per request decoding throughput remains at 150 TPS even under long-context settings of up to 64K input tokens with per DP rank batch size 16.\n\n![figure3](/images/blog/mimo-v2-flash/decode_1.png)\u003csmall\u003e\u003ccenter\u003eMiMo-V2-Flash Decode Benchmark (DP 2, TP 4, EP 8, MTP Accept Length 3.6, Input Token Length 16k, Varying Batch Size)\u003c/center\u003e\u003c/small\u003e\n\n![figure4](/images/blog/mimo-v2-flash/decode_2.png)\u003csmall\u003e\u003ccenter\u003eMiMo-V2-Flash Decode Benchmark (DP 2, TP 4, EP 8, MTP Accept Length 3.6, Per DP Rank Batch Size 16, Varying Input Token Length)\u003c/center\u003e\u003c/small\u003e\n\n## Fast MTP Serving with SGLang Spec v2\nMiMo’s multi-layer MTP is implemented natively on SGLang’s spec v2. We apply the fully overlapped MTP feature to improve throughput and latency, delivering faster MTP serving. In spec v2, the overlap scheduler is fused with speculative decoding: output sync/processing is delayed while the next batch’s kernels launch early, so CPU overhead for batching and syncing is hidden in GPU forward. This cuts GPU bubbles and improves throughput and latency.\n\nThe figure below is a screenshot of the profiling, showing the overlapped decoding process with spec v2.\n\n![figure4](/images/blog/mimo-v2-flash/profile.png)\u003csmall\u003e\u003ccenter\u003eOverlapped Speculative Decoding Profile\u003c/center\u003e\u003c/small\u003e\n\n## More Discussions\nIn most LLM-serving workloads, the decoding stage is memory-bounded, leaving substantial compute underutilized, particularly on the mainstream training-oriented GPUs. While inference-specific accelerators with high bandwidth and lower FLOPs offer a cost-efficient choice, their speed is limited. MiMo-V2-Flash attempts to take another perspective to make the model itself inference-efficient. The multi-layer MTP model may be a generalizable solution - if the acceptance rate can be further optimized, it allows people to leverage their GPU's computation to achieve faster decoding. With a more adaptable architecture, hardware selection becomes more flexible: each device can operate at its own optimal compute–memory balance point. This opens the possibility of using the same class of hardware for both training and inference, simplifying deployment and reducing overall system cost.\n\nMiMo-V2-Flash support is already available in SGLang via PR ([#15207](https://github.com/sgl-project/sglang/pull/15207), [#15208](https://github.com/sgl-project/sglang/pull/15208)) and will be merged into the main branch shortly. The benchmarks in this blog were conducted on MiMo’s optimized branch, and the corresponding optimizations will be upstreamed into SGLang main in the near future.\n\n## Getting Started\n\nMiMo-V2-Flash is currently available in SGLang via Docker image and pip install. Please see the instructions below to launch the SGLang server and start using MiMo-V2-Flash.\n\nSee Instructions below:\n\n\u003cbr\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cspan style=\"font-size: 1.3em; font-weight: bold;\"\u003eDocker\u003c/span\u003e\u003c/summary\u003e\n\n```bash\n# Pull the docker image\ndocker pull lmsysorg/sglang:dev-pr-15207\n\n# Launch the container\ndocker run -it --gpus all \\\n  --shm-size=32g \\\n  --ipc=host \\\n  --network=host \\\n  lmsysorg/sglang:dev-pr-15207 bash\n\n# Start the server\nSGLANG_ENABLE_SPEC_V2=1 python3 -m sglang.launch_server \\\n        --model-path XiaomiMiMo/MiMo-V2-Flash \\\n        --dp-size 2 \\\n        --enable-dp-attention \\\n        --tp-size 8 \\\n        --trust-remote-code \\\n        --mem-fraction-static 0.75 \\\n        --max-running-requests 128 \\\n        --chunked-prefill-size 16384 \\\n        --reasoning-parser qwen3 \\\n        --tool-call-parser mimo \\\n        --model-loader-extra-config '{\"enable_multithread_load\": \"true\",\"num_threads\": 64}' \\\n        --attention-backend fa3 \\\n        --speculative-algorithm EAGLE \\\n        --speculative-num-steps=3 \\\n        --speculative-eagle-topk=1 \\\n        --speculative-num-draft-tokens=4 \\\n        --enable-mtp\n```\n\n\u003c/details\u003e\n\n\u003cbr\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cspan style=\"font-size: 1.3em; font-weight: bold;\"\u003ePip Installation\u003c/span\u003e\u003c/summary\u003e\n\n```bash\n# On a machine with SGLang dependencies installed or inside a SGLang nightly container\n# Start an SGLang nightly container\ndocker run -it --gpus all \\\n  --shm-size=32g \\\n  --ipc=host \\\n  --network=host \\\n  lmsysorg/sglang:nightly-dev-20251215-4449c170 bash\n\n# If you already have SGLang installed, uninstall the current SGLang version\npip uninstall sglang -y\n\n# Install the PyPI Package\npip install sglang==0.5.6.post2.dev8005+pr.15207.g39d5bd57a \\\n  --index-url https://sgl-project.github.io/whl/pr/ \\\n  --extra-index-url https://pypi.org/simple\n\n#Launch the server\nSGLANG_ENABLE_SPEC_V2=1 python3 -m sglang.launch_server \\\n        --model-path XiaomiMiMo/MiMo-V2-Flash \\\n        --dp-size 2 \\\n        --enable-dp-attention \\\n        --tp-size 8 \\\n        --trust-remote-code \\\n        --mem-fraction-static 0.75 \\\n        --max-running-requests 128 \\\n        --chunked-prefill-size 16384 \\\n        --reasoning-parser qwen3 \\\n        --tool-call-parser mimo \\\n        --model-loader-extra-config '{\"enable_multithread_load\": \"true\",\"num_threads\": 64}' \\\n        --attention-backend fa3 \\\n        --speculative-algorithm EAGLE \\\n        --speculative-num-steps=3 \\\n        --speculative-eagle-topk=1 \\\n        --speculative-num-draft-tokens=4 \\\n        --enable-mtp\n```\n\n\u003c/details\u003e\n\n\u003cbr\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cspan style=\"font-size: 1.3em; font-weight: bold;\"\u003eTesting the deployment\u003c/span\u003e\u003c/summary\u003e\n\nOnce the server is running, test it with a chat completion request in another terminal:\n\n```bash\ncurl http://localhost:30000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"XiaomiMiMo/MiMo-V2-Flash\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello! What can you help me with?\"}\n    ],\n    \"temperature\": 0.7,\n    \"max_tokens\": 100\n  }'\n\n```\n\n**Expected response:**\n\n```json\n{\n  \"id\": \"...\",\n  \"object\": \"chat.completion\",\n  \"model\": \"XiaomiMiMo/MiMo-V2-Flash\",\n  \"choices\": [{\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"Hello! I can help you with...\"\n    }\n  }]\n}\n```\n\n\u003c/details\u003e\n\n\u003cbr\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cspan style=\"font-size: 1.3em; font-weight: bold;\"\u003eTroubleshooting\u003c/span\u003e\u003c/summary\u003e\n\n**DeepGEMM Timeout Error**\nOccasionally DeepGEMM timeout errors occur during first launch. Simply rerun the server command in the same container - the compiled kernels are cached and subsequent launches will be fast.\n\n\n\u003c/details\u003e","slug":"2025-12-16-mimo-v2-flash"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2025-12-16-mimo-v2-flash"},"buildId":"QRPdtedtwlzVI-v5jUlfR","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>