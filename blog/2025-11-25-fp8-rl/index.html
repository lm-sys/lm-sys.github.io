<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>Unified FP8: Moving Beyond Mixed Precision for Stable and Accelerated MoE RL | LMSYS Org</title><meta name="title" content="Unified FP8: Moving Beyond Mixed Precision for Stable and Accelerated MoE RL | LMSYS Org"/><meta property="og:title" content="Unified FP8: Moving Beyond Mixed Precision for Stable and Accelerated MoE RL | LMSYS Org"/><meta name="twitter:title" content="Unified FP8: Moving Beyond Mixed Precision for Stable and Accelerated MoE RL | LMSYS Org"/><meta name="description" content="&lt;blockquote&gt;
&lt;p&gt;TL;DR: We have implemented fully FP8-based sampling and training in RL. Experiments show that for MoE models, the larger the model, the more ..."/><meta property="og:description" content="&lt;blockquote&gt;
&lt;p&gt;TL;DR: We have implemented fully FP8-based sampling and training in RL. Experiments show that for MoE models, the larger the model, the more ..."/><meta name="twitter:description" content="&lt;blockquote&gt;
&lt;p&gt;TL;DR: We have implemented fully FP8-based sampling and training in RL. Experiments show that for MoE models, the larger the model, the more ..."/><meta property="og:image" content="https://lmsys.org/images/blog/fp8-rl/3_Megatron.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/fp8-rl/3_Megatron.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2025-11-25-fp8-rl"/><meta name="twitter:url" content="https://lmsys.org/blog/2025-11-25-fp8-rl"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-eef2afd147d8eda9.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/HOCqR_uxXD79xgqqQW-Dx/_buildManifest.js" defer=""></script><script src="/_next/static/HOCqR_uxXD79xgqqQW-Dx/_ssgManifest.js" defer=""></script><script src="/_next/static/HOCqR_uxXD79xgqqQW-Dx/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">Unified FP8: Moving Beyond Mixed Precision for Stable and Accelerated MoE RL</h1><p class="text-xl pt-2 pb-2">by: <!-- -->InfiXAI Team, Ant Group AQ Team, SGLang RL Team, Miles Team<!-- -->,<!-- --> <!-- -->Nov 25, 2025<!-- --></p><hr/><div class="pt-2 article"><blockquote>
<p>TL;DR: We have implemented fully FP8-based sampling and training in RL. Experiments show that for MoE models, the larger the model, the more severe the train–inference discrepancy becomes when using BF16 training with FP8 rollout. In contrast, using unified FP8 for both training and rollout effectively eliminates train–inference inconsistency caused by quantization error, improving both the speed and stability of RL training.</p>
</blockquote>
<p>SGLang RL Team and the Miles community have conducted some interesting explorations around RL training stability and acceleration:</p>
<p><a href="https://github.com/radixark/miles/tree/main/examples/true_on_policy">Aligning the SGLang and FSDP backends</a> for <strong>strictly zero KL divergence</strong></p>
<p><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/slime/spec/readme-en.md"><strong>Speculative Decoding</strong></a> with online SFT for the draft model</p>
<p>Building on this, we now share a new progress that balances both stability and performance—<strong>implementing an end-to-end FP8 pipeline for RL training and sampling</strong>. FP8 RL training for Qwen3-4B and Qwen3-30B-A3B has been <a href="https://github.com/radixark/miles/tree/main/examples/low_precision">fully supported in miles</a> and is ready to use out of the box.</p>
<p>This work is jointly completed by the <strong>InfiXAI Team, Ant Group AQ Team, SGLang RL Team, and Miles Team</strong>. Special thanks to <strong>DataCrunch</strong> for compute sponsorship and to <strong>NVIDIA</strong> for technical support on Transformer Engine (TE).</p>
<h2><a id="hardware-foundations-of-fp8-training" class="anchor" href="#hardware-foundations-of-fp8-training" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hardware Foundations of FP8 Training</h2>
<h3><a id="tensor-cores-and-low-precision-support" class="anchor" href="#tensor-cores-and-low-precision-support" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Tensor Cores and Low-Precision Support</strong></h3>
<p>Low-precision computing is a gem of hardware–software co-design. We first introduce its hardware foundation—<strong>Tensor Cores</strong>, a type of <strong>GPU hardware acceleration unit</strong> designed specifically for <strong>large-scale matrix multiplication and accumulation</strong>, the core computation in deep learning. Compared with traditional CUDA cores, Tensor Cores can process low-precision data formats (such as FP16, BF16, FP8) with <strong>much higher throughput</strong>. Their evolution began with basic FMA (fused multiply–add) instructions and early vectorization through DP4A, but the real milestone came with the Volta architecture, which first introduced Tensor Cores as dedicated units for large-scale matrix operations. Since then, Ampere, Hopper, and the latest Blackwell architectures have continued to push this idea further:</p>
<ul>
<li><strong>Scaling up</strong>: Letting Tensor Cores process larger matrices per operation, thereby improving compute-to-memory ratios.</li>
<li><strong>Lowering precision</strong>: Continuously adding support for FP/BF16, FP8, and even lower-precision data formats.</li>
</ul>
<table>
<thead>
<tr>
<th>Arch</th>
<th>FP64</th>
<th><a href="https://newsletter.semianalysis.com/p/nvidia-tensor-core-evolution-from-volta-to-blackwell">F16</a></th>
<th>INT8</th>
<th>INT4</th>
<th>FP8</th>
<th>MXFP</th>
</tr>
</thead>
<tbody>
<tr>
<td>Volta</td>
<td>❌</td>
<td>✅ FP16</td>
<td>❌</td>
<td>❌</td>
<td>❌</td>
<td>❌</td>
</tr>
<tr>
<td>Turing</td>
<td>❌</td>
<td>✅ FP16</td>
<td>✅</td>
<td>✅</td>
<td>❌</td>
<td>❌</td>
</tr>
<tr>
<td>Ampere</td>
<td>✅</td>
<td>✅ FP16/BF16</td>
<td>✅</td>
<td>✅</td>
<td>❌</td>
<td>❌</td>
</tr>
<tr>
<td>Hopper</td>
<td>✅</td>
<td>✅ FP16/BF16</td>
<td>✅</td>
<td>❌</td>
<td>✅ <a href="https://arxiv.org/html/2505.09343v1">(accumulation precision only supports FP22)</a></td>
<td>❌</td>
</tr>
<tr>
<td>Blackwell</td>
<td>✅</td>
<td>✅ FP16/BF16</td>
<td>✅</td>
<td>❌</td>
<td>✅</td>
<td>✅ MXFP(8/6/4)<br>NVFP4</td>
</tr>
<tr>
<td>Blackwell Ultra</td>
<td>✅ (reduced FLOPs)</td>
<td>✅ FP16/BF16</td>
<td>✅ (reduced FLOPs)</td>
<td>❌</td>
<td>✅</td>
<td>✅ MXFP(8/6/4)<br>NVFP4</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Figure source: <a href="https://mp.weixin.qq.com/s?__biz=MzUxNzQ5MTExNw==&amp;mid=2247496740&amp;idx=1&amp;sn=c9403138fa59d126fe6cfda19d9b2f76&amp;chksm=f995e4e6cee26df07bf7101b58cbdfdf80d577c67122304482e3e788edfa74a71135dbf77d36&amp;cur_album_id=3289258526057463810&amp;scene=189#wechat_redirect">zartbot</a>, <a href="https://newsletter.semianalysis.com/p/nvidia-tensor-core-evolution-from-volta-to-blackwell">SemiAnalysis</a></p>
</blockquote>
<p>Under this hardware trend, using lower precision for storage and computation becomes increasingly attractive. Concretely, lower-precision floating-point formats offer several potential advantages:</p>
<ol>
<li><strong>Significantly reduced memory footprint</strong>: Compared with mainstream BF16, FP8 can theoretically halve the memory consumed by model weights and activations, directly alleviating ever-growing VRAM pressure.</li>
<li><strong>Theoretically 2× compute throughput</strong>: On mainstream GPUs (e.g., H100 SXM), FP8 Tensor Cores offer up to 1979 TFLOPS of theoretical performance, twice that of BF16 units (989 TFLOPS). This substantial performance gain is a key driver behind FP8 training.</li>
<li><strong>Mitigated memory bandwidth bottlenecks</strong>: With more compact data representation, less data must be transferred from GPU HBM to compute cores. This means less time spent on data movement and effectively reduces memory-bandwidth pressure.</li>
</ol>
<h3><a id="fp8-formats" class="anchor" href="#fp8-formats" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>FP8 Formats</strong></h3>
<p>FP8 is a floating-point format that uses 8 bits to represent values. Compared with FP32 (32 bits) and FP16/BF16 (16 bits), FP8 can reduce storage and transmission costs for the same amount of data to 1/4 or 1/2, greatly easing VRAM and bandwidth bottlenecks and improving training and inference performance. Currently, there are two major FP8 formats:</p>
<ul>
<li><strong>E4M3</strong>: 4-bit exponent + 3-bit mantissa. Smaller dynamic range but higher precision.</li>
<li><strong>E5M2</strong>: 5-bit exponent + 2-bit mantissa. Larger dynamic range but lower precision.</li>
</ul>
<p align="center">
  <img src="/images/blog/fp8-rl/1_E4vsE5.png" alt="FP8 E4M3 vs E5M2" width="80%" />
</p>
<blockquote>
<p>Figure source: <a href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf">OCP whitepaper</a></p>
</blockquote>
<p>This design allows FP8 to maintain sufficient numerical range and precision while maximizing hardware throughput.</p>
<h3><a id="fp8-scale-selection" class="anchor" href="#fp8-scale-selection" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>FP8 Scale Selection</h3>
<table>
<thead>
<tr>
<th><strong>Dimension</strong></th>
<th><strong>FP32 Scale (full-precision scaling factor)</strong></th>
<th><strong>E8M0 Scale (exponent-only scaling)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Format definition</strong></td>
<td><strong>FP32</strong> (IEEE 754 single-precision float)</td>
<td><strong>E8M0</strong> (8-bit exponent, 0-bit mantissa)</td>
</tr>
<tr>
<td><strong>Numeric properties</strong></td>
<td>Can represent real numbers with arbitrary precision.</td>
<td>Only supports <strong>powers of 2</strong>, such as 1, 2, 0.5; cannot represent values like 1.5.</td>
</tr>
<tr>
<td><strong>Core idea</strong></td>
<td>Manage scaling factors in high precision to ensure numerical stability during training.</td>
<td>Bring scaling factors into the low-precision regime and leverage bit operations for efficiency.</td>
</tr>
<tr>
<td><strong>Main advantages</strong></td>
<td>1. <strong>High precision, stable training</strong>: Accurately captures dynamic ranges, reduces quantization error, and prevents divergence.<br>2. <strong>Broad support</strong>: Default choice in mainstream libraries such as NVIDIA Transformer Engine; mature ecosystem.</td>
<td>1. <strong>Extremely hardware-friendly</strong>: Scaling can be implemented as simple <strong>bit shifts</strong>, which are very fast and energy-efficient.<br>2. <strong>Unified pipeline</strong>: The entire pipeline (including scale) runs in 8 bits, simplifying hardware design.</td>
</tr>
<tr>
<td><strong>Main disadvantages</strong></td>
<td>1. <strong>Storage overhead</strong>: Each quantized tensor needs to store one extra FP32 scale value, consuming some VRAM.<br>2. <strong>Compute overhead</strong>: Scale calculations and conversions must be done in FP32.</td>
<td>1. <strong>Precision-loss risk</strong>: Forcing rounding to powers of 2 introduces quantization noise, which can accumulate during backprop and cause divergence.<br>2. <strong>Limited dynamic-range resolution</strong>: Harder to finely adapt to complex tensor distributions.</td>
</tr>
<tr>
<td><strong>Summary</strong></td>
<td>Currently the most common and safest scheme in industry.</td>
<td>Sacrifices some precision in exchange for extreme hardware efficiency.</td>
</tr>
</tbody>
</table>
<p>After a comprehensive evaluation, we ultimately chose <strong>FP32</strong> as the scale precision during training. The reasons are:</p>
<ol>
<li><strong>Precision alignment and training stability</strong>: FP32 scales provide fine-grained numerical scaling that captures tensor dynamic ranges and keeps FP8 training loss curves as close as possible to the <strong>BF16</strong> baseline.</li>
<li><strong>Consistency with inference ecosystems</strong>: Mainstream inference models also use FP32 as the quantization scale format.</li>
<li><strong>Real-world hardware benefits</strong>:
<ul>
<li><strong>Hopper (H100/H800)</strong>: Although it supports FP8 Tensor Cores, it has no dedicated compute units for E8M0 scaling.</li>
<li><strong>Blackwell (B100/B200)</strong>: Introduces support for MXFP8 (micro-scaling), which provides hardware acceleration for block-level scaling like E8M0 (see <a href="https://arxiv.org/abs/2506.08027"><strong>arXiv:2506.08027</strong></a>).</li>
</ul>
</li>
</ol>
<p>Therefore, under current H-series clusters, forcing the use of E8M0 not only fails to deliver clear speedups, but may also introduce additional software-emulation overhead and precision risks.</p>
<h3><a id="fp8-quantization" class="anchor" href="#fp8-quantization" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>FP8 Quantization</h3>
<p>Common quantization strategies include <strong>per-tensor</strong>, <strong>per-block</strong>, and <strong>per-token</strong>. Regardless of granularity, quantization usually follows two simple steps:</p>
<p align="center">
  <img src="/images/blog/fp8-rl/2_size.png" alt="FP8 quantization flow" width="80%" />
</p>
<blockquote>
<p>Figure source: <a href="https://arxiv.org/html/2509.22536v4">InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models</a></p>
</blockquote>
<p><strong>Step 1: Compute the scaling factor $S$</strong></p>
<p>Take the maximum absolute value $\max|X|$ in a given tensor (or block) and divide it by the maximum representable FP8 value $V_{\max}$:</p>
<p>$$
S = \frac{\max|X|}{V_{\max}}
$$</p>
<p><strong>Step 2: Compute the quantized value $Q$</strong></p>
<p>Using the scaling factor <strong>$S$</strong>, divide each element $x$ in the original tensor $X$ by <strong>$S$</strong> and round to the nearest integer to obtain quantized values:</p>
<p>$$
Q(x) = \mathrm{round}\left(\frac{x}{S}\right)
$$</p>
<p>Because FP8 has lower precision than FP16/BF16, we must trade off between training stability and efficiency in practice, so forward and backward passes often use different quantization strategies and granularities:</p>
<ul>
<li><strong>Activations</strong>: Typically use per-token quantization. Activations often contain significant outliers; finer quantization granularity can localize the effect of outliers and better preserve overall precision.</li>
<li><strong>Weights</strong>: Typically use per-block quantization. After convergence, weight distributions are usually smooth (close to Gaussian) with few outliers, but are highly sensitive to quantization error. Blockwise quantization (e.g., block_size × block_size) maintains precision while working well with hardware optimizations, balancing compute efficiency and memory savings.</li>
<li><strong>Gradients</strong>: Typically use per-token quantization. Gradients have large dynamic-range variation but relatively low absolute precision requirements. Historically, most schemes used <strong>per-tensor E5M2</strong> to ensure dynamic range, but DeepSeek-V3 shows that fine-grained E4M3 can also balance precision and range.</li>
</ul>
<p align="center">
  <img src="/images/blog/fp8-rl/3_Megatron.png" alt="Mixed-granularity FP8 quantization in Megatron" width="80%" />
</p>
<blockquote>
<p>Figure source: <a href="https://arxiv.org/html/2509.22536v4">InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models</a></p>
</blockquote>
<p>The figure shows the mixed-granularity FP8 strategy used in Megatron compared to a standard BF16 pipeline. In the FP8 pipeline, different quantization methods are applied: weights use per-block quantization (blue), while activations use per-token quantization (purple). The figure presents the full training process, including forward propagation (FProp), weight-gradient computation (Wgrad), and input-gradient computation (Dgrad), and details the FProp workflow.</p>
<h2><a id="challenges-of-fp8-training" class="anchor" href="#challenges-of-fp8-training" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Challenges of FP8 Training</h2>
<p>Although FP8 shows great potential, in real engineering practice—especially when combining Megatron-Core and TransformerEngine (TE)—we encounter three main challenges: <strong>memory/efficiency not meeting expectations, difficulty aligning precision, and stability issues in the framework itself.</strong> We refer to our unified FP8 training-and-inference setup as <strong>FP8-TI (FP8 Training &amp; Inference)</strong>.</p>
<h3><a id="memory-and-compute-efficiency-theory-vs-reality" class="anchor" href="#memory-and-compute-efficiency-theory-vs-reality" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Memory and Compute Efficiency: Theory vs. Reality</strong></h3>
<p>In practice, the memory savings and speedups brought by FP8 are often less significant than theory suggests, mainly due to:</p>
<ul>
<li><strong>Limited memory optimization</strong>:
<ul>
<li><strong>Redundant weight copies</strong>: To speed up backprop, TransformerEngine <a href="https://github.com/NVIDIA/TransformerEngine/blob/main/transformer_engine/common/transpose/quantize_transpose_square_blockwise.cu#L481">keeps an extra transposed copy of quantized weights</a>. This prevents weight memory usage from being reduced by the expected factor of 2.</li>
<li><strong>High-precision activation copies</strong>: In the forward pass of attention and activation layers, frameworks typically retain a high-precision copy of activations for accurate gradient computation later. FP8 does not reduce this portion of memory usage.</li>
</ul>
</li>
<li><strong>Compute-efficiency bottlenecks</strong>:
<ul>
<li><strong>Performance degradation with small batch sizes</strong>: When batch_size is small, FP8 training may fail to fully utilize GPU compute units and can even underperform BF16. The root cause is that FP8 introduces extra quantization and dequantization operations, which add CPU overhead. In Agentic RL scenarios, which typically use small batch sizes (e.g., batch_size=4), this issue is particularly pronounced—frequent CPU overhead can make FP8 training slower than BF16. (As shown below, GPU kernels are not densely scheduled; often the GPU has already finished the previous work but the next kernel launch is delayed because the system is CPU-bound.)</li>
</ul>
</li>
</ul>
<p align="center">
  <img src="/images/blog/fp8-rl/4_cpu_bound.png" alt="CPU bound for FP8 training" width="50%" />
</p>
<blockquote>
<p>Figure: CPU-bound behavior in FP8 training</p>
</blockquote>
<h3><a id="precision-alignment-cumulative-error-matters" class="anchor" href="#precision-alignment-cumulative-error-matters" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Precision Alignment: Cumulative Error Matters</strong></h3>
<p>The low-precision nature of FP8 inherently introduces numerical discrepancies relative to BF16, which can be amplified in deep models and cause training-instability problems:</p>
<ul>
<li><strong>Intrinsic quantization error</strong>: Even if accumulation is performed in FP32, quantizing FP8 inputs for a single GEMM operation introduces error. Experiments show that compared with BF16 GEMM, the typical error is about 0.0007.</li>
<li><strong>Layer-wise cumulative effect</strong>: In deep Transformer models, these small errors accumulate layer by layer during forward and backward passes:
<ul>
<li><strong>In pre-training and fine-tuning (SFT)</strong>: Gradients are mainly dominated by the log probabilities of ground-truth labels. With fine-grained blockwise quantization, errors can usually be kept within an acceptable range and models are unlikely to collapse.</li>
<li><strong>In reinforcement learning (RL)</strong>: Gradients are often determined by the difference between two log probabilities from two forward passes. In this case, accumulated FP8 error can be amplified, causing gradients to deviate from their ideal direction and impacting convergence efficiency—or even pushing the optimization “in the wrong direction” (as discussed later).</li>
</ul>
</li>
</ul>
<h3><a id="framework-adaptation-transformerengine-version-compatibility" class="anchor" href="#framework-adaptation-transformerengine-version-compatibility" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Framework Adaptation: TransformerEngine Version Compatibility</strong></h3>
<p>Besides algorithmic challenges, there is room for improvement in how Megatron-Core integrates with <strong>Transformer Engine (TE)</strong>, especially given TE’s rapid iteration:</p>
<ul>
<li><strong>Version dependencies and migration overhead</strong>: TE’s fast iteration brings new features but also strict version dependencies. In practice, we found that even the same training script can yield different numerical behaviors across TE versions, and sometimes code adjustments are required to avoid issues such as NaNs.</li>
<li><strong>Maturity for specific architectures</strong>: Full FP8 support for <strong>all mainstream model architectures</strong> is an ongoing process. For some <strong>nonstandard or newer components</strong> (such as MLA), we observed that FP8 training support is still maturing. Even in later versions (e.g., 2.4.0 → 2.8.0), certain errors and limitations remain to be resolved.</li>
<li><strong>Conflicts with memory-optimization strategies</strong>: In RL training, enabling Optimizer CPU Offload can significantly reduce memory usage, but current TE does not support using it together with <code>--fp8-param-gather</code>. Because of this limitation, end-to-end FP8 training can end up consuming more memory than BF16 training with FP8 rollout, which needs further optimization from the community and maintainers.</li>
</ul>
<h2><a id="fp8--rl-attributing-abnormal-kl-loss" class="anchor" href="#fp8--rl-attributing-abnormal-kl-loss" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>FP8 + RL: Attributing Abnormal KL Loss</strong></h2>
<p>The <strong>InfiXAI Team</strong> has already successfully run full FP8 training on <strong>pre-training and fine-tuning tasks</strong> (see <a href="https://arxiv.org/html/2509.22536v4">Pre-training and Fine-tuning</a>). Building on this, we apply FP8 training to RL. Thanks to Miles' good support for Megatron FP8 training, we were able to run a series of FP8 RL experiments smoothly.</p>
<h3><a id="abnormal-initial-kl-loss" class="anchor" href="#abnormal-initial-kl-loss" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Abnormal Initial KL Loss</strong></h3>
<p>When we directly switched from BF16 to FP8 and started training, we observed a striking phenomenon: compared with BF16 training, FP8 training has a significantly higher KL loss at the first step. As shown below, the initial KL loss for <strong>FP8-TI</strong> is significantly higher than that of BF16 training with FP8 inference (T denotes Training, I denotes Inference):</p>
<p align="center">
  <img src="/images/blog/fp8-rl/5_KLloss.png" alt="Initial KL loss comparison" width="50%" />
</p>
<h3><a id="locating-the-source-of-error" class="anchor" href="#locating-the-source-of-error" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Locating the Source of Error</strong></h3>
<p>To understand why initial KL loss is higher, we analyze two potential error sources in the quantization process:</p>
<ol>
<li><strong>Error from quantized compute kernels</strong>: Numerical error from specific FP8 GEMM implementations.</li>
<li><strong>Intrinsic quantization error</strong>: Precision loss from quantization and dequantization themselves.</li>
</ol>
<p><strong>Error analysis of quantized compute kernels</strong></p>
<p>Initially, we suspected that the closed-source cuBLAS GEMM implementation used in TransformerEngine might be less accurate than the widely used open-source DeepGEMM, so we designed experiments to compare the precision of these two FP8 GEMM implementations against BF16. We evaluated their errors under various shapes (based on TE’s test cases), with results shown below:</p>
<table>
<thead>
<tr>
<th><strong>Kernel (M, K, N)</strong></th>
<th><strong>cuBLAS(TE)</strong></th>
<th><strong>DeepGEMM</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>128,128,128</td>
<td>0.00068</td>
<td>0.00036</td>
</tr>
<tr>
<td>256,128,256</td>
<td>0.00068</td>
<td>0.00037</td>
</tr>
<tr>
<td>320,128,336</td>
<td>0.000684</td>
<td>0.00037</td>
</tr>
<tr>
<td>320,64,336</td>
<td>0.00067</td>
<td>0.00024</td>
</tr>
<tr>
<td>320,256,336</td>
<td>0.00068</td>
<td>0.00048</td>
</tr>
<tr>
<td>1024,4096,1024</td>
<td>0.000681</td>
<td>0.00065</td>
</tr>
<tr>
<td>2048,2048,512</td>
<td>0.00068</td>
<td>0.00063</td>
</tr>
<tr>
<td>1024,1024,1024</td>
<td>0.000683</td>
<td>0.0006</td>
</tr>
</tbody>
</table>
<p>The results show that the errors of the two GEMM implementations are of the same order of magnitude with no significant difference, so replacing TE’s FP8 GEMM does not reduce the initial KL loss.</p>
<p><strong>Analysis of intrinsic quantization error</strong></p>
<p>For the second potential source, we designed a set of comparative experiments to isolate and validate the <strong>intrinsic</strong> error of quantization:</p>
<ul>
<li><strong>Baseline</strong>: Qwen3-4B on a single H800.</li>
<li><strong>Experimental modes</strong>:
<ol>
<li><strong>Baseline</strong>: Weights and inputs in BF16, using BF16 GEMM.</li>
<li><strong>FP8 Real Quant</strong>: Weights and inputs in FP8, using FP8 GEMM (e.g., DeepGEMM/cuBLAS GEMM; we mainly tested cuBLAS to avoid large changes to TE).</li>
<li><strong>FP8 Fake Quant</strong>: Weights and inputs kept in BF16, but we simulate the quantization process (quantize to FP8 then dequantize back to BF16), and finally use BF16 GEMM.</li>
</ol>
</li>
</ul>
<p>Based on these modes, we run two comparisons:</p>
<ul>
<li><strong>FP8 Real Quant vs. FP8 Fake Quant</strong>: To verify the precision of the FP8 GEMM kernels (cuBLAS), isolating any additional error from the implementation.</li>
<li><strong>Baseline vs. FP8 Fake Quant</strong>: To ignore GEMM kernel effects and focus on the intrinsic error introduced by quantization/dequantization themselves.</li>
</ul>
<p><strong>Metric</strong>: We collect the output differences (Diff) of all GEMM operations at the beginning of RL training (Step 0 and Step 1).</p>
<p><strong>Results</strong>:</p>
<p>The figure below visualizes the error distribution of all GEMM outputs over one full forward + backward pass, in execution order:</p>
<p align="center">
  <img src="/images/blog/fp8-rl/6_FP8_quant_error.png" alt="FP8 quantization error distribution" width="50%" />
</p>
<blockquote>
<p>The figure shows how GEMM output errors evolve over one full iteration.</p>
<ul>
<li><strong>Grey/high points (Baseline vs. FP8 Fake Quant)</strong>: Represent error from quantization itself. We can see significant differences between the BF16 baseline and fake quantization.</li>
<li><strong>Green/low points (FP8 Real Quant vs. FP8 Fake Quant)</strong>: Represent error from the kernel implementation. These differences are extremely small, nearly zero.</li>
</ul>
</blockquote>
<p>From this we conclude:</p>
<ul>
<li><strong>Error mainly comes from the quantization principle, not the kernel implementation</strong>: Both Fake Quant and Real Quant differ significantly from the baseline (by two orders of magnitude), strongly indicating that <strong>the dominant error source is the lossy quantization/dequantization itself</strong>, rather than computation.</li>
<li><strong>FP8 GEMM kernels are highly reliable</strong>: The tiny difference between Real Quant and Fake Quant outputs shows that the cuBLAS FP8 GEMM we use in TE is extremely accurate and closely matches the ideal mathematical simulation, making it safe for production.</li>
</ul>
<h3><a id="how-quantization-error-leads-to-training-anomalies" class="anchor" href="#how-quantization-error-leads-to-training-anomalies" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>How Quantization Error Leads to Training Anomalies</strong></h3>
<p>Based on the above experiments, we hypothesize:</p>
<ol>
<li>The main error in training is already introduced at the quantization step and is substantial.</li>
<li>The higher initial KL loss in FP8 training likely comes from this quantization error.</li>
<li>In hybrid BF16 training + FP8 inference (rollout), the same quantization error also causes train–inference inconsistency.</li>
</ol>
<p>To validate these hypotheses, we modified Transformer Engine (TE) and designed the following experiments:</p>
<ul>
<li><strong>Baseline</strong>: Qwen3-4B on an H800 cluster.</li>
<li><strong>Cases</strong>:
<ul>
<li><strong>Case 1</strong>: BF16 training, FP8 rollout (inference).</li>
<li><strong>Case 2</strong>: BF16 training, FP8 rollout; in the forward pass during training, quantize BF16 weights and activations to FP8 then dequantize back to BF16 before running BF16 GEMM.</li>
<li><strong>Case 3</strong>: BF16 training, FP8 rollout; in both forward and backward passes during training, quantize both input matrices A and B to FP8 then dequantize back to BF16 before BF16 GEMM.</li>
<li><strong>Case 4 (FP8-TI)</strong>: FP8 training, FP8 rollout.</li>
</ul>
</li>
</ul>
<p><strong>Validating hypothesis 2 — KL-loss analysis</strong></p>
<p>The figure below shows KL-loss curves for the four cases. We see that Case 2, Case 3, and Case 4 (FP8-TI) have nearly identical KL loss at step 1, all significantly higher than Case 1:</p>
<p align="center">
  <img src="/images/blog/fp8-rl/7_KLloss2.png" alt="KL-loss comparison under different cases" width="50%" />
</p>
<p><strong>Validating hypothesis 3 — TIS-clipfrac analysis</strong></p>
<p>We introduce <strong>clipfrac</strong> from <strong>Truncated Importance Sampling (TIS)</strong> to validate hypothesis 3. This metric reflects the degree of off-policy training, i.e., the consistency between the model used for training and for generating experience. Higher clipfrac generally indicates more severe train–inference inconsistency.</p>
<p align="center">
    <img src="/images/blog/fp8-rl/8_TIS.png" alt="TIS-clipfrac comparison under different cases" width="50%" />
</p>
<p>From the figure we see that Case 2, Case 3, and Case 4 (FP8-TI) have clipfrac values of roughly the same order, all significantly lower than Case 1. This confirms:</p>
<ol>
<li>The root cause of the elevated initial KL loss is quantization error.</li>
<li><strong>FP8-TI (Case 4)</strong> can significantly alleviate train–inference inconsistency compared with the hybrid BF16 training + FP8 rollout (Case 1).</li>
<li>For training bias, quantization error in the forward pass matters more than in the backward pass (as shown by the similarity between Case 2 and Case 3). Similarly, for train–inference consistency, forward quantization error is the primary factor.</li>
</ol>
<h2><a id="applying-fp8-to-moe-rl-experiments-and-validation" class="anchor" href="#applying-fp8-to-moe-rl-experiments-and-validation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Applying FP8 to MoE RL: Experiments and Validation</strong></h2>
<p>Dense-model experiments demonstrate that <strong>FP8-TI</strong> effectively suppresses train–inference inconsistency. Building on this, the <strong>Ant Group AQ Team</strong> extended the study to MoE models in RL to evaluate whether <strong>FP8-TI</strong> works well for more complex architectures. We find that <strong>FP8-TI</strong>:</p>
<ol>
<li><strong>Reduces TIS clip fraction</strong>: Its TIS-clipfrac is significantly lower than that of BF16 Train / FP8 Rollout, meaning fewer clipped updates and higher training stability.</li>
<li><strong>Narrows the train–rollout log-probability gap</strong>: Compared with BF16 Train / FP8 Rollout, this FP8 scheme yields smaller and more stable differences between training and rollout log probabilities.</li>
</ol>
<h3><a id="moe-experiment-design" class="anchor" href="#moe-experiment-design" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>MoE Experiment Design</h3>
<p>To isolate variables for clean comparison, we set up two experimental schemes:</p>
<ul>
<li><strong>Case 1 (mixed precision)</strong>: BF16 training, FP8 rollout.</li>
<li><strong>Case 2 (unified precision)</strong>: FP8 training, FP8 rollout.</li>
</ul>
<p><strong>Key metrics</strong>:</p>
<ul>
<li><strong>TIS clip fraction (TIS-clipfrac)</strong>: Measures off-policy training stability; lower is better.</li>
<li><strong>Absolute difference between train and rollout log probabilities (train_rollout_logprob_abs_diff)</strong>: Measures how consistent model behavior is between training and rollout; smaller and more stable is better.</li>
</ul>
<h3><a id="moe-results-and-analysis" class="anchor" href="#moe-results-and-analysis" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>MoE Results and Analysis</h3>
<p><strong>Qwen3-30B-A3B</strong></p>
<ul>
<li><strong>Setup</strong>: 2× H20 servers.</li>
</ul>
<p>On a 30B-scale MoE model, the results clearly show the advantages of <strong>FP8-TI</strong>:</p>
<ul>
<li><strong>Lower TIS-clipfrac</strong>: <strong>FP8-TI</strong> achieves significantly lower TIS-clipfrac than the BF16 Train / FP8 Rollout baseline, indicating fewer clipped updates and more stable training.</li>
<li><strong>Smaller train–rollout log-probability gap</strong>: <strong>FP8-TI</strong> produces a narrower and more stable range for Train_rollout_logprob_abs_diff, indicating more consistent behavior between training and inference.</li>
</ul>
<p align="center">
  <img src="/images/blog/fp8-rl/9_1_TIS.png" alt="Qwen3-30B-A3B TIS-clipfrac" width="49%" />
  <img src="/images/blog/fp8-rl/9_2_rollout_logprob_abs_diff.png" alt="Qwen3-30B-A3B train_rollout_logprob_abs_diff" width="49%" />
</p>
<p><strong>Qwen3-235B-A22B</strong></p>
<ul>
<li><strong>Setup</strong>: 16× H20 servers.</li>
</ul>
<p>To evaluate scalability, we replicated the experiments on a 235B-scale model and obtained consistent conclusions:</p>
<ul>
<li><strong>Consistent improvements in TIS-clipfrac and train–rollout discrepancy</strong>: As shown below, even at 235B scale, <strong>FP8-TI</strong> continues to reduce TIS-clipfrac and Train_rollout_logprob_abs_diff compared with BF16 Train / FP8 Rollout, demonstrating good scalability.</li>
</ul>
<p align="center">
  <img src="/images/blog/fp8-rl/10_1_TIS.png" alt="Qwen3-235B-A22B TIS-clipfrac" width="49%" />
  <img src="/images/blog/fp8-rl/10_2_rollout_logprob_abs_diff.png" alt="Qwen3-235B-A22B train_rollout_logprob_abs_diff" width="49%" />
</p>
<p><strong>Conclusion</strong>: For MoE RL tasks, using <strong>unified FP8 for both training and inference</strong> improves training stability and effectively suppresses train–inference inconsistency compared with BF16 Train / FP8 Rollout. This advantage is consistently observed from 30B to 235B MoE models.</p>
<h2><a id="effect-of-moe-model-scale-on-traininference-inconsistency" class="anchor" href="#effect-of-moe-model-scale-on-traininference-inconsistency" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>Effect of MoE Model Scale on Train–Inference Inconsistency</strong></h2>
<p>We further investigate how MoE model size affects train–inference inconsistency under the mixed-precision setting (BF16 Train / FP8 Rollout). Experiments show that <strong>as MoE model size increases, train–inference inconsistency becomes more severe</strong>.</p>
<p>As shown below, from 30B up to 1T, both <strong>TIS-clipfrac</strong> and <strong>Train_rollout_logprob_abs_diff</strong> increase significantly. This suggests that for BF16 Train / FP8 Rollout, larger models tend to suffer more severe train–inference inconsistency, indirectly highlighting the importance of unified-precision schemes such as <strong>FP8-TI</strong>.</p>
<p align="center">
  <img src="/images/blog/fp8-rl/11_1_TIS.png" alt="TIS-clipfrac under different model scales" width="49%" />
  <img src="/images/blog/fp8-rl/11_2_rollout_logprob_abs_diff.png" alt="train_rollout_logprob_abs_diff under different model scales" width="49%" />
</p>
<h2><a id="future-work" class="anchor" href="#future-work" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Future Work</h2>
<p>Thank you for reading. We see several directions worth further exploration:</p>
<ol>
<li>Study train–inference inconsistency more deeply, analyze its root causes, and explore better solutions.</li>
<li>Investigate quantization strategies more thoroughly, understand how quantization error arises, and design schemes with lower error.</li>
<li>Improve low-precision training efficiency via better algorithms, frameworks, and hardware–software co-design, hiding the latency of kernel launches and quantization and truly realizing acceleration for both training and inference.</li>
</ol>
<h2><a id="acknowledgments" class="anchor" href="#acknowledgments" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgments</h2>
<ol>
<li>InfiXAI Team: Congkai Xie, Mingfa Feng, Shuo Cai</li>
<li>Ant Group AQ Team: Yanan Gao, Zhiling Ye, Hansong Xiao</li>
<li>SGLang RL Team: JiLi, Yefei Chen, Xi Chen, Zilin Zhu</li>
<li>Miles Team: Chenyang Zhao</li>
<li>NVIDIA: Juan Yu, NeMo-RL Team</li>
</ol>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Unified FP8: Moving Beyond Mixed Precision for Stable and Accelerated MoE RL","author":"InfiXAI Team, Ant Group AQ Team, SGLang RL Team, Miles Team","date":"November 25, 2025","previewImg":"/images/blog/fp8-rl/3_Megatron.png"},"content":" \n\u003e TL;DR: We have implemented fully FP8-based sampling and training in RL. Experiments show that for MoE models, the larger the model, the more severe the train–inference discrepancy becomes when using BF16 training with FP8 rollout. In contrast, using unified FP8 for both training and rollout effectively eliminates train–inference inconsistency caused by quantization error, improving both the speed and stability of RL training.\n\nSGLang RL Team and the Miles community have conducted some interesting explorations around RL training stability and acceleration:\n\n[Aligning the SGLang and FSDP backends](https://github.com/radixark/miles/tree/main/examples/true_on_policy) for **strictly zero KL divergence**\n\n[**Speculative Decoding**](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/slime/spec/readme-en.md) with online SFT for the draft model\n\nBuilding on this, we now share a new progress that balances both stability and performance—**implementing an end-to-end FP8 pipeline for RL training and sampling**. FP8 RL training for Qwen3-4B and Qwen3-30B-A3B has been [fully supported in miles](https://github.com/radixark/miles/tree/main/examples/low_precision) and is ready to use out of the box.\n\nThis work is jointly completed by the **InfiXAI Team, Ant Group AQ Team, SGLang RL Team, and Miles Team**. Special thanks to **DataCrunch** for compute sponsorship and to **NVIDIA** for technical support on Transformer Engine (TE).\n\n## Hardware Foundations of FP8 Training\n\n### **Tensor Cores and Low-Precision Support**\n\nLow-precision computing is a gem of hardware–software co-design. We first introduce its hardware foundation—**Tensor Cores**, a type of **GPU hardware acceleration unit** designed specifically for **large-scale matrix multiplication and accumulation**, the core computation in deep learning. Compared with traditional CUDA cores, Tensor Cores can process low-precision data formats (such as FP16, BF16, FP8) with **much higher throughput**. Their evolution began with basic FMA (fused multiply–add) instructions and early vectorization through DP4A, but the real milestone came with the Volta architecture, which first introduced Tensor Cores as dedicated units for large-scale matrix operations. Since then, Ampere, Hopper, and the latest Blackwell architectures have continued to push this idea further:\n\n- **Scaling up**: Letting Tensor Cores process larger matrices per operation, thereby improving compute-to-memory ratios.\n- **Lowering precision**: Continuously adding support for FP/BF16, FP8, and even lower-precision data formats.\n\n| Arch | FP64 | [F16](https://newsletter.semianalysis.com/p/nvidia-tensor-core-evolution-from-volta-to-blackwell) | INT8 | INT4 | FP8 | MXFP |\n| --- | --- | --- | --- | --- | --- | --- |\n| Volta | ❌ | ✅ FP16 | ❌ | ❌ | ❌ | ❌ |\n| Turing | ❌ | ✅ FP16 | ✅ | ✅ | ❌ | ❌ |\n| Ampere | ✅ | ✅ FP16/BF16 | ✅ | ✅ | ❌ | ❌ |\n| Hopper | ✅ | ✅ FP16/BF16 | ✅ | ❌ | ✅ [(accumulation precision only supports FP22)](https://arxiv.org/html/2505.09343v1) | ❌ |\n| Blackwell | ✅ | ✅ FP16/BF16 | ✅ | ❌ | ✅ | ✅ MXFP(8/6/4)\u003cbr\u003eNVFP4 |\n| Blackwell Ultra | ✅ (reduced FLOPs) | ✅ FP16/BF16 | ✅ (reduced FLOPs) | ❌ | ✅ | ✅ MXFP(8/6/4)\u003cbr\u003eNVFP4 |\n\n\u003e Figure source: [zartbot](https://mp.weixin.qq.com/s?__biz=MzUxNzQ5MTExNw==\u0026mid=2247496740\u0026idx=1\u0026sn=c9403138fa59d126fe6cfda19d9b2f76\u0026chksm=f995e4e6cee26df07bf7101b58cbdfdf80d577c67122304482e3e788edfa74a71135dbf77d36\u0026cur_album_id=3289258526057463810\u0026scene=189#wechat_redirect), [SemiAnalysis](https://newsletter.semianalysis.com/p/nvidia-tensor-core-evolution-from-volta-to-blackwell)\n\nUnder this hardware trend, using lower precision for storage and computation becomes increasingly attractive. Concretely, lower-precision floating-point formats offer several potential advantages:\n\n1. **Significantly reduced memory footprint**: Compared with mainstream BF16, FP8 can theoretically halve the memory consumed by model weights and activations, directly alleviating ever-growing VRAM pressure.\n2. **Theoretically 2× compute throughput**: On mainstream GPUs (e.g., H100 SXM), FP8 Tensor Cores offer up to 1979 TFLOPS of theoretical performance, twice that of BF16 units (989 TFLOPS). This substantial performance gain is a key driver behind FP8 training.\n3. **Mitigated memory bandwidth bottlenecks**: With more compact data representation, less data must be transferred from GPU HBM to compute cores. This means less time spent on data movement and effectively reduces memory-bandwidth pressure.\n\n### **FP8 Formats**\n\nFP8 is a floating-point format that uses 8 bits to represent values. Compared with FP32 (32 bits) and FP16/BF16 (16 bits), FP8 can reduce storage and transmission costs for the same amount of data to 1/4 or 1/2, greatly easing VRAM and bandwidth bottlenecks and improving training and inference performance. Currently, there are two major FP8 formats:\n\n- **E4M3**: 4-bit exponent + 3-bit mantissa. Smaller dynamic range but higher precision.\n- **E5M2**: 5-bit exponent + 2-bit mantissa. Larger dynamic range but lower precision.\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/fp8-rl/1_E4vsE5.png\" alt=\"FP8 E4M3 vs E5M2\" width=\"80%\" /\u003e\n\u003c/p\u003e\n\n\u003e Figure source: [OCP whitepaper](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf)\n\nThis design allows FP8 to maintain sufficient numerical range and precision while maximizing hardware throughput.\n\n### FP8 Scale Selection\n\n| **Dimension** | **FP32 Scale (full-precision scaling factor)** | **E8M0 Scale (exponent-only scaling)** |\n| --- | --- | --- |\n| **Format definition** | **FP32** (IEEE 754 single-precision float) | **E8M0** (8-bit exponent, 0-bit mantissa) |\n| **Numeric properties** | Can represent real numbers with arbitrary precision. | Only supports **powers of 2**, such as 1, 2, 0.5; cannot represent values like 1.5. |\n| **Core idea** | Manage scaling factors in high precision to ensure numerical stability during training. | Bring scaling factors into the low-precision regime and leverage bit operations for efficiency. |\n| **Main advantages** | 1. **High precision, stable training**: Accurately captures dynamic ranges, reduces quantization error, and prevents divergence.\u003cbr\u003e2. **Broad support**: Default choice in mainstream libraries such as NVIDIA Transformer Engine; mature ecosystem. | 1. **Extremely hardware-friendly**: Scaling can be implemented as simple **bit shifts**, which are very fast and energy-efficient.\u003cbr\u003e2. **Unified pipeline**: The entire pipeline (including scale) runs in 8 bits, simplifying hardware design. |\n| **Main disadvantages** | 1. **Storage overhead**: Each quantized tensor needs to store one extra FP32 scale value, consuming some VRAM.\u003cbr\u003e2. **Compute overhead**: Scale calculations and conversions must be done in FP32. | 1. **Precision-loss risk**: Forcing rounding to powers of 2 introduces quantization noise, which can accumulate during backprop and cause divergence.\u003cbr\u003e2. **Limited dynamic-range resolution**: Harder to finely adapt to complex tensor distributions. |\n| **Summary** | Currently the most common and safest scheme in industry. | Sacrifices some precision in exchange for extreme hardware efficiency. |\n\nAfter a comprehensive evaluation, we ultimately chose **FP32** as the scale precision during training. The reasons are:\n\n1. **Precision alignment and training stability**: FP32 scales provide fine-grained numerical scaling that captures tensor dynamic ranges and keeps FP8 training loss curves as close as possible to the **BF16** baseline.\n2. **Consistency with inference ecosystems**: Mainstream inference models also use FP32 as the quantization scale format.\n3. **Real-world hardware benefits**:\n   - **Hopper (H100/H800)**: Although it supports FP8 Tensor Cores, it has no dedicated compute units for E8M0 scaling.\n   - **Blackwell (B100/B200)**: Introduces support for MXFP8 (micro-scaling), which provides hardware acceleration for block-level scaling like E8M0 (see [**arXiv:2506.08027**](https://arxiv.org/abs/2506.08027)).\n\nTherefore, under current H-series clusters, forcing the use of E8M0 not only fails to deliver clear speedups, but may also introduce additional software-emulation overhead and precision risks.\n\n### FP8 Quantization\n\nCommon quantization strategies include **per-tensor**, **per-block**, and **per-token**. Regardless of granularity, quantization usually follows two simple steps:\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/fp8-rl/2_size.png\" alt=\"FP8 quantization flow\" width=\"80%\" /\u003e\n\u003c/p\u003e\n\n\u003e Figure source: [InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models](https://arxiv.org/html/2509.22536v4)\n\n**Step 1: Compute the scaling factor $S$**\n\nTake the maximum absolute value $\\max|X|$ in a given tensor (or block) and divide it by the maximum representable FP8 value $V_{\\max}$:\n\n$$\nS = \\frac{\\max|X|}{V_{\\max}}\n$$\n\n**Step 2: Compute the quantized value $Q$**\n\nUsing the scaling factor **$S$**, divide each element $x$ in the original tensor $X$ by **$S$** and round to the nearest integer to obtain quantized values:\n\n$$\nQ(x) = \\mathrm{round}\\left(\\frac{x}{S}\\right)\n$$\n\nBecause FP8 has lower precision than FP16/BF16, we must trade off between training stability and efficiency in practice, so forward and backward passes often use different quantization strategies and granularities:\n\n- **Activations**: Typically use per-token quantization. Activations often contain significant outliers; finer quantization granularity can localize the effect of outliers and better preserve overall precision.\n- **Weights**: Typically use per-block quantization. After convergence, weight distributions are usually smooth (close to Gaussian) with few outliers, but are highly sensitive to quantization error. Blockwise quantization (e.g., block_size × block_size) maintains precision while working well with hardware optimizations, balancing compute efficiency and memory savings.\n- **Gradients**: Typically use per-token quantization. Gradients have large dynamic-range variation but relatively low absolute precision requirements. Historically, most schemes used **per-tensor E5M2** to ensure dynamic range, but DeepSeek-V3 shows that fine-grained E4M3 can also balance precision and range.\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/fp8-rl/3_Megatron.png\" alt=\"Mixed-granularity FP8 quantization in Megatron\" width=\"80%\" /\u003e\n\u003c/p\u003e\n\n\u003e Figure source: [InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models](https://arxiv.org/html/2509.22536v4)\n\nThe figure shows the mixed-granularity FP8 strategy used in Megatron compared to a standard BF16 pipeline. In the FP8 pipeline, different quantization methods are applied: weights use per-block quantization (blue), while activations use per-token quantization (purple). The figure presents the full training process, including forward propagation (FProp), weight-gradient computation (Wgrad), and input-gradient computation (Dgrad), and details the FProp workflow.\n\n## Challenges of FP8 Training\n\nAlthough FP8 shows great potential, in real engineering practice—especially when combining Megatron-Core and TransformerEngine (TE)—we encounter three main challenges: **memory/efficiency not meeting expectations, difficulty aligning precision, and stability issues in the framework itself.** We refer to our unified FP8 training-and-inference setup as **FP8-TI (FP8 Training \u0026 Inference)**.\n\n### **Memory and Compute Efficiency: Theory vs. Reality**\n\nIn practice, the memory savings and speedups brought by FP8 are often less significant than theory suggests, mainly due to:\n\n- **Limited memory optimization**:\n  - **Redundant weight copies**: To speed up backprop, TransformerEngine [keeps an extra transposed copy of quantized weights](https://github.com/NVIDIA/TransformerEngine/blob/main/transformer_engine/common/transpose/quantize_transpose_square_blockwise.cu#L481). This prevents weight memory usage from being reduced by the expected factor of 2.\n  - **High-precision activation copies**: In the forward pass of attention and activation layers, frameworks typically retain a high-precision copy of activations for accurate gradient computation later. FP8 does not reduce this portion of memory usage.\n- **Compute-efficiency bottlenecks**:\n  - **Performance degradation with small batch sizes**: When batch_size is small, FP8 training may fail to fully utilize GPU compute units and can even underperform BF16. The root cause is that FP8 introduces extra quantization and dequantization operations, which add CPU overhead. In Agentic RL scenarios, which typically use small batch sizes (e.g., batch_size=4), this issue is particularly pronounced—frequent CPU overhead can make FP8 training slower than BF16. (As shown below, GPU kernels are not densely scheduled; often the GPU has already finished the previous work but the next kernel launch is delayed because the system is CPU-bound.)\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/fp8-rl/4_cpu_bound.png\" alt=\"CPU bound for FP8 training\" width=\"50%\" /\u003e\n\u003c/p\u003e\n\n\u003e Figure: CPU-bound behavior in FP8 training\n\n### **Precision Alignment: Cumulative Error Matters**\n\nThe low-precision nature of FP8 inherently introduces numerical discrepancies relative to BF16, which can be amplified in deep models and cause training-instability problems:\n\n- **Intrinsic quantization error**: Even if accumulation is performed in FP32, quantizing FP8 inputs for a single GEMM operation introduces error. Experiments show that compared with BF16 GEMM, the typical error is about 0.0007.\n- **Layer-wise cumulative effect**: In deep Transformer models, these small errors accumulate layer by layer during forward and backward passes:\n  - **In pre-training and fine-tuning (SFT)**: Gradients are mainly dominated by the log probabilities of ground-truth labels. With fine-grained blockwise quantization, errors can usually be kept within an acceptable range and models are unlikely to collapse.\n  - **In reinforcement learning (RL)**: Gradients are often determined by the difference between two log probabilities from two forward passes. In this case, accumulated FP8 error can be amplified, causing gradients to deviate from their ideal direction and impacting convergence efficiency—or even pushing the optimization “in the wrong direction” (as discussed later).\n\n### **Framework Adaptation: TransformerEngine Version Compatibility**\n\nBesides algorithmic challenges, there is room for improvement in how Megatron-Core integrates with **Transformer Engine (TE)**, especially given TE’s rapid iteration:\n\n- **Version dependencies and migration overhead**: TE’s fast iteration brings new features but also strict version dependencies. In practice, we found that even the same training script can yield different numerical behaviors across TE versions, and sometimes code adjustments are required to avoid issues such as NaNs.\n- **Maturity for specific architectures**: Full FP8 support for **all mainstream model architectures** is an ongoing process. For some **nonstandard or newer components** (such as MLA), we observed that FP8 training support is still maturing. Even in later versions (e.g., 2.4.0 → 2.8.0), certain errors and limitations remain to be resolved.\n- **Conflicts with memory-optimization strategies**: In RL training, enabling Optimizer CPU Offload can significantly reduce memory usage, but current TE does not support using it together with `--fp8-param-gather`. Because of this limitation, end-to-end FP8 training can end up consuming more memory than BF16 training with FP8 rollout, which needs further optimization from the community and maintainers.\n\n## **FP8 + RL: Attributing Abnormal KL Loss**\n\nThe **InfiXAI Team** has already successfully run full FP8 training on **pre-training and fine-tuning tasks** (see [Pre-training and Fine-tuning](https://arxiv.org/html/2509.22536v4)). Building on this, we apply FP8 training to RL. Thanks to Miles' good support for Megatron FP8 training, we were able to run a series of FP8 RL experiments smoothly.\n\n### **Abnormal Initial KL Loss**\n\nWhen we directly switched from BF16 to FP8 and started training, we observed a striking phenomenon: compared with BF16 training, FP8 training has a significantly higher KL loss at the first step. As shown below, the initial KL loss for **FP8-TI** is significantly higher than that of BF16 training with FP8 inference (T denotes Training, I denotes Inference):\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/fp8-rl/5_KLloss.png\" alt=\"Initial KL loss comparison\" width=\"50%\" /\u003e\n\u003c/p\u003e\n\n### **Locating the Source of Error**\n\nTo understand why initial KL loss is higher, we analyze two potential error sources in the quantization process:\n\n1. **Error from quantized compute kernels**: Numerical error from specific FP8 GEMM implementations.\n2. **Intrinsic quantization error**: Precision loss from quantization and dequantization themselves.\n\n**Error analysis of quantized compute kernels**\n\nInitially, we suspected that the closed-source cuBLAS GEMM implementation used in TransformerEngine might be less accurate than the widely used open-source DeepGEMM, so we designed experiments to compare the precision of these two FP8 GEMM implementations against BF16. We evaluated their errors under various shapes (based on TE’s test cases), with results shown below:\n\n| **Kernel (M, K, N)** | **cuBLAS(TE)** | **DeepGEMM** |\n| --- | --- | --- |\n| 128,128,128 | 0.00068 | 0.00036 |\n| 256,128,256 | 0.00068 | 0.00037 |\n| 320,128,336 | 0.000684 | 0.00037 |\n| 320,64,336 | 0.00067 | 0.00024 |\n| 320,256,336 | 0.00068 | 0.00048 |\n| 1024,4096,1024 | 0.000681 | 0.00065 |\n| 2048,2048,512 | 0.00068 | 0.00063 |\n| 1024,1024,1024 | 0.000683 | 0.0006 |\n\nThe results show that the errors of the two GEMM implementations are of the same order of magnitude with no significant difference, so replacing TE’s FP8 GEMM does not reduce the initial KL loss.\n\n**Analysis of intrinsic quantization error**\n\nFor the second potential source, we designed a set of comparative experiments to isolate and validate the **intrinsic** error of quantization:\n\n- **Baseline**: Qwen3-4B on a single H800.\n- **Experimental modes**:\n  1. **Baseline**: Weights and inputs in BF16, using BF16 GEMM.\n  2. **FP8 Real Quant**: Weights and inputs in FP8, using FP8 GEMM (e.g., DeepGEMM/cuBLAS GEMM; we mainly tested cuBLAS to avoid large changes to TE).\n  3. **FP8 Fake Quant**: Weights and inputs kept in BF16, but we simulate the quantization process (quantize to FP8 then dequantize back to BF16), and finally use BF16 GEMM.\n\nBased on these modes, we run two comparisons:\n\n- **FP8 Real Quant vs. FP8 Fake Quant**: To verify the precision of the FP8 GEMM kernels (cuBLAS), isolating any additional error from the implementation.\n- **Baseline vs. FP8 Fake Quant**: To ignore GEMM kernel effects and focus on the intrinsic error introduced by quantization/dequantization themselves.\n\n**Metric**: We collect the output differences (Diff) of all GEMM operations at the beginning of RL training (Step 0 and Step 1).\n\n**Results**:\n\nThe figure below visualizes the error distribution of all GEMM outputs over one full forward + backward pass, in execution order:\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/fp8-rl/6_FP8_quant_error.png\" alt=\"FP8 quantization error distribution\" width=\"50%\" /\u003e\n\u003c/p\u003e\n\n\u003e The figure shows how GEMM output errors evolve over one full iteration.\n\u003e\n\u003e - **Grey/high points (Baseline vs. FP8 Fake Quant)**: Represent error from quantization itself. We can see significant differences between the BF16 baseline and fake quantization.\n\u003e - **Green/low points (FP8 Real Quant vs. FP8 Fake Quant)**: Represent error from the kernel implementation. These differences are extremely small, nearly zero.\n\nFrom this we conclude:\n\n- **Error mainly comes from the quantization principle, not the kernel implementation**: Both Fake Quant and Real Quant differ significantly from the baseline (by two orders of magnitude), strongly indicating that **the dominant error source is the lossy quantization/dequantization itself**, rather than computation.\n- **FP8 GEMM kernels are highly reliable**: The tiny difference between Real Quant and Fake Quant outputs shows that the cuBLAS FP8 GEMM we use in TE is extremely accurate and closely matches the ideal mathematical simulation, making it safe for production.\n\n### **How Quantization Error Leads to Training Anomalies**\n\nBased on the above experiments, we hypothesize:\n\n1. The main error in training is already introduced at the quantization step and is substantial.\n2. The higher initial KL loss in FP8 training likely comes from this quantization error.\n3. In hybrid BF16 training + FP8 inference (rollout), the same quantization error also causes train–inference inconsistency.\n\nTo validate these hypotheses, we modified Transformer Engine (TE) and designed the following experiments:\n\n- **Baseline**: Qwen3-4B on an H800 cluster.\n- **Cases**:\n  - **Case 1**: BF16 training, FP8 rollout (inference).\n  - **Case 2**: BF16 training, FP8 rollout; in the forward pass during training, quantize BF16 weights and activations to FP8 then dequantize back to BF16 before running BF16 GEMM.\n  - **Case 3**: BF16 training, FP8 rollout; in both forward and backward passes during training, quantize both input matrices A and B to FP8 then dequantize back to BF16 before BF16 GEMM.\n  - **Case 4 (FP8-TI)**: FP8 training, FP8 rollout.\n\n**Validating hypothesis 2 — KL-loss analysis**\n\nThe figure below shows KL-loss curves for the four cases. We see that Case 2, Case 3, and Case 4 (FP8-TI) have nearly identical KL loss at step 1, all significantly higher than Case 1:\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/fp8-rl/7_KLloss2.png\" alt=\"KL-loss comparison under different cases\" width=\"50%\" /\u003e\n\u003c/p\u003e\n\n**Validating hypothesis 3 — TIS-clipfrac analysis**\n\nWe introduce **clipfrac** from **Truncated Importance Sampling (TIS)** to validate hypothesis 3. This metric reflects the degree of off-policy training, i.e., the consistency between the model used for training and for generating experience. Higher clipfrac generally indicates more severe train–inference inconsistency.\n\n\u003cp align=\"center\"\u003e\n    \u003cimg src=\"/images/blog/fp8-rl/8_TIS.png\" alt=\"TIS-clipfrac comparison under different cases\" width=\"50%\" /\u003e\n\u003c/p\u003e\n\nFrom the figure we see that Case 2, Case 3, and Case 4 (FP8-TI) have clipfrac values of roughly the same order, all significantly lower than Case 1. This confirms:\n\n1. The root cause of the elevated initial KL loss is quantization error.\n2. **FP8-TI (Case 4)** can significantly alleviate train–inference inconsistency compared with the hybrid BF16 training + FP8 rollout (Case 1).\n3. For training bias, quantization error in the forward pass matters more than in the backward pass (as shown by the similarity between Case 2 and Case 3). Similarly, for train–inference consistency, forward quantization error is the primary factor.\n\n## **Applying FP8 to MoE RL: Experiments and Validation**\n\nDense-model experiments demonstrate that **FP8-TI** effectively suppresses train–inference inconsistency. Building on this, the **Ant Group AQ Team** extended the study to MoE models in RL to evaluate whether **FP8-TI** works well for more complex architectures. We find that **FP8-TI**:\n\n1. **Reduces TIS clip fraction**: Its TIS-clipfrac is significantly lower than that of BF16 Train / FP8 Rollout, meaning fewer clipped updates and higher training stability.\n2. **Narrows the train–rollout log-probability gap**: Compared with BF16 Train / FP8 Rollout, this FP8 scheme yields smaller and more stable differences between training and rollout log probabilities.\n\n### MoE Experiment Design\n\nTo isolate variables for clean comparison, we set up two experimental schemes:\n\n- **Case 1 (mixed precision)**: BF16 training, FP8 rollout.\n- **Case 2 (unified precision)**: FP8 training, FP8 rollout.\n\n**Key metrics**:\n\n- **TIS clip fraction (TIS-clipfrac)**: Measures off-policy training stability; lower is better.\n- **Absolute difference between train and rollout log probabilities (train_rollout_logprob_abs_diff)**: Measures how consistent model behavior is between training and rollout; smaller and more stable is better.\n\n### MoE Results and Analysis\n\n**Qwen3-30B-A3B**\n\n- **Setup**: 2× H20 servers.\n\nOn a 30B-scale MoE model, the results clearly show the advantages of **FP8-TI**:\n\n- **Lower TIS-clipfrac**: **FP8-TI** achieves significantly lower TIS-clipfrac than the BF16 Train / FP8 Rollout baseline, indicating fewer clipped updates and more stable training.\n- **Smaller train–rollout log-probability gap**: **FP8-TI** produces a narrower and more stable range for Train_rollout_logprob_abs_diff, indicating more consistent behavior between training and inference.\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/fp8-rl/9_1_TIS.png\" alt=\"Qwen3-30B-A3B TIS-clipfrac\" width=\"49%\" /\u003e\n  \u003cimg src=\"/images/blog/fp8-rl/9_2_rollout_logprob_abs_diff.png\" alt=\"Qwen3-30B-A3B train_rollout_logprob_abs_diff\" width=\"49%\" /\u003e\n\u003c/p\u003e\n\n**Qwen3-235B-A22B**\n\n- **Setup**: 16× H20 servers.\n\nTo evaluate scalability, we replicated the experiments on a 235B-scale model and obtained consistent conclusions:\n\n- **Consistent improvements in TIS-clipfrac and train–rollout discrepancy**: As shown below, even at 235B scale, **FP8-TI** continues to reduce TIS-clipfrac and Train_rollout_logprob_abs_diff compared with BF16 Train / FP8 Rollout, demonstrating good scalability.\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/fp8-rl/10_1_TIS.png\" alt=\"Qwen3-235B-A22B TIS-clipfrac\" width=\"49%\" /\u003e\n  \u003cimg src=\"/images/blog/fp8-rl/10_2_rollout_logprob_abs_diff.png\" alt=\"Qwen3-235B-A22B train_rollout_logprob_abs_diff\" width=\"49%\" /\u003e\n\u003c/p\u003e\n\n**Conclusion**: For MoE RL tasks, using **unified FP8 for both training and inference** improves training stability and effectively suppresses train–inference inconsistency compared with BF16 Train / FP8 Rollout. This advantage is consistently observed from 30B to 235B MoE models.\n\n## **Effect of MoE Model Scale on Train–Inference Inconsistency**\n\nWe further investigate how MoE model size affects train–inference inconsistency under the mixed-precision setting (BF16 Train / FP8 Rollout). Experiments show that **as MoE model size increases, train–inference inconsistency becomes more severe**.\n\nAs shown below, from 30B up to 1T, both **TIS-clipfrac** and **Train_rollout_logprob_abs_diff** increase significantly. This suggests that for BF16 Train / FP8 Rollout, larger models tend to suffer more severe train–inference inconsistency, indirectly highlighting the importance of unified-precision schemes such as **FP8-TI**.\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/fp8-rl/11_1_TIS.png\" alt=\"TIS-clipfrac under different model scales\" width=\"49%\" /\u003e\n  \u003cimg src=\"/images/blog/fp8-rl/11_2_rollout_logprob_abs_diff.png\" alt=\"train_rollout_logprob_abs_diff under different model scales\" width=\"49%\" /\u003e\n\u003c/p\u003e\n\n## Future Work\n\nThank you for reading. We see several directions worth further exploration:\n\n1. Study train–inference inconsistency more deeply, analyze its root causes, and explore better solutions.\n2. Investigate quantization strategies more thoroughly, understand how quantization error arises, and design schemes with lower error.\n3. Improve low-precision training efficiency via better algorithms, frameworks, and hardware–software co-design, hiding the latency of kernel launches and quantization and truly realizing acceleration for both training and inference.\n\n## Acknowledgments\n\n1. InfiXAI Team: Congkai Xie, Mingfa Feng, Shuo Cai\n2. Ant Group AQ Team: Yanan Gao, Zhiling Ye, Hansong Xiao \n3. SGLang RL Team: JiLi, Yefei Chen, Xi Chen, Zilin Zhu\n4. Miles Team: Chenyang Zhao\n5. NVIDIA: Juan Yu, NeMo-RL Team\n","slug":"2025-11-25-fp8-rl"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2025-11-25-fp8-rl"},"buildId":"HOCqR_uxXD79xgqqQW-Dx","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>