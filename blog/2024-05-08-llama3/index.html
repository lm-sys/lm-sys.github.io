<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>What’s up with Llama 3? Arena data analysis | LMSYS Org</title><meta name="title" content="What’s up with Llama 3? Arena data analysis | LMSYS Org"/><meta property="og:title" content="What’s up with Llama 3? Arena data analysis | LMSYS Org"/><meta name="twitter:title" content="What’s up with Llama 3? Arena data analysis | LMSYS Org"/><meta name="description" content="&lt;p&gt;On April 18th, Meta released Llama 3, their newest open-weight large language model. Since then, Llama 3-70B has quickly risen to the top of the English &lt;..."/><meta property="og:description" content="&lt;p&gt;On April 18th, Meta released Llama 3, their newest open-weight large language model. Since then, Llama 3-70B has quickly risen to the top of the English &lt;..."/><meta name="twitter:description" content="&lt;p&gt;On April 18th, Meta released Llama 3, their newest open-weight large language model. Since then, Llama 3-70B has quickly risen to the top of the English &lt;..."/><meta property="og:image" content="https://lmsys.org/images/blog/llama3/llama3_blog_cover.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/llama3/llama3_blog_cover.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2024-05-08-llama3"/><meta name="twitter:url" content="https://lmsys.org/blog/2024-05-08-llama3"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-0bb93d4b49319e30.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/ojoCr5F2fI8MLYJMtD0Zz/_buildManifest.js" defer=""></script><script src="/_next/static/ojoCr5F2fI8MLYJMtD0Zz/_ssgManifest.js" defer=""></script><script src="/_next/static/ojoCr5F2fI8MLYJMtD0Zz/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">What’s up with Llama 3? Arena data analysis</h1><p class="text-xl pt-2 pb-2">by: <!-- -->Lisa Dunlap, Evan Frick, Tianle Li, Isaac Ong, Joseph E. Gonzalez, Wei-Lin Chiang<!-- -->,<!-- --> <!-- -->May 08, 2024<!-- --></p><hr/><div class="pt-2 article"><p>On April 18th, Meta released Llama 3, their newest open-weight large language model. Since then, Llama 3-70B has quickly risen to the top of the English <a href="https://leaderboard.lmsys.org">Chatbot Arena leaderboard</a> with over 50,000 battles. This remarkable achievement by Meta is excellent news for the open-source community. In this blog post, we aim to provide more insight into why users rank Llama 3-70b on par with top-ranked models like GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus.</p>
<br />
<p>We investigate the following:</p>
<ol>
<li>What types of prompts are users asking? Do users prefer Llama 3 on certain types of prompts?</li>
<li>How challenging are these prompts? Does the ranking change if the prompts are easier/harder?</li>
<li>Are certain users or prompts overrepresented? Do duplicate prompts or rankings from a small number of users affect the win rate?</li>
<li>Does Llama 3 have qualitative differences which make users like it more?</li>
</ol>
<p>We focus on battles consisting of Llama 3-70b against 5 top-ranked models (claude-3-opus-20240229, gpt-4-0125-preview, gpt-4-1106-preview, gpt-4-turbo-2024-04-09, gemini-1.5-pro-0409-preview) and reach the following conclusions:</p>
<ol>
<li>Llama 3 beats other top-ranking models on open-ended writing and creative problems but loses on more close-ended math and coding problems.</li>
<li>As prompts get harder, Llama 3’s win rate against top-tier models drops significantly.</li>
<li>Deduplication or outliers do not significantly affect the win rate.</li>
<li>Qualitatively, Llama 3’s outputs are friendlier and more conversational than other models, and these traits appear more often in battles that Llama 3 wins.</li>
</ol>
<br/>
<img src="/images/blog/llama3/topic_win_rate.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 85%"></img>
<p style="color:gray; text-align: center;">Figure 1. Llama 3-70b's win rate (excluding ties) against top 5 models across prompt topics. * denotes that the category contains less than 50 battles.</p>
<h2><a id="analyzing-win-rate-across-different-types-of-prompts" class="anchor" href="#analyzing-win-rate-across-different-types-of-prompts" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Analyzing win rate across different types of prompts</h2>
<p><strong>Topic Analysis.</strong> We utilize an LLM labeler (Llama 3-70b) to categorize user prompts into a pre-established taxonomy of topics (<a href="https://arxiv.org/pdf/2404.12387">from Reka's paper</a>) and visualize the win rate of Llama 3-70b against the other top models in Figure 1. We see that Llama 3’s win rate is highest for open-ended and creative tasks like brainstorming and writing, and lowest for more close-ended technical tasks like math and translation. Interestingly, Llama 3 achieves the highest win rate over data processing tasks which mainly consist of parsing and dataframe operations, but as this category has only 19 examples, this remains inconclusive.</p>
<p><strong>Win Rate versus Prompt Difficulty.</strong> We employ our <a href="https://lmsys.org/blog/2024-04-19-arena-hard/">recently released pipeline</a> which scores the difficulty of prompts to determine how Llama 3 compares to the other top models as prompts get harder. We define a set of &quot;hardness&quot; criteria and use GPT-4-turbo to annotate each prompt from 0 to 7 to indicate how many of these criteria are satisfied (a higher score indicates a harder prompt). Our 7 criteria are:</p>
<table style="width:100%; border-collapse: collapse; border: 1px solid black;">
  <tr style="background-color: black; color: white;">
    <!-- <th style="border: 1px solid black; padding: 10px; text-align: left;">7 Key "Hardness" Criteria</th> -->
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>1. Specificity:</strong> Does the prompt ask for a specific output?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>2. Domain Knowledge:</strong> Does the prompt cover one or more specific domains?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>3. Complexity:</strong> Does the prompt have multiple levels of reasoning, components, or variables?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>4. Problem-Solving:</strong> Does the prompt directly involve the AI to demonstrate active problem-solving skills?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>5. Creativity:</strong> Does the prompt involve a level of creativity in approaching the problem?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>6. Technical Accuracy:</strong> Does the prompt require technical accuracy in the response?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>7. Real-world Application:</strong> Does the prompt relate to real-world applications?</td>
  </tr>
</table>
<p>We score 1000 battles against the top 3 models on the leaderboard and plot their win rates versus prompt score in Figure 2. We observe a significant drop in Llama 3's performance compared to the other top models, from a high 50% win rate to a low 40% win rate. We conclude that as more of these &quot;hardness&quot; criteria are met, Llama 3's win rate drop rapidly compared to other models. Note that these criteria may not be exhaustive, see <a href="https://lmsys.org/blog/2024-04-19-arena-hard/">the blog</a> for further discussion.</p>
<p><img src="/images/blog/llama3/winrate-over-criteria.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 70%"></img></p>
<p style="color:gray; text-align: center;">Figure 2. Several top models' win rate against the strongest 6 models over the intervals of number of key criteria satisfied. *English battles between strongest models: llama-3-70b-chat, claude-3-opus-20240229, gpt-4-0125-preview, gpt-4-1106-preview, gpt-4-turbo-2024-04-09, gemini-1.5-pro-api-0409-preview.</p>
<p><img src="/images/blog/llama3/criteria_dist.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 50%"></img></p>
<p style="color:gray; text-align: center;">Figure 3. The percentage of prompts with number of hardness criteria met in 3.5K sample of arena battles. We observe a significant portion of the battles are classified as hard (~27%).</p>
<p>We can further analyze which types of prompts affect win rate by fitting a decision tree on the 7 binary columns representing if a given prompt has satisfied each of the criteria above. From this decision tree, we can segment prompts into criteria subsets such that Llama 3-70b-Instruct either performs very well or very poorly. The tree shown in Figure 4 shows us which subsets change the model’s win rate the most when conditioned on.</p>
<p><img src="/images/blog/llama3/dtree.svg" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 100%"></img></p>
<p style="color:gray; text-align: center;">Figure 4. Llama 3-70b-Instruct's win rate conditioned on hierarchical prompt criteria subsets as fitted using a standard decision tree algorithm.</p>
<p>The first thing to notice is that “Specificity” is the root node of the tree, suggesting that this criteria most immediately divides Llama3-70b-Instruct’s performance into its strengths and weaknesses. It supports our initial findings above that Llama3-70b-Instruct is stronger on open-ended tasks rather than more closed-ended tasks. We can traverse further down the tree and see that Llama3-70b-Instruct is quite strong on open-ended creative questions (see the blue path), reaching around a 60% win-rate against these top models. Emperically, these types of questions are often writing and brainstorming style questions. For example two prompts where Llama-3-70B-Instruct won are: &quot;Write the first chapter of a novel.&quot; and &quot;Could you provide two story suggestions for children that promote altruism? &quot;. On the other hand, following the orange path, we can notice that Llama3-70b-Instruct has a lower win-rate against top models when answering close-ended, non-real-world, reasoning-based questions. These questions are often logic puzzles and math word word problems. Two examples where Llama-3-70B-Instruct won are: &quot;123x = -4x * 2 - 65&quot; and &quot;There are two ducks in front of a duck, two ducks behind a duck and a duck in the middle. How many ducks are there?&quot;</p>
<h2><a id="the-effect-of-overrepresented-prompts-and-judges" class="anchor" href="#the-effect-of-overrepresented-prompts-and-judges" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The effect of overrepresented prompts and judges</h2>
<p><strong>Effect of duplicate prompts.</strong> Using fuzzy string matching, we find that ~9% (6658/7327) of the user prompts in battles between Llama 3 and the other top models are duplicates, and show in Table 1 that deduplication does not significantly affect Llama 3's win rate.</p>
<style>
th {text-align: left, text-weight: bold}
td {text-align: left}
</style>
<br>
<p style="color:gray; text-align: center;">Table 1: Llama 3-70b battle stats.</p>
<table style="display: flex; justify-content: center;">
<tbody>
<tr>
<th>Model</th> <th># battles</th> <th># battles no tie</th> <th># battles (dedup, no tie)</th> <th>Llama 3 win rate</th> <th>Llama 3 win rate (dedup, no tie)</th>
</tr>
<tr>
<td>Claude 3 Opus</td> <td>1959</td> <td>1328</td> <td>1171</td> <td>51.28%</td> <td>51.58%</td>
</tr>
<tr>
<td>Gemini 1.5</td> <td>2413</td> <td>1620</td> <td>1437</td> <td>50.06%</td> <td>49.48%</td>
</tr>
<tr>
<td>GPT-4 0125</td> <td>1271</td> <td>881</td> <td>779</td> <td>48.58%</td> <td>49.04%</td>
</tr>
<tr>
<td>GPT-4 1106</td> <td>526</td> <td>349</td> <td>307</td> <td>50.72%</td> <td>52.12%</td>
</tr>
<tr>
<td>GPT-4-Turbo</td> <td>2097</td> <td>1437</td> <td>1287</td> <td>47.74%</td> <td>47.73%</td>
</tr>
</tbody>
</table>
<p><strong>User analysis.</strong> First we consider some basic user statistics in Table 2 to check that judging behavior is similar between Claude-3-Opus-20240229 and Llama 3-70B-Instruct.</p>
<br>
<p style="color:gray; text-align: center;">Table 2. Detailed Engagement Metrics for LLMs (Timeframe: April 24 - May 1, 2023). The latest and detailed version <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard" target="_blank">here</a>.</p>
<table style="display: flex; justify-content: center;">
<tbody>
<tr>
<th>Model</th> <th>Battles</th> <th>Unique Judges</th> <th>Mean Votes per Judge</th> <th>Median Votes per Judge</th> <th>Max Votes per Judge</th>
</tr>
<tr>
<td>Llama 3-70B-Instruct</td> <td>12,719</td> <td>7,591</td> <td>1.68</td> <td>1</td> <td>65</td>
</tr>
<tr>
<td>Claude-3-Opus-20240229</td> <td>68,656</td> <td>48,570</td> <td>1.41</td> <td>1</td> <td>73</td>
</tr>
<tr>
<td>All Models All Time</td> <td>749,205</td> <td>316,372</td> <td>2.37</td> <td>1</td> <td>591</td>
</tr>
</tbody>
</table>
<p>In order to limit the impact of users that vote many times, we can take the mean of each judge’s win rate, thereby bounding the impact of each individual judge. In this case, we find that this stratified win rate shown in Table 3 is still very similar to the original win rate, suggesting that very active judges are not skewing the result.</p>
<br>
<p style="color:gray; text-align: center;">Table 3. Model Win Rates (Timeframe: April 24 - May 1, 2023). The latest and detailed version <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard" target="_blank">here</a>. Note that ties are counted as 0.5, with wins and losses as 1 and 0, respectively.</p>
<table style="display: flex; justify-content: center;">
<tbody>
<tr>
<th>Model</th> <th>Win rate</th> <th>Stratified Win Rate</th>
</tr>
<tr>
<td>Llama 3-70B-Instruct</td> <td>0.541</td> <td>0.543</td>
</tr>
<tr>
<td>Claude-3-Opus-20240229</td> <td>0.619</td> <td>0.621</td>
</tr>
</tbody>
</table>
<p><strong>Qualitative differences between Llama 3 outputs versus other models.</strong> From qualitative analysis of outputs between Llama 3 and other models, we observe that Llama 3 outputs are often more excited, positive, conversational, and friendly than other models.</p>
<p><strong>Measuring sentiment.</strong> To measure excitement, we assign a binary label to each output based on the presence of an exclamation point. For positivity, friendliness, and conversationality, we use GPT-3.5 as a judge to rate each output on a scale of 1-5. In a given battle, Llama 3's outputs are labeled as more excited, positive, conversational, or friendly if their score is higher than the opponent's. Figure 5 displays the distribution of these qualities across models, revealing that Llama 3's outputs generally exhibit higher levels of excitement, positivity, friendliness, and conversationality as compared to their opponents.</p>
<p><img src="/images/blog/llama3/llama_sentiment_distribution.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 85%"></img></p>
<p style="color:gray; text-align: center;">Figure 5: Proportion of arena prompts where Llama 3 is more positive/friendly/conversational/exclamatory than its opponent.</p>
<p><strong>Is sentiment related to win rate?</strong> Figure 6 compares the sentiment qualities of Llama 3's outputs in battles it wins versus those it loses. We see that all traits appear more in winning battles and less in losing battles, but this difference is relatively small, especially for positivity and friendliness. This suggests that while these traits might play a role in competitive success, their influence requires further exploration for more definitive insights.</p>
<p><img src="/images/blog/llama3/sentiment_win_rate.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 85%"></img></p>
<p style="color:gray; text-align: center;">Figure 6: Llama 3's sentiment versus its win rate when Llama 3 is more positive/friendly/conversational/exclamatory than its opponent.</p>
<h2><a id="conclusion" class="anchor" href="#conclusion" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>From the beginning, our mission has been to advance LLM development and understanding. While in the past we have focused on high-level ranking and benchmark design, moving forward, we hope to extend the analysis here and conduct more in-depth analysis into changes in human preference as well as model behavior.</p>
<h2><a id="acknowledgment" class="anchor" href="#acknowledgment" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgment</h2>
<p>We thank Matei Zaharia and Ion Stoica for their valuable feedback.</p>
<h2><a id="citation" class="anchor" href="#citation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Citation</h2>
<pre><code class="hljs"><span class="language-xml">@misc</span><span class="hljs-template-variable">{llama3arena2024,
    title = {What’s up with Llama 3? Arena data analysis}</span><span class="language-xml">,
    url = </span><span class="hljs-template-variable">{https://lmsys.org/blog/2024-05-08-llama3/}</span><span class="language-xml">,
    author = </span><span class="hljs-template-variable">{Lisa Dunlap, Evan Frick, Tianle Li, Isaac Ong, Joseph E. Gonzalez, Wei-Lin Chiang}</span><span class="language-xml">,
    month = </span><span class="hljs-template-variable">{May}</span><span class="language-xml">,
    year = </span><span class="hljs-template-variable">{2024}</span><span class="language-xml">
}
</span></code></pre>
<h2><a id="qualitative-examples" class="anchor" href="#qualitative-examples" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Qualitative examples</h2>
<p><strong>Prompt:</strong> What is entropy? Explain using only words that start by a or t</p>
<p><strong>Llama-70b-Instruct:</strong> What an intriguing inquiry!</p>
<p>Alright, Adept Thinker, here's an attempt to articulate the abstract abstraction of entropy using only words that start with A or T:</p>
<p>&quot;Astonishingly, the tendency to array things in an orderly arrangement tends to topple, allowing anomalous atoms to transition towards tumultuous, aimless arrangements. This transformation tends to accelerate, augmenting the amount of apparent arbitrariness. As a result, the aggregate arrangement assumes an attitude of advanced ambiguity, making it arduous to accurately anticipate the trajectory of attending atoms.&quot;</p>
<p>There you have it, an all-A-and-T explanation of entropy!</p>
<p><strong>GPT-4-Turbo:</strong> Entropy, abstractly articulated, always alludes to a thermodynamic theory, attributing attributes typically tied to the transformation, transfer, or turmoil within thermodynamic systems. Technically, this term tackles the tendency to transition from an arranged state to a tangled, tumultuous state. Theoretically, entropy assesses the amount attainable arrangements accessible at a thermodynamic level. Thus, through time, systems tend toward these tangled states, termed &quot;thermodynamic equilibrium.&quot; This transformation typifies the arrow of time, tracing transitions toward total disorder.</p>
<p><strong>Winner:</strong> Llama 3</p>
<hr>
<p><strong>Prompt:</strong> What is this called you are right until you are proven wrong</p>
<p><strong>Llama 3-70b-Instruct:</strong> You're referring to a common phenomenon!</p>
<p>What you're describing is often called &quot;Presumption of innocence until proven guilty&quot; or simply &quot;Presumption of innocence.&quot; This concept is a fundamental principle in many legal systems, (including criminal law and ethics) that assumes a person is innocent until they are proven guilty beyond a reasonable doubt.</p>
<p>However, if we broaden the context to include everyday conversations and debates, this phenomenon can also be referred to as:</p>
<ol>
<li>Appeal to ignorance: The assumption that a statement is true simply because it has not been proven false.</li>
<li>Burden of proof: The idea that someone making a claim must provide evidence to support it, and until they do, the claim is not accepted as true.</li>
<li>Skepticism: A healthy dose of doubt or prompting of a claim until sufficient evidence is provided to support it.</li>
</ol>
<p>In general, it's essential to approach claims and arguments with a critical and open-minded perspective, recognizing that assumptions can be wrong until proven right, and that evidence and reasoning are necessary to support a claim.</p>
<p><strong>Claude-3-Opus:</strong> The concept you are referring to is known as the &quot;null hypothesis&quot; or &quot;presumption of innocence.&quot;</p>
<p>In scientific research, the null hypothesis is the default assumption that there is no relationship or effect between the variables being studied. Researchers aim to reject the null hypothesis by providing evidence supporting their alternative hypothesis. Until sufficient evidence is presented, the null hypothesis is presumed to be true.</p>
<p>In the legal context, the presumption of innocence is a fundamental principle stating that a person accused of a crime is considered innocent until proven guilty in a court of law. The burden of proof lies with the prosecution to establish guilt beyond a reasonable doubt.</p>
<p>Both concepts share the idea that a claim or accusation is initially assumed to be false or unsupported until sufficient evidence is provided to prove otherwise. This approach helps to prevent unjustified assertions and encourages rigorous evidence-based reasoning.</p>
<p><strong>Winner:</strong> Claude-3-Opus</p>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"What’s up with Llama 3? Arena data analysis","author":"Lisa Dunlap, Evan Frick, Tianle Li, Isaac Ong, Joseph E. Gonzalez, Wei-Lin Chiang","date":"May 8, 2024","previewImg":"/images/blog/llama3/llama3_blog_cover.png"},"content":"\nOn April 18th, Meta released Llama 3, their newest open-weight large language model. Since then, Llama 3-70B has quickly risen to the top of the English [Chatbot Arena leaderboard](https://leaderboard.lmsys.org) with over 50,000 battles. This remarkable achievement by Meta is excellent news for the open-source community. In this blog post, we aim to provide more insight into why users rank Llama 3-70b on par with top-ranked models like GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus.\n\n\u003cbr /\u003e\n\nWe investigate the following:\n1. What types of prompts are users asking? Do users prefer Llama 3 on certain types of prompts? \n2. How challenging are these prompts? Does the ranking change if the prompts are easier/harder?\n3. Are certain users or prompts overrepresented? Do duplicate prompts or rankings from a small number of users affect the win rate?\n4. Does Llama 3 have qualitative differences which make users like it more?\n\nWe focus on battles consisting of Llama 3-70b against 5 top-ranked models (claude-3-opus-20240229, gpt-4-0125-preview, gpt-4-1106-preview, gpt-4-turbo-2024-04-09, gemini-1.5-pro-0409-preview) and reach the following conclusions:\n1. Llama 3 beats other top-ranking models on open-ended writing and creative problems but loses on more close-ended math and coding problems.\n2. As prompts get harder, Llama 3’s win rate against top-tier models drops significantly.\n3. Deduplication or outliers do not significantly affect the win rate.\n4. Qualitatively, Llama 3’s outputs are friendlier and more conversational than other models, and these traits appear more often in battles that Llama 3 wins.\n\n\u003cbr/\u003e\n\u003cimg src=\"/images/blog/llama3/topic_win_rate.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 85%\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 1. Llama 3-70b's win rate (excluding ties) against top 5 models across prompt topics. * denotes that the category contains less than 50 battles.\u003c/p\u003e\n\n\n\n## Analyzing win rate across different types of prompts\n\n**Topic Analysis.** We utilize an LLM labeler (Llama 3-70b) to categorize user prompts into a pre-established taxonomy of topics ([from Reka's paper](https://arxiv.org/pdf/2404.12387)) and visualize the win rate of Llama 3-70b against the other top models in Figure 1. We see that Llama 3’s win rate is highest for open-ended and creative tasks like brainstorming and writing, and lowest for more close-ended technical tasks like math and translation. Interestingly, Llama 3 achieves the highest win rate over data processing tasks which mainly consist of parsing and dataframe operations, but as this category has only 19 examples, this remains inconclusive. \n\n**Win Rate versus Prompt Difficulty.** We employ our [recently released pipeline](https://lmsys.org/blog/2024-04-19-arena-hard/) which scores the difficulty of prompts to determine how Llama 3 compares to the other top models as prompts get harder. We define a set of \"hardness\" criteria and use GPT-4-turbo to annotate each prompt from 0 to 7 to indicate how many of these criteria are satisfied (a higher score indicates a harder prompt). Our 7 criteria are:\n\n\u003ctable style=\"width:100%; border-collapse: collapse; border: 1px solid black;\"\u003e\n  \u003ctr style=\"background-color: black; color: white;\"\u003e\n    \u003c!-- \u003cth style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003e7 Key \"Hardness\" Criteria\u003c/th\u003e --\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003e\u003cstrong\u003e1. Specificity:\u003c/strong\u003e Does the prompt ask for a specific output?\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003e\u003cstrong\u003e2. Domain Knowledge:\u003c/strong\u003e Does the prompt cover one or more specific domains?\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003e\u003cstrong\u003e3. Complexity:\u003c/strong\u003e Does the prompt have multiple levels of reasoning, components, or variables?\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003e\u003cstrong\u003e4. Problem-Solving:\u003c/strong\u003e Does the prompt directly involve the AI to demonstrate active problem-solving skills?\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003e\u003cstrong\u003e5. Creativity:\u003c/strong\u003e Does the prompt involve a level of creativity in approaching the problem?\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003e\u003cstrong\u003e6. Technical Accuracy:\u003c/strong\u003e Does the prompt require technical accuracy in the response?\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003e\u003cstrong\u003e7. Real-world Application:\u003c/strong\u003e Does the prompt relate to real-world applications?\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\nWe score 1000 battles against the top 3 models on the leaderboard and plot their win rates versus prompt score in Figure 2. We observe a significant drop in Llama 3's performance compared to the other top models, from a high 50% win rate to a low 40% win rate. We conclude that as more of these \"hardness\" criteria are met, Llama 3's win rate drop rapidly compared to other models. Note that these criteria may not be exhaustive, see [the blog](https://lmsys.org/blog/2024-04-19-arena-hard/) for further discussion.\n\n\u003cimg src=\"/images/blog/llama3/winrate-over-criteria.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 70%\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 2. Several top models' win rate against the strongest 6 models over the intervals of number of key criteria satisfied. *English battles between strongest models: llama-3-70b-chat, claude-3-opus-20240229, gpt-4-0125-preview, gpt-4-1106-preview, gpt-4-turbo-2024-04-09, gemini-1.5-pro-api-0409-preview.\u003c/p\u003e\n\n\u003cimg src=\"/images/blog/llama3/criteria_dist.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 50%\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 3. The percentage of prompts with number of hardness criteria met in 3.5K sample of arena battles. We observe a significant portion of the battles are classified as hard (~27%).\u003c/p\u003e\n\nWe can further analyze which types of prompts affect win rate by fitting a decision tree on the 7 binary columns representing if a given prompt has satisfied each of the criteria above. From this decision tree, we can segment prompts into criteria subsets such that Llama 3-70b-Instruct either performs very well or very poorly. The tree shown in Figure 4 shows us which subsets change the model’s win rate the most when conditioned on.\n\n\u003cimg src=\"/images/blog/llama3/dtree.svg\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 100%\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 4. Llama 3-70b-Instruct's win rate conditioned on hierarchical prompt criteria subsets as fitted using a standard decision tree algorithm.\u003c/p\u003e\n\nThe first thing to notice is that “Specificity” is the root node of the tree, suggesting that this criteria most immediately divides Llama3-70b-Instruct’s performance into its strengths and weaknesses. It supports our initial findings above that Llama3-70b-Instruct is stronger on open-ended tasks rather than more closed-ended tasks. We can traverse further down the tree and see that Llama3-70b-Instruct is quite strong on open-ended creative questions (see the blue path), reaching around a 60% win-rate against these top models. Emperically, these types of questions are often writing and brainstorming style questions. For example two prompts where Llama-3-70B-Instruct won are: \"Write the first chapter of a novel.\" and \"Could you provide two story suggestions for children that promote altruism? \". On the other hand, following the orange path, we can notice that Llama3-70b-Instruct has a lower win-rate against top models when answering close-ended, non-real-world, reasoning-based questions. These questions are often logic puzzles and math word word problems. Two examples where Llama-3-70B-Instruct won are: \"123x = -4x * 2 - 65\" and \"There are two ducks in front of a duck, two ducks behind a duck and a duck in the middle. How many ducks are there?\"\n\n## The effect of overrepresented prompts and judges\n\n**Effect of duplicate prompts.** Using fuzzy string matching, we find that ~9% (6658/7327) of the user prompts in battles between Llama 3 and the other top models are duplicates, and show in Table 1 that deduplication does not significantly affect Llama 3's win rate. \n\n\u003cstyle\u003e\nth {text-align: left, text-weight: bold}\ntd {text-align: left}\n\u003c/style\u003e\n\n\n\u003cbr\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 1: Llama 3-70b battle stats.\u003c/p\u003e\n\u003ctable style=\"display: flex; justify-content: center;\"\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003cth\u003eModel\u003c/th\u003e \u003cth\u003e# battles\u003c/th\u003e \u003cth\u003e# battles no tie\u003c/th\u003e \u003cth\u003e# battles (dedup, no tie)\u003c/th\u003e \u003cth\u003eLlama 3 win rate\u003c/th\u003e \u003cth\u003eLlama 3 win rate (dedup, no tie)\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eClaude 3 Opus\u003c/td\u003e \u003ctd\u003e1959\u003c/td\u003e \u003ctd\u003e1328\u003c/td\u003e \u003ctd\u003e1171\u003c/td\u003e \u003ctd\u003e51.28%\u003c/td\u003e \u003ctd\u003e51.58%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eGemini 1.5\u003c/td\u003e \u003ctd\u003e2413\u003c/td\u003e \u003ctd\u003e1620\u003c/td\u003e \u003ctd\u003e1437\u003c/td\u003e \u003ctd\u003e50.06%\u003c/td\u003e \u003ctd\u003e49.48%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eGPT-4 0125\u003c/td\u003e \u003ctd\u003e1271\u003c/td\u003e \u003ctd\u003e881\u003c/td\u003e \u003ctd\u003e779\u003c/td\u003e \u003ctd\u003e48.58%\u003c/td\u003e \u003ctd\u003e49.04%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eGPT-4 1106\u003c/td\u003e \u003ctd\u003e526\u003c/td\u003e \u003ctd\u003e349\u003c/td\u003e \u003ctd\u003e307\u003c/td\u003e \u003ctd\u003e50.72%\u003c/td\u003e \u003ctd\u003e52.12%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eGPT-4-Turbo\u003c/td\u003e \u003ctd\u003e2097\u003c/td\u003e \u003ctd\u003e1437\u003c/td\u003e \u003ctd\u003e1287\u003c/td\u003e \u003ctd\u003e47.74%\u003c/td\u003e \u003ctd\u003e47.73%\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\n**User analysis.** First we consider some basic user statistics in Table 2 to check that judging behavior is similar between Claude-3-Opus-20240229 and Llama 3-70B-Instruct.\n\n\u003cbr\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 2. Detailed Engagement Metrics for LLMs (Timeframe: April 24 - May 1, 2023). The latest and detailed version \u003ca href=\"https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ctable style=\"display: flex; justify-content: center;\"\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003cth\u003eModel\u003c/th\u003e \u003cth\u003eBattles\u003c/th\u003e \u003cth\u003eUnique Judges\u003c/th\u003e \u003cth\u003eMean Votes per Judge\u003c/th\u003e \u003cth\u003eMedian Votes per Judge\u003c/th\u003e \u003cth\u003eMax Votes per Judge\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLlama 3-70B-Instruct\u003c/td\u003e \u003ctd\u003e12,719\u003c/td\u003e \u003ctd\u003e7,591\u003c/td\u003e \u003ctd\u003e1.68\u003c/td\u003e \u003ctd\u003e1\u003c/td\u003e \u003ctd\u003e65\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eClaude-3-Opus-20240229\u003c/td\u003e \u003ctd\u003e68,656\u003c/td\u003e \u003ctd\u003e48,570\u003c/td\u003e \u003ctd\u003e1.41\u003c/td\u003e \u003ctd\u003e1\u003c/td\u003e \u003ctd\u003e73\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eAll Models All Time\u003c/td\u003e \u003ctd\u003e749,205\u003c/td\u003e \u003ctd\u003e316,372\u003c/td\u003e \u003ctd\u003e2.37\u003c/td\u003e \u003ctd\u003e1\u003c/td\u003e \u003ctd\u003e591\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\nIn order to limit the impact of users that vote many times, we can take the mean of each judge’s win rate, thereby bounding the impact of each individual judge. In this case, we find that this stratified win rate shown in Table 3 is still very similar to the original win rate, suggesting that very active judges are not skewing the result.\n\n\n\u003cbr\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 3. Model Win Rates (Timeframe: April 24 - May 1, 2023). The latest and detailed version \u003ca href=\"https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\" target=\"_blank\"\u003ehere\u003c/a\u003e. Note that ties are counted as 0.5, with wins and losses as 1 and 0, respectively.\u003c/p\u003e\n\u003ctable style=\"display: flex; justify-content: center;\"\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003cth\u003eModel\u003c/th\u003e \u003cth\u003eWin rate\u003c/th\u003e \u003cth\u003eStratified Win Rate\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLlama 3-70B-Instruct\u003c/td\u003e \u003ctd\u003e0.541\u003c/td\u003e \u003ctd\u003e0.543\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eClaude-3-Opus-20240229\u003c/td\u003e \u003ctd\u003e0.619\u003c/td\u003e \u003ctd\u003e0.621\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n**Qualitative differences between Llama 3 outputs versus other models.** From qualitative analysis of outputs between Llama 3 and other models, we observe that Llama 3 outputs are often more excited, positive, conversational, and friendly than other models.\n\n**Measuring sentiment.** To measure excitement, we assign a binary label to each output based on the presence of an exclamation point. For positivity, friendliness, and conversationality, we use GPT-3.5 as a judge to rate each output on a scale of 1-5. In a given battle, Llama 3's outputs are labeled as more excited, positive, conversational, or friendly if their score is higher than the opponent's. Figure 5 displays the distribution of these qualities across models, revealing that Llama 3's outputs generally exhibit higher levels of excitement, positivity, friendliness, and conversationality as compared to their opponents.\n\n\u003cimg src=\"/images/blog/llama3/llama_sentiment_distribution.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 85%\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 5: Proportion of arena prompts where Llama 3 is more positive/friendly/conversational/exclamatory than its opponent.\u003c/p\u003e\n\n**Is sentiment related to win rate?** Figure 6 compares the sentiment qualities of Llama 3's outputs in battles it wins versus those it loses. We see that all traits appear more in winning battles and less in losing battles, but this difference is relatively small, especially for positivity and friendliness. This suggests that while these traits might play a role in competitive success, their influence requires further exploration for more definitive insights.\n\n\u003cimg src=\"/images/blog/llama3/sentiment_win_rate.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 85%\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 6: Llama 3's sentiment versus its win rate when Llama 3 is more positive/friendly/conversational/exclamatory than its opponent.\u003c/p\u003e\n\n## Conclusion\nFrom the beginning, our mission has been to advance LLM development and understanding. While in the past we have focused on high-level ranking and benchmark design, moving forward, we hope to extend the analysis here and conduct more in-depth analysis into changes in human preference as well as model behavior.  \n\n\n## Acknowledgment\nWe thank Matei Zaharia and Ion Stoica for their valuable feedback.\n\n## Citation\n```\n@misc{llama3arena2024,\n    title = {What’s up with Llama 3? Arena data analysis},\n    url = {https://lmsys.org/blog/2024-05-08-llama3/},\n    author = {Lisa Dunlap, Evan Frick, Tianle Li, Isaac Ong, Joseph E. Gonzalez, Wei-Lin Chiang},\n    month = {May},\n    year = {2024}\n}\n```\n\n## Qualitative examples\n**Prompt:** What is entropy? Explain using only words that start by a or t\n\n**Llama-70b-Instruct:** What an intriguing inquiry!\n\nAlright, Adept Thinker, here's an attempt to articulate the abstract abstraction of entropy using only words that start with A or T:\n\n\"Astonishingly, the tendency to array things in an orderly arrangement tends to topple, allowing anomalous atoms to transition towards tumultuous, aimless arrangements. This transformation tends to accelerate, augmenting the amount of apparent arbitrariness. As a result, the aggregate arrangement assumes an attitude of advanced ambiguity, making it arduous to accurately anticipate the trajectory of attending atoms.\"\n\nThere you have it, an all-A-and-T explanation of entropy!\n\n**GPT-4-Turbo:** Entropy, abstractly articulated, always alludes to a thermodynamic theory, attributing attributes typically tied to the transformation, transfer, or turmoil within thermodynamic systems. Technically, this term tackles the tendency to transition from an arranged state to a tangled, tumultuous state. Theoretically, entropy assesses the amount attainable arrangements accessible at a thermodynamic level. Thus, through time, systems tend toward these tangled states, termed \"thermodynamic equilibrium.\" This transformation typifies the arrow of time, tracing transitions toward total disorder.\n\n**Winner:** Llama 3\n\n\n---\n\n**Prompt:** What is this called you are right until you are proven wrong\n\n**Llama 3-70b-Instruct:** You're referring to a common phenomenon!\n\nWhat you're describing is often called \"Presumption of innocence until proven guilty\" or simply \"Presumption of innocence.\" This concept is a fundamental principle in many legal systems, (including criminal law and ethics) that assumes a person is innocent until they are proven guilty beyond a reasonable doubt.\n\nHowever, if we broaden the context to include everyday conversations and debates, this phenomenon can also be referred to as:\n\n1. Appeal to ignorance: The assumption that a statement is true simply because it has not been proven false.\n2. Burden of proof: The idea that someone making a claim must provide evidence to support it, and until they do, the claim is not accepted as true.\n3. Skepticism: A healthy dose of doubt or prompting of a claim until sufficient evidence is provided to support it.\n\nIn general, it's essential to approach claims and arguments with a critical and open-minded perspective, recognizing that assumptions can be wrong until proven right, and that evidence and reasoning are necessary to support a claim.\n\n**Claude-3-Opus:** The concept you are referring to is known as the \"null hypothesis\" or \"presumption of innocence.\" \n\nIn scientific research, the null hypothesis is the default assumption that there is no relationship or effect between the variables being studied. Researchers aim to reject the null hypothesis by providing evidence supporting their alternative hypothesis. Until sufficient evidence is presented, the null hypothesis is presumed to be true.\n\nIn the legal context, the presumption of innocence is a fundamental principle stating that a person accused of a crime is considered innocent until proven guilty in a court of law. The burden of proof lies with the prosecution to establish guilt beyond a reasonable doubt.\n\nBoth concepts share the idea that a claim or accusation is initially assumed to be false or unsupported until sufficient evidence is provided to prove otherwise. This approach helps to prevent unjustified assertions and encourages rigorous evidence-based reasoning.\n\n**Winner:** Claude-3-Opus\n","slug":"2024-05-08-llama3"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2024-05-08-llama3"},"buildId":"ojoCr5F2fI8MLYJMtD0Zz","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>