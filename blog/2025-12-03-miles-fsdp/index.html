<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>Power Up FSDP2 as a Flexible Training Backend for Miles | LMSYS Org</title><meta name="title" content="Power Up FSDP2 as a Flexible Training Backend for Miles | LMSYS Org"/><meta property="og:title" content="Power Up FSDP2 as a Flexible Training Backend for Miles | LMSYS Org"/><meta name="twitter:title" content="Power Up FSDP2 as a Flexible Training Backend for Miles | LMSYS Org"/><meta name="description" content="&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We have added FSDP to &lt;a href=&quot;https://github.com/radixark/miles&quot;&gt;Miles&lt;/a&gt; as a more flexible trainin..."/><meta property="og:description" content="&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We have added FSDP to &lt;a href=&quot;https://github.com/radixark/miles&quot;&gt;Miles&lt;/a&gt; as a more flexible trainin..."/><meta name="twitter:description" content="&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We have added FSDP to &lt;a href=&quot;https://github.com/radixark/miles&quot;&gt;Miles&lt;/a&gt; as a more flexible trainin..."/><meta property="og:image" content="https://lmsys.org/images/blog/miles-fsdp/2_fsdp_train.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/miles-fsdp/2_fsdp_train.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2025-12-03-miles-fsdp"/><meta name="twitter:url" content="https://lmsys.org/blog/2025-12-03-miles-fsdp"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-eef2afd147d8eda9.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/iUALlhdQzCWTY537sUjtz/_buildManifest.js" defer=""></script><script src="/_next/static/iUALlhdQzCWTY537sUjtz/_ssgManifest.js" defer=""></script><script src="/_next/static/iUALlhdQzCWTY537sUjtz/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">Power Up FSDP2 as a Flexible Training Backend for Miles</h1><p class="text-xl pt-2 pb-2">by: <!-- -->SGLang RL Team, Miles Team<!-- -->,<!-- --> <!-- -->Dec 03, 2025<!-- --></p><hr/><div class="pt-2 article"><blockquote>
<p><strong>TL;DR:</strong></p>
<p><strong>We have added FSDP to <a href="https://github.com/radixark/miles">Miles</a> as a more flexible training framework and have aligned it with Megatron. FSDP supports architecture-innovative models such as Qwen3-Next more flexibly and helps us further support VLM RL.</strong></p>
</blockquote>
<p>SGLang RL Team and the Miles community have conducted some interesting explorations around RL training stability and acceleration:</p>
<p><a href="https://github.com/radixark/miles/tree/main/examples/true_on_policy">Aligning the SGLang and FSDP backends</a> for <strong>strictly zero KL divergence</strong></p>
<p><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/slime/spec/readme-en.md">Speculative Decoding</a> with online SFT for the draft model</p>
<p><a href="https://lmsys.org/blog/2025-11-25-fp8-rl/">Unified FP8 RL</a>: Moving Beyond Mixed Precision for Stable and Accelerated MoE RL</p>
<p>Building on this, we now share a new progress that seeks the best adaptbility and usability to new model architectures, enable FSDP2 a more flexible training backend for Miles.</p>
<p>This work is jointly completed by the <strong>SGLang RL Team and Miles Team</strong>. Special thanks to <strong>DataCrunch, AtlasCloud and EigenAI</strong> for compute sponsorship.</p>
<h2><a id="background" class="anchor" href="#background" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Background</h2>
<h3><a id="what-is-fsdp" class="anchor" href="#what-is-fsdp" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What is FSDP?</h3>
<p><strong>FSDP (Fully Sharded Data Parallel)</strong> inherits the design philosophy of <a href="https://www.deepspeed.ai/2021/03/07/zero3-offload.html">DeepSpeed ZeRO Stage 3</a> and can be seen as a powerful optimization of traditional <a href="https://docs.pytorch.org/tutorials/beginner/ddp_series_theory.html">DDP (Distributed Data Parallel)</a>.</p>
<p><strong>From Replicate to Shard</strong></p>
<p>In traditional DDP, each GPU maintains a complete copy of model weights, gradients, and optimizer states (Replication), synchronizing gradients via <code>all-reduce</code>. In FSDP, we shift to a <strong>Sharding</strong> mode: all the aforementioned data is sharded and distributed across different GPU ranks.</p>
<ul>
<li><strong>Forward Propagation</strong>: When a layer needs to be calculated, full parameters are temporarily collected via <code>all-gather</code> and released immediately after calculation.</li>
<li><strong>Backward Propagation</strong>: After gradient calculation is complete, <code>reduce-scatter</code> is performed immediately to synchronize and shard, then the full gradients are released.</li>
</ul>
<p><strong>FSDP1 vs FSDP2</strong></p>
<p>Compared to FSDP1 which flattens all parameters into a giant <code>FlatParameter</code>, FSDP2 introduces <strong>DTensor (Distributed Tensor)</strong>. It allows for better sharding on specified parallel dimensions while preserving the original Tensor structure (such as shape, stride). This not only solves the pain points of volatile metadata and complex padding in FSDP1, but also provides out-of-the-box support for MixedPrecision Training and LoRA; FSDP mentioned in this article refers to <strong>FSDP2</strong> natively supported by PyTorch.</p>
<blockquote>
<p>✅ For more content about FSDP, you can check the previous blogs of the SGLang RL team: <a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/sys-design/readme-2-en.md"><strong>RL System Deep Dive: FSDP Training Backend</strong></a></p>
</blockquote>
<h3><a id="why-does-miles-need-fsdp" class="anchor" href="#why-does-miles-need-fsdp" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why does Miles need FSDP?</h3>
<p>Miles is an enterprise-facing reinforcement learning framework for large-scale MoE post-training and production workloads, forked from and co-evolving with <a href="https://github.com/THUDM/slime">slime</a>. People familiar with <a href="https://github.com/radixark/miles">Miles</a> know that we already have a mature training engine based on <a href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM</a>. Considering the significant maintenance cost brought by introducing a new backend, why are we still determined to support FSDP?</p>
<ol>
<li><strong>VLM Architecture Adaptation</strong>: The modal interaction architecture of VLM is complex, and FSDP's flexibility makes it much easier to adapt than Megatron. Therefore, we choose FSDP as the preferred path for VLM RL training (of course, Megatron version adaptation is also planned).</li>
<li><strong>Agility for Architecture Innovation</strong>: For new architectures under rapid iteration like Qwen3-Next, FSDP allows us to support RL processes with maximum speed.</li>
<li><strong>Low Barrier and High Usability</strong>: As a PyTorch native training backend, FSDP does not have complex environment dependencies and installation processes. Both the learning curve and debug cost are significantly lower than Megatron.</li>
<li><strong>Seamless Ecosystem Compatibility</strong>: FSDP is directly compatible with HuggingFace Model format. This means we don't need to perform tedious weight conversion via <code>mbridge</code> like when using Megatron, and community models work out of the box.</li>
</ol>
<blockquote>
<p>⚠️ Some models in Megatron now also do not require manual weight conversion, as it is automatically converted internally.</p>
</blockquote>
<h2><a id="fsdp-in-miles-architecture-design" class="anchor" href="#fsdp-in-miles-architecture-design" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>FSDP in Miles: Architecture Design</h2>
<p>To support two distinct distributed backends, Megatron and FSDP, in Miles simultaneously, how should we avoid underlying conflicts and keep the code clean? We adopted a top-level design of &quot;Interface Standardization + Physical Isolation&quot;, meaning we only expose core FSDP functions outwardly: <code>init</code>, <code>save</code>, <code>sleep</code>, <code>wake_up</code>, <code>train</code>. Other functions try to follow the underscore convention, like <code>_train_core</code>. Specifically:</p>
<p>We utilize the Ray Actor mechanism to encapsulate different backends in independent process spaces, exposing unified training primitives (such as <code>train</code>) to the upper-level scheduler, so that the upper-level algorithm logic does not need to care about the underlying gradient synchronization details. This design largely eliminates global variable conflicts and reduces conditional branch complexity, allowing us to deeply optimize for FSDP2's Sharding mechanism and DTensor structure. The core implementation is located in <code>miles/backends/fsdp_utils/actor.py</code>. While keeping external business logic (such as Data Packing, Context Parallel) highly consistent with Megatron, we refactored the data flow path in the kernel implementation, ensuring that while enjoying FSDP's flexibility, we maximize training efficiency and maintain numerical precision.</p>
<p>The robust FSDP design leaves the top-level architecture unaffected, and the overall process remains the standard RLHF loop: Rollout → Data Sharding → Packing → Forward/LogProb → Loss → Backward → Update. On this basis, we have made multiple optimizations for FSDP, including Data Packing, True On-Policy mode, CUDA Graph Aware Weight Wake Up, and numerous mitigation mechanisms for Training-Inference Mismatch. Next, we discuss the top-level <code>init</code> and <code>train</code> function entry points.</p>
<h3><a id="initialization" class="anchor" href="#initialization" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Initialization</h3>
<p>In the <code>init</code> stage, the following work is mainly completed:</p>
<p align="center">
  <img src="/images/blog/miles-fsdp/1_fsdp_init.png" alt="FSDP actor init flow" width="50%" />
</p>
<p>FSDP actor init flow</p>
<ul>
<li><strong>Model and Optimizer</strong>: Initialize Actor Model and Reference Model, and support resuming from Checkpoint; set <code>true_on_policy_mode</code> and Optimizer.</li>
<li><strong>Weight Updater</strong>: Supports two modes: Colocate (training tasks and inference tasks on the same group of GPUs) and Disaggregated (training tasks and inference tasks on different GPUs), used to synchronize trained weights back to the Inference Engine.</li>
<li><strong>Device Mesh</strong>: Build DP + CP communication topology based on <code>DeviceMesh</code>, and call <code>fully_shard</code> to shard parameters.</li>
<li><strong>Operator Optimization</strong>:
<ul>
<li>Force the training end to use operators consistent with SGLang via <code>enable_batch_invariant_mode</code>, eliminating the impact of batch size on calculation results.</li>
<li>Use <code>torch.compile</code> to solidify RoPE implementation, eliminating operator behavior differences at the bottom layer to ensure True On-Policy alignment.</li>
</ul>
</li>
</ul>
<h3><a id="training-flow" class="anchor" href="#training-flow" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Training Flow</h3>
<p>The <code>train</code> function serves as the main training entry point:</p>
<p align="center">
  <img src="/images/blog/miles-fsdp/2_fsdp_train.png" alt="FSDP actor train flow" width="50%" />
</p>
<p>FSDP actor train flow</p>
<ol>
<li><strong>wake up</strong>: Load the previously Offloaded Actor Model back to GPU.</li>
<li><strong>data preparation</strong>:
<ul>
<li>Get data required for the current DP rank via <code>process_rollout_data</code>.</li>
<li>Call <code>_pack_rollout_data</code> to pack data into <code>packed_batches</code> (see Appendix Data Packing for details), eliminating performance loss caused by Padding.</li>
</ul>
</li>
<li><strong>forward &amp; log prob</strong>:
<ul>
<li>Calculate log_prob and entropy for Actor and Ref.</li>
</ul>
</li>
<li><strong>loss calculation</strong>:
<ul>
<li>Calculate PPO/GRPO loss (importance ratio, clip, KL penalty, entropy bonus).</li>
<li><strong>mismatch feature</strong>: Real-time calculation of <code>train_rollout_logprob_abs_diff</code> to monitor numerical deviation between training and inference. Enable <strong>TIS (Truncated Importance Sampling)<a href="https://fengyao.notion.site/off-policy-rl#245721e3f6c48025aaeadec35aa6da9f">Source</a></strong> to re-weight policy gradient loss, preventing model collapse due to off-policyness caused by training-inference differences.</li>
</ul>
</li>
<li><strong>update &amp; offload</strong>:
<ul>
<li>Perform gradient accumulation and parameter update.</li>
<li><strong>offload strategy</strong>: Call <code>sleep</code> after training to offload model and optimizer to CPU (colocated mode); Ref model is loaded only when calculating log prob and offloaded immediately after use.</li>
</ul>
</li>
</ol>
<h2><a id="fsdp-in-miles-features--optimization" class="anchor" href="#fsdp-in-miles-features--optimization" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>FSDP in Miles Features &amp; Optimization</h2>
<p>Based on the architecture design, we further analyze the optimizations made so far.</p>
<h3><a id="data-prepare-and-packing" class="anchor" href="#data-prepare-and-packing" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data Prepare And Packing</h3>
<p>At the beginning of each training round, the FSDP actor (i.e., this actor class) first gets a batch of <strong>balanced</strong> rollout sequences from rollout, then does simple sample splitting by DP rank. This step is no different from conventional implementation. For extreme efficiency, we implemented <strong>Data Packing</strong>. Simply put, <code>pack_sequences</code> is processed in <code>miles/backends/fsdp_utils/data_packing.py</code>. For a batch of input sequences, we estimate how many packs are needed, i.e., the number of <code>micro-batch</code>es, based on the length of each sequence and <code>max_tokens_per_gpu</code>. Next, sequences of varying lengths are distributed into different packs so that the total tokens in each pack are as close as possible. Within each pack, multiple sequences are flattened into a long tokens vector, and <code>cu_seqlens</code> is constructed to record the start and end positions of each sequence. This strategy ensures that the total Token amount of each Pack is highly consistent, eliminating the computational waste caused by traditional Padding. Specific details can be found in the Appendix.</p>
<h3><a id="strict-training-inference-consistency" class="anchor" href="#strict-training-inference-consistency" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Strict Training-Inference Consistency</h3>
<p>After completing Data Packing, the actor calculates log-prob and entropy of ref/actor for the packed micro-batch. We implemented True On Policy on FSDP. That is, for the recently very popular training inference mismatch problem, we gave the strictest answer, achieving absolute consistency of logprob for the same policy model in training backend and inference backend, solving training-infer mismatch from the system level.</p>
<blockquote>
<p>✅ Briefly speaking, the implementation and idea of training-infer kl = 0 are as follows:</p>
<ul>
<li>Both Training and Inference use FlashAttn3 as backend to achieve bitwise equal.</li>
<li>Use DeepGEMM for matrix multiplication, Batch-invariant Kernels to achieve batch invariance.
Specific details are documented in more detail in Miles's Docs.</li>
</ul>
</blockquote>
<p align="center">
  <img src="/images/blog/miles-fsdp/3_kl_0.png" alt="training-rollout logprob diff = 0" width="50%" />
</p>
<p>We further optimize performance under true on policy conditions. <code>get_logprob_and_entropy_with_cp</code> directly reuses the temperature passed in by Rollout, and turns off <code>allow_compile</code> which may introduce deviation. Disabling compile will forbid compiling <code>selective_log_softmax_raw</code>, preventing estimation deviation caused by different calculation paths due to compilation and batch invariant. This ensures that the <code>log-prob</code> re-calculated at the training end can <strong>accurately restore</strong> the numerical performance during Rollout.</p>
<blockquote>
<p>⚠️ Here we discovered and solved an imperceptible Bug that caused on policy kl ≠ 0 when using kl-loss, see Appendix PPO KL Precision Error for details.</p>
</blockquote>
<h3><a id="algorithms-mitigation-for-mismatch" class="anchor" href="#algorithms-mitigation-for-mismatch" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Algorithms Mitigation For Mismatch</h3>
<p>The default set up in Miles and most of RL communities do not enable true on policy features, which would lose about 30% of training efficiency. So there will still be training-inference mismatch. For accuracy, we call the rollout policy log probs recorded during the rollout phase <code>rollout_log_probs</code>; after entering the training loop, the log probs of the policy model recalculated in the training backend are recorded as <code>old_log_probs</code>.</p>
<p>Without considering training-infer mismatch, the actor constructs loss in <code>_train_step</code> in the conventional GRPO/GSPO/PPO way. Specifically, each training step calculates the log probs of the current training data batch based on the current policy model, directly noted as <code>log_probs</code>. Use <code>old_log_probs</code> and <code>log_probs</code> to construct importance ratio, superimpose clip, KL norm and entropy bonus to get loss, then do gradient accumulation and optimizer backward.</p>
<p>Considering mismatch, <code>rollout_log_probs, old_log_probs, log_probs</code> will all participate in loss construction:</p>
<ul>
<li>In <code>_train_step</code> of <code>actor.py</code>, calculate the absolute difference <code>train_rollout_logprob_abs_diff</code> between <code>old_log_probs</code> and <code>rollout_log_probs</code> to quantify numerical deviation between training and inference in real-time.</li>
<li>Enable <strong>TIS (<a href="https://fengyao.notion.site/off-policy-rl#245721e3f6c48025aaeadec35aa6da9f">Truncated Importance Sampling</a>)</strong>. Calculate importance weight, i.e., <code>tis = torch.exp(old_log_probs - rollout_log_probs)</code>, and truncate (Clip) it, using this weight to re-weight Policy Gradient Loss (<code>pg_loss</code>). This method ensures that the model can still <strong>mitigate model training collapse</strong> even in a not-so-perfect on-policy environment (Thanks to the author teams of <a href="https://www.notion.so/271211a558b7808d8b12d403fd15edda?pvs=21">MIS</a> and <a href="https://fengyao.notion.site/off-policy-rl#245721e3f6c48025aaeadec35aa6da9f">TIS</a>).</li>
</ul>
<p>Taking GRPO as an example, the final loss function is:</p>
<p>$$
\mathcal{L}(\theta)
= \frac{1}{L} \sum_{t=1}^L \left[ \bar{w}_t \cdot \mathcal{L}^{\text{clip}}_t(\theta) - \beta ,\text{KL}_t + \lambda H_t \right]
$$</p>
<p>where</p>
<p>$$
\mathcal{L}^{\text{clip}}_t
= \min \left( r_t(\theta) A_t,\ \text{clip}(r_t(\theta), 1\pm\epsilon), A_t \right)
$$</p>
<p>and</p>
<p>$$
r_t(\theta) = \frac{\pi_\theta}{\pi_{\text{old}}}, \quad \bar{w}_t = \min \left( \frac{\pi_{\text{old}}}{\pi_{\text{rollout}}}, C \right)
$$</p>
<h3><a id="weight-update-optimization-weight-update-and-colocated-mode" class="anchor" href="#weight-update-optimization-weight-update-and-colocated-mode" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Weight Update Optimization: Weight Update and Colocated Mode</h3>
<p>After training ends, the latest weights are synchronized back to the Inference Engine (this is the best definition of the term refit). In <code>update_weight_utis.py</code>, we fully support all modes: <code>colocated</code> and <code>distributed</code>. The former alternates train / rollout occupying the same batch of GPUs, while the latter distributes train / rollout on different GPUs. For both methods, we adopted a bucketed asynchronous update strategy <a href="https://hebiao064.github.io/rl-weight-sync">Reference</a>, synchronizing chunked weights to the inference engine one by one, minimizing peak memory usage as much as possible.</p>
<p align="center">
  <img src="/images/blog/miles-fsdp/4_fsdp_refit.png" alt="Update weights from training to inference with async tensor handle and bucket" width="50%" />
</p>
<blockquote>
<p>✅ For specific mechanisms of weight update, welcome to check the previous blogs of SGLang RL group: <a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/sys-design/readme-1-EN.md"><strong>RL System Deep Thinking: Weight Update Mechanisms</strong></a></p>
</blockquote>
<h3><a id="vram-optimization-offload-strategy" class="anchor" href="#vram-optimization-offload-strategy" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>VRAM Optimization: Offload Strategy</h3>
<p>In the FSDP training process, we save memory by offloading weights in the following scenarios:</p>
<ul>
<li><strong>Train offload</strong>: In colocated scenarios, call <code>sleep</code> after training completes to offload model weights and optimizer to CPU, avoiding memory occupation during the rollout phase.</li>
<li><strong>Ref model</strong>: When using KL penalty, the reference model is only loaded to GPU during <code>compute_log_prob</code>, and offloaded back to CPU immediately after calculation completes, avoiding GPU occupation.</li>
<li><strong>Optimizer offload</strong>: During the training phase, model parameters are offloaded to CPU when not participating in calculation, and gradients are also offloaded to CPU; this significantly saves VRAM consumption during training, but optimizer steps will be performed on CPU, and training time will increase significantly.</li>
</ul>
<h2><a id="fsdpmegatron-training-precision-alignment" class="anchor" href="#fsdpmegatron-training-precision-alignment" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>FSDP/Megatron Training Precision Alignment</h2>
<p>Experimental Environment: Single node H100, Miles 0.5.5post1</p>
<p><a href="https://github.com/radixark/miles/blob/main/scripts/run-qwen3-4B-fsdp.sh">Script</a></p>
<p>Megatron, FSDP colocated w ref model, FSDP colocated w/o ref model</p>
<p align="center">
  <img src="/images/blog/miles-fsdp/5_fsdp_mcore_match.png" alt="Raw reward match" width="50%" />
</p>
<h3><a id="context-parallelism" class="anchor" href="#context-parallelism" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Context Parallelism</h3>
<p>For CP, we want to ensure that Megatron and FSDP can support similar response lengths under the same Context Parallelism degree:</p>
<blockquote>
<p>✅ Theoretically <code>max_reponse_length_with_cp = max_reponse_length_without_cp * cp_size</code> <a href="https://arxiv.org/pdf/2310.01889">ref link</a></p>
</blockquote>
<p>For experimental configuration: 4 B200s, global_batch_size = 64:</p>
<table>
<thead>
<tr>
<th></th>
<th>response_length = 8k</th>
<th>response_length = 16k</th>
</tr>
</thead>
<tbody>
<tr>
<td>FSDP, cp = 1</td>
<td>work</td>
<td><strong>OOM</strong></td>
</tr>
<tr>
<td>FSDP, cp = 2</td>
<td>work</td>
<td>work</td>
</tr>
<tr>
<td>Megatron(TP = 1), cp = 1</td>
<td>work</td>
<td><strong>OOM</strong></td>
</tr>
<tr>
<td>Megatron(TP = 1), cp = 2</td>
<td>work</td>
<td>work</td>
</tr>
</tbody>
</table>
<p>Experimental results meet expectations, and convergence effects are similar.</p>
<h2><a id="quick-start-fsdp-backend" class="anchor" href="#quick-start-fsdp-backend" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Quick Start FSDP Backend</h2>
<h3><a id="fsdp-one-click-start" class="anchor" href="#fsdp-one-click-start" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>FSDP One-Click Start</h3>
<pre><code class="hljs language-bash"><span class="hljs-comment"># If you need to use WANDB, you need to set the environment variable WANDB_API_KEY in advance</span>
<span class="hljs-comment"># Download model weights (Qwen3-4B)</span>
hf download Qwen/Qwen3-4B --local-dir /root/Qwen3-4B

<span class="hljs-comment"># Download training dataset (dapo-math-17k)</span>
hf download --repo-type dataset zhuzilin/dapo-math-17k \
  --local-dir /root/dapo-math-17k

<span class="hljs-comment"># Download evaluation dataset (aime-2024)</span>
hf download --repo-type dataset zhuzilin/aime-2024 \
  --local-dir /root/aime-2024
  
<span class="hljs-comment"># Clone code and install dependencies</span>
git <span class="hljs-built_in">clone</span> https://github.com/radixark/miles.git
<span class="hljs-built_in">cd</span> miles
pip install -e .


<span class="hljs-comment"># FSDP does not require weight conversion, natively supports huggingface format</span>
<span class="hljs-comment"># Enable reference model, train Qwen3-4B in colocate mode</span>
bash /root/miles/scripts/run-qwen3-4B-fsdp.sh
</code></pre>
<h3><a id="from-megatron-to-fsdp" class="anchor" href="#from-megatron-to-fsdp" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>From Megatron to FSDP</h3>
<p>FSDP automatically reads all architecture information via <code>AutoModelForCausalLM.from_pretrained()</code>, without manual specification. Megatron requires manual configuration of parameters to read model architecture information, or automatic inference via <code>--use-hf-config-for-megatron</code>. FSDP can read entirely from <code>config.json</code>, directly avoiding the weight format conversion step.</p>
<h3><a id="megatron-vs-fsdp-parameters-comparison-table" class="anchor" href="#megatron-vs-fsdp-parameters-comparison-table" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Megatron vs FSDP Parameters Comparison Table</h3>
<table>
<thead>
<tr>
<th>Configuration Category</th>
<th>Megatron Parameter</th>
<th>FSDP Parameter</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Model Loading</strong></td>
<td><code>--load</code> (Megatron checkpoint) + architecture args (<code>--num-layers</code>, <code>--hidden-size</code> etc.) or <code>--use-hf-config-for-megatron</code></td>
<td><code>--hf-checkpoint</code> (Required)</td>
<td><strong>FSDP</strong>: Directly uses HuggingFace format, no weight conversion needed, architecture inferred via <code>AutoConfig</code></td>
</tr>
<tr>
<td><strong>Tensor Parallel</strong></td>
<td><code>--tensor-model-parallel-size</code></td>
<td>Coming Soon</td>
<td></td>
</tr>
<tr>
<td><strong>Pipeline Parallel</strong></td>
<td><code>--pipeline-model-parallel-size</code></td>
<td>Coming Soon</td>
<td></td>
</tr>
<tr>
<td><strong>Expert Parallel</strong></td>
<td><code>--expert-model-parallel-size</code></td>
<td>Coming Soon</td>
<td></td>
</tr>
<tr>
<td><strong>Context Parallel</strong></td>
<td><code>--context-parallel-size</code></td>
<td><code>--context-parallel-size</code></td>
<td>Both support CP</td>
</tr>
<tr>
<td><strong>Initial Learning Rate</strong></td>
<td><code>--lr</code></td>
<td><code>--lr</code></td>
<td>Same parameter</td>
</tr>
<tr>
<td><strong>Learning Rate Decay</strong></td>
<td><code>--lr-decay-style</code> (linear/cosine)</td>
<td><code>--lr-decay-style</code> (only constant)</td>
<td></td>
</tr>
<tr>
<td><strong>Warmup</strong></td>
<td><code>--lr-warmup-iters</code> (steps)</td>
<td>Coming Soon</td>
<td></td>
</tr>
<tr>
<td><strong>Min Learning Rate</strong></td>
<td><code>--min-lr</code></td>
<td>Coming Soon</td>
<td></td>
</tr>
<tr>
<td><strong>Optimizer Type</strong></td>
<td><code>--optimizer</code> (adam/sgd etc.)</td>
<td><code>--optimizer</code> (default adam)</td>
<td>Basically same</td>
</tr>
<tr>
<td><strong>Distributed Optimizer</strong></td>
<td><code>--use-distributed-optimizer</code></td>
<td>Built-in to FSDP</td>
<td>FSDP uses distributed optimizer by default</td>
</tr>
<tr>
<td><strong>Gradient Checkpoint</strong></td>
<td><code>--recompute-granularity</code>, <code>--recompute-method</code></td>
<td><code>--gradient-checkpointing</code></td>
<td><strong>FSDP</strong>: Simplified to boolean switch</td>
</tr>
<tr>
<td><strong>CPU Offload</strong></td>
<td>Implemented via distributed optimizer</td>
<td><code>--fsdp-cpu-offload</code></td>
<td><strong>FSDP</strong>: Offload parameters/gradients/optimizer states to CPU</td>
</tr>
<tr>
<td><strong>Attention Backend</strong></td>
<td>Decided by Megatron Core</td>
<td><code>--attn-implementation</code> (flash_attention_2/sdpa/eager)</td>
<td><strong>FSDP</strong>: Directly passed to HuggingFace</td>
</tr>
<tr>
<td><strong>Mixed Precision</strong></td>
<td><code>--fp16</code> or <code>--bf16</code></td>
<td><code>--fp16</code> (bf16 inferred automatically)</td>
<td>Basically same</td>
</tr>
<tr>
<td><strong>Offload on Save</strong></td>
<td>-</td>
<td><code>--fsdp-state-dict-cpu-offload</code> (Default True)</td>
<td><strong>FSDP</strong>: Offload to CPU when saving checkpoint</td>
</tr>
<tr>
<td><strong>Training Backend</strong></td>
<td>Default or <code>--train-backend megatron</code></td>
<td><code>--train-backend fsdp</code> (Required)</td>
<td>Used to switch backend</td>
</tr>
</tbody>
</table>
<h2><a id="features-currently-not-supported-in-fsdp" class="anchor" href="#features-currently-not-supported-in-fsdp" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Features Currently Not Supported in FSDP</h2>
<p>FSDP currently only supports <strong>DP + CP</strong>, and does not support <strong>TP, EP, PP</strong>. The implementation of CP is different from Megatron.</p>
<p>Megatron Core has native implementation (deeply integrated with TP/PP), while FSDP implements it via external Ring Flash Attention library.</p>
<p>In addition, Megatron's <code>--recompute-granularity</code> (full/selective), <code>--recompute-method</code> (uniform/block), <code>--recompute-num-layers</code> are not supported. FSDP only has a simple <code>--gradient-checkpointing</code> switch.</p>
<p>Finally, FSDP optimizer's learning rate currently only supports being set to constant, and there is no warmup strategy.</p>
<h2><a id="future-plans" class="anchor" href="#future-plans" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Future Plans</h2>
<p>As a lightweight backend, our future plans for FSDP include the following directions:</p>
<ul>
<li>Implement TP and EP while maintaining clean and tidy code.</li>
<li>Add a set of FSDP VLM training capabilities and corresponding scripts: Prioritize Qwen2.5-VL / Qwen3-VL (HF default weights) models, using Geo3K / Deepeyes datasets to implement and test single-turn and multi-turn VLM RL Training respectively, and finally support vision + language joint training or partial freezing on FSDP2.</li>
<li>Support Qwen3-next and other hybrid models for training and optimization.</li>
</ul>
<h2><a id="acknowledgements" class="anchor" href="#acknowledgements" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgements</h2>
<p>Thanks to all friends who contributed code, testing, and optimization to miles X FSDP:</p>
<p>SGLang RL team: Chengxi Li, Zilin Zhu, Chengxing Xie, Haoran Wang, Lei Li, Yusheng Su, Zhuohao Li, Ji Li, Jiahui Wang, Jin Pan, William Ren, Qisheng Liu, Yuzhen Zhou, Jiajun Li, Yuqi Xiang</p>
<p>Miles Team: Huapeng Zhou, Mao Cheng, Chenyang Zhao, Tom</p>
<p>We sincerely thank the AtlasCloud and DataCrunch for their computing support.</p>
<p>Linkedin: Lancert</p>
<h2><a id="appendix" class="anchor" href="#appendix" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Appendix</h2>
<details>
<summary>Engineering Implementation Details</summary>
<h3><a id="context-parallel" class="anchor" href="#context-parallel" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Context Parallel</h3>
<p>FSDP's CP is directly implemented via <a href="https://github.com/zhuzilin/ring-flash-attention">ring flash attention</a> library. Compared to Megatron's complex chunk mechanism, FSDP only needs to implement simple continuous chunks, and the load balancing part is handed over to ring flash attn. We only need to focus on input data slicing and result aggregation.</p>
<p><strong>Specific implementation flow is as follows:</strong></p>
<ol>
<li><strong>Device Mesh Setup:</strong> Establish (DP, CP) 2D communication group in <code>setup_device_mesh</code>, and use <code>substitute_hf_flash_attn</code> to replace HuggingFace model's original Flash Attention operator with Ring Flash Attention implementation supporting CP.</li>
<li><strong>Input Slicing:</strong> In the <code>_get_model_inputs_args</code> stage before forward, we directly use <code>torch.chunk</code> to slice Data Packed <code>input_ids</code> and <code>position_ids</code> into <code>cp_size</code> parts on the sequence dimension. The current rank only loads its own part of data. Meanwhile, call <code>update_ring_flash_attn_params</code> to pass global <code>cu_seqlens</code> info to the underlying Attention operator.</li>
<li><strong>Result Gathering</strong>: When calculating Log Probs (<code>get_logprob_and_entropy_with_cp</code>), each rank calculates local shard's log_probs and entropy in parallel. Finally, splice the results distributed on different ranks back into a complete sequence via <code>all_gather</code>, and remove Padding filled to meet CP alignment requirements.</li>
</ol>
<h3><a id="data-packing" class="anchor" href="#data-packing" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data Packing</h3>
<p>To avoid waste caused by large amounts of padding on each CP rank due to direct padding, we splice long sequences into continuous vectors and use <code>cu_seqlens</code> to record boundaries. We first reused megatron's <code>process_rollout_data()</code> to split rollout by DP rank, then <code>packed_data</code> estimates how many <code>micro_batch</code>es are needed to complete a <code>global_batch</code> based on rollout token count and DP size. The relationship between <code>global_batch</code> and <code>micro_batch</code> in miles is seen in Batch &amp; Sample.</p>
<ul>
<li>When <code>use_dynamic_batch_size</code> is enabled, the number of micro-batches needs to be dynamically calculated based on actual sequence length: Use First-Fit algorithm via <code>get_minimum_num_micro_batch_size()</code> to estimate the minimum number of micro-batches needed to accommodate all data based on each sequence's length and <code>max_tokens_per_gpu</code> limit. This number will be synchronized across all DP ranks via <code>all_reduce(MAX)</code> to ensure consistent gradient accumulation steps for each rank.</li>
<li>If dynamic batch size is not enabled, directly use static formula global_batch_size // (micro_batch_size * dp_size) to calculate fixed micro-batch count.</li>
</ul>
<p>Next, execute actual packing operation in <code>pack_sequences()</code>:</p>
<ul>
<li>Calculate partition count <code>k_partitions = ceil(total_tokens / max_tokens_per_gpu)</code></li>
<li>Call <code>get_seqlen_balanced_partitions()</code> to perform load balanced allocation using <a href="https://en.wikipedia.org/wiki/Largest_differencing_method">Karmarkar-Karp</a> algorithm (Largest Differencing Method). This algorithm maintains partition states via priority queue, merging the two partitions with the largest token total difference each time, making the final token count of each pack highly balanced.</li>
<li>For each pack, splice assigned sequences into continuous <code>flat_tokens</code> vector, and construct <code>cu_seqlens</code> array to record each sequence's boundaries, e.g., <code>[0, 128, 384, 512]</code> means 3 sequences with lengths 128, 256, 128 respectively.</li>
</ul>
<p>In Context Parallel mode (<code>cp_size &gt; 1</code>), <code>pad_packed_sequence_with_cp()</code> will perform minimum alignment padding (at most cp_size-1 tokens) on the spliced sequence, ensuring total length can be divisible by cp_size for cross-rank slicing. Although this is still naive direct padding, since padding ≤ cp_size -1, it will not cause visible overhead.</p>
<p>During training, <code>cu_seqlens</code> is directly passed to Flash Attention to handle variable length sequences; when calculating loss, <code>unpack_sequences()</code> accurately restores indicators like log_probs, advantages for each sequence based on boundary information. This method basically avoids overhead caused by naive padding.</p>
<h3><a id="ppo-kl-precision-error" class="anchor" href="#ppo-kl-precision-error" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>PPO KL Precision Error</h3>
<p>The PPO training process involves three batch-related parameters: Batch, Micro batch size &amp; Sample.</p>
<p>Ideally, when <code>sample</code> count × <code>micro_batch_size</code> = <code>global_batch_size</code>, it means all samples generated in one rollout (sample count × prompts processed per batch) exactly equal one complete training batch. At this time, rollout phase and training phase use <strong>the same unupdated actor weight version</strong>.</p>
<ul>
<li>Use weight <code>W_t</code> to generate responses during Rollout.</li>
<li>Still use weight <code>W_t</code> to calculate log probabilities during training.</li>
</ul>
<p>Therefore, theoretically PPO KL divergence should be 0. However, in actual operation (only when reference model is enabled), KL divergence maintains a small positive value instead of 0 starting from the first micro batch, indicating a numerical drift problem.</p>
<p>This problem is caused by precision errors in weight exchange logic. The original implementation referred to Megatron's way, manually exchanging ref and actor tensors between CPU and GPU. To be compatible with FSDP2's DTensor, we manually created DTensor for swap. However, manual weight exchange leads to slight numerical deviations during weight loading. Megatron uses this manual exchange because the offload process of distributed optimizer is very complex, so it simply exchanges weights directly.</p>
<p>Finally, we switched to a cleaner solution: treat reference model as an independent FSDP model, use FSDP native CPU Offload for management, and load it to GPU only during forward. This method completely avoids manual weight exchange, fully utilizes FSDP native CPU/GPU transfer mechanism, eliminates numerical drift from the root cause, making PPO KL converge to theoretical value 0, while not introducing additional GPU memory overhead.</p>
<h3><a id="true-on-policy" class="anchor" href="#true-on-policy" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><strong>True on policy</strong></h3>
<p>After the CP PR was merged, the true on policy of the main branch actually failed. After investigation, it was found that precision was autocast to bf16 after indentation. After fixing, training-infer mismatch was successfully restored to 0.</p>
<p>To avoid precision problems caused by improper application of auto cast, we finally chose <a href="https://docs.pytorch.org/tutorials/intermediate/FSDP_advanced_tutorial.html#mixed-precision">Mixed Precision</a> newly supported by FSDP2, implementing clearer and cleaner precision management.</p>
<h3><a id="batch--sample" class="anchor" href="#batch--sample" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Batch &amp; Sample</h3>
<ul>
<li><strong>Sample count (n-samples-per-prompt)</strong>: Number of candidate responses generated per prompt</li>
<li><strong>Micro batch size</strong>: Number of samples processed per forward/backward pass during training (limited by GPU VRAM)</li>
<li><strong>Global batch size</strong>: Total samples for a complete training iteration, usually completed by gradient accumulation of multiple micro batches</li>
</ul>
</details>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Power Up FSDP2 as a Flexible Training Backend for Miles","author":"SGLang RL Team, Miles Team","date":"December 3, 2025","previewImg":"/images/blog/miles-fsdp/2_fsdp_train.png"},"content":"\n\u003e **TL;DR:**\n\u003e \n\u003e **We have added FSDP to [Miles](https://github.com/radixark/miles) as a more flexible training framework and have aligned it with Megatron. FSDP supports architecture-innovative models such as Qwen3-Next more flexibly and helps us further support VLM RL.**\n\nSGLang RL Team and the Miles community have conducted some interesting explorations around RL training stability and acceleration:\n\n[Aligning the SGLang and FSDP backends](https://github.com/radixark/miles/tree/main/examples/true_on_policy) for **strictly zero KL divergence**\n\n[Speculative Decoding](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/slime/spec/readme-en.md) with online SFT for the draft model\n\n[Unified FP8 RL](https://lmsys.org/blog/2025-11-25-fp8-rl/): Moving Beyond Mixed Precision for Stable and Accelerated MoE RL\n\nBuilding on this, we now share a new progress that seeks the best adaptbility and usability to new model architectures, enable FSDP2 a more flexible training backend for Miles.\n\nThis work is jointly completed by the **SGLang RL Team and Miles Team**. Special thanks to **DataCrunch, AtlasCloud and EigenAI** for compute sponsorship.\n\n## Background\n\n### What is FSDP?\n\n**FSDP (Fully Sharded Data Parallel)** inherits the design philosophy of [DeepSpeed ZeRO Stage 3](https://www.deepspeed.ai/2021/03/07/zero3-offload.html) and can be seen as a powerful optimization of traditional [DDP (Distributed Data Parallel)](https://docs.pytorch.org/tutorials/beginner/ddp_series_theory.html).\n\n**From Replicate to Shard**\n\nIn traditional DDP, each GPU maintains a complete copy of model weights, gradients, and optimizer states (Replication), synchronizing gradients via `all-reduce`. In FSDP, we shift to a **Sharding** mode: all the aforementioned data is sharded and distributed across different GPU ranks.\n\n- **Forward Propagation**: When a layer needs to be calculated, full parameters are temporarily collected via `all-gather` and released immediately after calculation.\n- **Backward Propagation**: After gradient calculation is complete, `reduce-scatter` is performed immediately to synchronize and shard, then the full gradients are released.\n\n**FSDP1 vs FSDP2**\n\nCompared to FSDP1 which flattens all parameters into a giant `FlatParameter`, FSDP2 introduces **DTensor (Distributed Tensor)**. It allows for better sharding on specified parallel dimensions while preserving the original Tensor structure (such as shape, stride). This not only solves the pain points of volatile metadata and complex padding in FSDP1, but also provides out-of-the-box support for MixedPrecision Training and LoRA; FSDP mentioned in this article refers to **FSDP2** natively supported by PyTorch.\n\n\u003e ✅ For more content about FSDP, you can check the previous blogs of the SGLang RL team: [**RL System Deep Dive: FSDP Training Backend**](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/sys-design/readme-2-en.md)\n\n### Why does Miles need FSDP?\n\nMiles is an enterprise-facing reinforcement learning framework for large-scale MoE post-training and production workloads, forked from and co-evolving with [slime](https://github.com/THUDM/slime). People familiar with [Miles](https://github.com/radixark/miles) know that we already have a mature training engine based on [Megatron-LM](https://github.com/NVIDIA/Megatron-LM). Considering the significant maintenance cost brought by introducing a new backend, why are we still determined to support FSDP?\n\n1. **VLM Architecture Adaptation**: The modal interaction architecture of VLM is complex, and FSDP's flexibility makes it much easier to adapt than Megatron. Therefore, we choose FSDP as the preferred path for VLM RL training (of course, Megatron version adaptation is also planned).\n2. **Agility for Architecture Innovation**: For new architectures under rapid iteration like Qwen3-Next, FSDP allows us to support RL processes with maximum speed.\n3. **Low Barrier and High Usability**: As a PyTorch native training backend, FSDP does not have complex environment dependencies and installation processes. Both the learning curve and debug cost are significantly lower than Megatron.\n4. **Seamless Ecosystem Compatibility**: FSDP is directly compatible with HuggingFace Model format. This means we don't need to perform tedious weight conversion via `mbridge` like when using Megatron, and community models work out of the box.\n\n\u003e ⚠️ Some models in Megatron now also do not require manual weight conversion, as it is automatically converted internally.\n\n## FSDP in Miles: Architecture Design\n\nTo support two distinct distributed backends, Megatron and FSDP, in Miles simultaneously, how should we avoid underlying conflicts and keep the code clean? We adopted a top-level design of \"Interface Standardization + Physical Isolation\", meaning we only expose core FSDP functions outwardly: `init`, `save`, `sleep`, `wake_up`, `train`. Other functions try to follow the underscore convention, like `_train_core`. Specifically:\n\nWe utilize the Ray Actor mechanism to encapsulate different backends in independent process spaces, exposing unified training primitives (such as `train`) to the upper-level scheduler, so that the upper-level algorithm logic does not need to care about the underlying gradient synchronization details. This design largely eliminates global variable conflicts and reduces conditional branch complexity, allowing us to deeply optimize for FSDP2's Sharding mechanism and DTensor structure. The core implementation is located in `miles/backends/fsdp_utils/actor.py`. While keeping external business logic (such as Data Packing, Context Parallel) highly consistent with Megatron, we refactored the data flow path in the kernel implementation, ensuring that while enjoying FSDP's flexibility, we maximize training efficiency and maintain numerical precision.\n\nThe robust FSDP design leaves the top-level architecture unaffected, and the overall process remains the standard RLHF loop: Rollout → Data Sharding → Packing → Forward/LogProb → Loss → Backward → Update. On this basis, we have made multiple optimizations for FSDP, including Data Packing, True On-Policy mode, CUDA Graph Aware Weight Wake Up, and numerous mitigation mechanisms for Training-Inference Mismatch. Next, we discuss the top-level `init` and `train` function entry points.\n\n### Initialization\n\nIn the `init` stage, the following work is mainly completed:\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/miles-fsdp/1_fsdp_init.png\" alt=\"FSDP actor init flow\" width=\"50%\" /\u003e\n\u003c/p\u003e\n\nFSDP actor init flow\n\n- **Model and Optimizer**: Initialize Actor Model and Reference Model, and support resuming from Checkpoint; set `true_on_policy_mode` and Optimizer.\n- **Weight Updater**: Supports two modes: Colocate (training tasks and inference tasks on the same group of GPUs) and Disaggregated (training tasks and inference tasks on different GPUs), used to synchronize trained weights back to the Inference Engine.\n- **Device Mesh**: Build DP + CP communication topology based on `DeviceMesh`, and call `fully_shard` to shard parameters.\n- **Operator Optimization**:\n    - Force the training end to use operators consistent with SGLang via `enable_batch_invariant_mode`, eliminating the impact of batch size on calculation results.\n    - Use `torch.compile` to solidify RoPE implementation, eliminating operator behavior differences at the bottom layer to ensure True On-Policy alignment.\n\n### Training Flow\n\nThe `train` function serves as the main training entry point:\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/miles-fsdp/2_fsdp_train.png\" alt=\"FSDP actor train flow\" width=\"50%\" /\u003e\n\u003c/p\u003e\n\nFSDP actor train flow\n\n1. **wake up**: Load the previously Offloaded Actor Model back to GPU.\n2. **data preparation**:\n    - Get data required for the current DP rank via `process_rollout_data`.\n    - Call `_pack_rollout_data` to pack data into `packed_batches` (see Appendix Data Packing for details), eliminating performance loss caused by Padding.\n3. **forward \u0026 log prob**:\n    - Calculate log_prob and entropy for Actor and Ref.\n4. **loss calculation**:\n    - Calculate PPO/GRPO loss (importance ratio, clip, KL penalty, entropy bonus).\n    - **mismatch feature**: Real-time calculation of `train_rollout_logprob_abs_diff` to monitor numerical deviation between training and inference. Enable **TIS (Truncated Importance Sampling)[Source](https://fengyao.notion.site/off-policy-rl#245721e3f6c48025aaeadec35aa6da9f)** to re-weight policy gradient loss, preventing model collapse due to off-policyness caused by training-inference differences.\n5. **update \u0026 offload**:\n    - Perform gradient accumulation and parameter update.\n    - **offload strategy**: Call `sleep` after training to offload model and optimizer to CPU (colocated mode); Ref model is loaded only when calculating log prob and offloaded immediately after use.\n\n## FSDP in Miles Features \u0026 Optimization\n\nBased on the architecture design, we further analyze the optimizations made so far.\n\n### Data Prepare And Packing\n\nAt the beginning of each training round, the FSDP actor (i.e., this actor class) first gets a batch of **balanced** rollout sequences from rollout, then does simple sample splitting by DP rank. This step is no different from conventional implementation. For extreme efficiency, we implemented **Data Packing**. Simply put, `pack_sequences` is processed in `miles/backends/fsdp_utils/data_packing.py`. For a batch of input sequences, we estimate how many packs are needed, i.e., the number of `micro-batch`es, based on the length of each sequence and `max_tokens_per_gpu`. Next, sequences of varying lengths are distributed into different packs so that the total tokens in each pack are as close as possible. Within each pack, multiple sequences are flattened into a long tokens vector, and `cu_seqlens` is constructed to record the start and end positions of each sequence. This strategy ensures that the total Token amount of each Pack is highly consistent, eliminating the computational waste caused by traditional Padding. Specific details can be found in the Appendix.\n\n### Strict Training-Inference Consistency\n\nAfter completing Data Packing, the actor calculates log-prob and entropy of ref/actor for the packed micro-batch. We implemented True On Policy on FSDP. That is, for the recently very popular training inference mismatch problem, we gave the strictest answer, achieving absolute consistency of logprob for the same policy model in training backend and inference backend, solving training-infer mismatch from the system level.\n\n\u003e ✅ Briefly speaking, the implementation and idea of training-infer kl = 0 are as follows:\n\u003e - Both Training and Inference use FlashAttn3 as backend to achieve bitwise equal.\n\u003e - Use DeepGEMM for matrix multiplication, Batch-invariant Kernels to achieve batch invariance.\n\u003e Specific details are documented in more detail in Miles's Docs.\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/miles-fsdp/3_kl_0.png\" alt=\"training-rollout logprob diff = 0\" width=\"50%\" /\u003e\n\u003c/p\u003e\n\n\nWe further optimize performance under true on policy conditions. `get_logprob_and_entropy_with_cp` directly reuses the temperature passed in by Rollout, and turns off `allow_compile` which may introduce deviation. Disabling compile will forbid compiling `selective_log_softmax_raw`, preventing estimation deviation caused by different calculation paths due to compilation and batch invariant. This ensures that the `log-prob` re-calculated at the training end can **accurately restore** the numerical performance during Rollout.\n\n\u003e ⚠️ Here we discovered and solved an imperceptible Bug that caused on policy kl ≠ 0 when using kl-loss, see Appendix PPO KL Precision Error for details.\n\n### Algorithms Mitigation For Mismatch\n\nThe default set up in Miles and most of RL communities do not enable true on policy features, which would lose about 30% of training efficiency. So there will still be training-inference mismatch. For accuracy, we call the rollout policy log probs recorded during the rollout phase `rollout_log_probs`; after entering the training loop, the log probs of the policy model recalculated in the training backend are recorded as `old_log_probs`.\n\nWithout considering training-infer mismatch, the actor constructs loss in `_train_step` in the conventional GRPO/GSPO/PPO way. Specifically, each training step calculates the log probs of the current training data batch based on the current policy model, directly noted as `log_probs`. Use `old_log_probs` and `log_probs` to construct importance ratio, superimpose clip, KL norm and entropy bonus to get loss, then do gradient accumulation and optimizer backward.\n\nConsidering mismatch, `rollout_log_probs, old_log_probs, log_probs` will all participate in loss construction:\n\n- In `_train_step` of `actor.py`, calculate the absolute difference `train_rollout_logprob_abs_diff` between `old_log_probs` and `rollout_log_probs` to quantify numerical deviation between training and inference in real-time.\n- Enable **TIS ([Truncated Importance Sampling](https://fengyao.notion.site/off-policy-rl#245721e3f6c48025aaeadec35aa6da9f))**. Calculate importance weight, i.e., `tis = torch.exp(old_log_probs - rollout_log_probs)`, and truncate (Clip) it, using this weight to re-weight Policy Gradient Loss (`pg_loss`). This method ensures that the model can still **mitigate model training collapse** even in a not-so-perfect on-policy environment (Thanks to the author teams of [MIS](https://www.notion.so/271211a558b7808d8b12d403fd15edda?pvs=21) and [TIS](https://fengyao.notion.site/off-policy-rl#245721e3f6c48025aaeadec35aa6da9f)).\n\nTaking GRPO as an example, the final loss function is:\n\n$$\n\\mathcal{L}(\\theta)\n= \\frac{1}{L} \\sum_{t=1}^L \\left[ \\bar{w}_t \\cdot \\mathcal{L}^{\\text{clip}}_t(\\theta) - \\beta \\,\\text{KL}_t + \\lambda H_t \\right]\n$$\n\nwhere\n\n$$\n\\mathcal{L}^{\\text{clip}}_t\n= \\min \\left( r_t(\\theta) A_t,\\ \\text{clip}(r_t(\\theta), 1\\pm\\epsilon)\\, A_t \\right)\n$$\n\nand\n\n$$\nr_t(\\theta) = \\frac{\\pi_\\theta}{\\pi\\_{\\text{old}}}, \\quad \\bar{w}_t = \\min \\left( \\frac{\\pi\\_{\\text{old}}}{\\pi\\_{\\text{rollout}}}, C \\right)\n$$\n\n### Weight Update Optimization: Weight Update and Colocated Mode\n\nAfter training ends, the latest weights are synchronized back to the Inference Engine (this is the best definition of the term refit). In `update_weight_utis.py`, we fully support all modes: `colocated` and `distributed`. The former alternates train / rollout occupying the same batch of GPUs, while the latter distributes train / rollout on different GPUs. For both methods, we adopted a bucketed asynchronous update strategy [Reference](https://hebiao064.github.io/rl-weight-sync), synchronizing chunked weights to the inference engine one by one, minimizing peak memory usage as much as possible.\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/miles-fsdp/4_fsdp_refit.png\" alt=\"Update weights from training to inference with async tensor handle and bucket\" width=\"50%\" /\u003e\n\u003c/p\u003e\n\n\u003e ✅ For specific mechanisms of weight update, welcome to check the previous blogs of SGLang RL group: [**RL System Deep Thinking: Weight Update Mechanisms**](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/sys-design/readme-1-EN.md)\n\n\n### VRAM Optimization: Offload Strategy\n\nIn the FSDP training process, we save memory by offloading weights in the following scenarios:\n\n- **Train offload**: In colocated scenarios, call `sleep` after training completes to offload model weights and optimizer to CPU, avoiding memory occupation during the rollout phase.\n- **Ref model**: When using KL penalty, the reference model is only loaded to GPU during `compute_log_prob`, and offloaded back to CPU immediately after calculation completes, avoiding GPU occupation.\n- **Optimizer offload**: During the training phase, model parameters are offloaded to CPU when not participating in calculation, and gradients are also offloaded to CPU; this significantly saves VRAM consumption during training, but optimizer steps will be performed on CPU, and training time will increase significantly.\n\n## FSDP/Megatron Training Precision Alignment\n\nExperimental Environment: Single node H100, Miles 0.5.5post1\n\n[Script](https://github.com/radixark/miles/blob/main/scripts/run-qwen3-4B-fsdp.sh)\n\nMegatron, FSDP colocated w ref model, FSDP colocated w/o ref model\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/images/blog/miles-fsdp/5_fsdp_mcore_match.png\" alt=\"Raw reward match\" width=\"50%\" /\u003e\n\u003c/p\u003e\n\n\n\n### Context Parallelism\n\nFor CP, we want to ensure that Megatron and FSDP can support similar response lengths under the same Context Parallelism degree:\n\n\u003e ✅ Theoretically `max_reponse_length_with_cp = max_reponse_length_without_cp * cp_size` [ref link](https://arxiv.org/pdf/2310.01889)\n\nFor experimental configuration: 4 B200s, global_batch_size = 64:\n\n| | response_length = 8k | response_length = 16k |\n| --- | --- | --- |\n| FSDP, cp = 1 | work | **OOM** |\n| FSDP, cp = 2 | work | work |\n| Megatron(TP = 1), cp = 1 | work | **OOM** |\n| Megatron(TP = 1), cp = 2 | work | work |\n\nExperimental results meet expectations, and convergence effects are similar.\n\n## Quick Start FSDP Backend\n\n### FSDP One-Click Start\n\n```bash\n# If you need to use WANDB, you need to set the environment variable WANDB_API_KEY in advance\n# Download model weights (Qwen3-4B)\nhf download Qwen/Qwen3-4B --local-dir /root/Qwen3-4B\n\n# Download training dataset (dapo-math-17k)\nhf download --repo-type dataset zhuzilin/dapo-math-17k \\\n  --local-dir /root/dapo-math-17k\n\n# Download evaluation dataset (aime-2024)\nhf download --repo-type dataset zhuzilin/aime-2024 \\\n  --local-dir /root/aime-2024\n  \n# Clone code and install dependencies\ngit clone https://github.com/radixark/miles.git\ncd miles\npip install -e .\n\n\n# FSDP does not require weight conversion, natively supports huggingface format\n# Enable reference model, train Qwen3-4B in colocate mode\nbash /root/miles/scripts/run-qwen3-4B-fsdp.sh\n```\n\n### From Megatron to FSDP\n\nFSDP automatically reads all architecture information via `AutoModelForCausalLM.from_pretrained()`, without manual specification. Megatron requires manual configuration of parameters to read model architecture information, or automatic inference via `--use-hf-config-for-megatron`. FSDP can read entirely from `config.json`, directly avoiding the weight format conversion step.\n\n\n### Megatron vs FSDP Parameters Comparison Table\n\n| Configuration Category | Megatron Parameter | FSDP Parameter | Description |\n| --- | --- | --- | --- |\n| **Model Loading** | `--load` (Megatron checkpoint) + architecture args (`--num-layers`, `--hidden-size` etc.) or `--use-hf-config-for-megatron` | `--hf-checkpoint` (Required) | **FSDP**: Directly uses HuggingFace format, no weight conversion needed, architecture inferred via `AutoConfig` |\n| **Tensor Parallel** | `--tensor-model-parallel-size` | Coming Soon | |\n| **Pipeline Parallel** | `--pipeline-model-parallel-size` | Coming Soon | |\n| **Expert Parallel** | `--expert-model-parallel-size` | Coming Soon | |\n| **Context Parallel** | `--context-parallel-size` | `--context-parallel-size` | Both support CP |\n| **Initial Learning Rate** | `--lr` | `--lr` | Same parameter |\n| **Learning Rate Decay** | `--lr-decay-style` (linear/cosine) | `--lr-decay-style` (only constant) | |\n| **Warmup** | `--lr-warmup-iters` (steps) | Coming Soon | |\n| **Min Learning Rate** | `--min-lr` | Coming Soon | |\n| **Optimizer Type** | `--optimizer` (adam/sgd etc.) | `--optimizer` (default adam) | Basically same |\n| **Distributed Optimizer** | `--use-distributed-optimizer` | Built-in to FSDP | FSDP uses distributed optimizer by default |\n| **Gradient Checkpoint** | `--recompute-granularity`, `--recompute-method` | `--gradient-checkpointing` | **FSDP**: Simplified to boolean switch |\n| **CPU Offload** | Implemented via distributed optimizer | `--fsdp-cpu-offload` | **FSDP**: Offload parameters/gradients/optimizer states to CPU |\n| **Attention Backend** | Decided by Megatron Core | `--attn-implementation` (flash_attention_2/sdpa/eager) | **FSDP**: Directly passed to HuggingFace |\n| **Mixed Precision** | `--fp16` or `--bf16` | `--fp16` (bf16 inferred automatically) | Basically same |\n| **Offload on Save** | - | `--fsdp-state-dict-cpu-offload` (Default True) | **FSDP**: Offload to CPU when saving checkpoint |\n| **Training Backend** | Default or `--train-backend megatron` | `--train-backend fsdp` (Required) | Used to switch backend |\n\n## Features Currently Not Supported in FSDP\n\nFSDP currently only supports **DP + CP**, and does not support **TP, EP, PP**. The implementation of CP is different from Megatron.\n\nMegatron Core has native implementation (deeply integrated with TP/PP), while FSDP implements it via external Ring Flash Attention library.\n\nIn addition, Megatron's `--recompute-granularity` (full/selective), `--recompute-method` (uniform/block), `--recompute-num-layers` are not supported. FSDP only has a simple `--gradient-checkpointing` switch.\n\nFinally, FSDP optimizer's learning rate currently only supports being set to constant, and there is no warmup strategy.\n\n## Future Plans\n\nAs a lightweight backend, our future plans for FSDP include the following directions:\n\n- Implement TP and EP while maintaining clean and tidy code.\n- Add a set of FSDP VLM training capabilities and corresponding scripts: Prioritize Qwen2.5-VL / Qwen3-VL (HF default weights) models, using Geo3K / Deepeyes datasets to implement and test single-turn and multi-turn VLM RL Training respectively, and finally support vision + language joint training or partial freezing on FSDP2.\n- Support Qwen3-next and other hybrid models for training and optimization.\n\n## Acknowledgements\n\nThanks to all friends who contributed code, testing, and optimization to miles X FSDP:\n\nSGLang RL team: Chengxi Li, Zilin Zhu, Chengxing Xie, Haoran Wang, Lei Li, Yusheng Su, Zhuohao Li, Ji Li, Jiahui Wang, Jin Pan, William Ren, Qisheng Liu, Yuzhen Zhou, Jiajun Li, Yuqi Xiang\n\nMiles Team: Huapeng Zhou, Mao Cheng, Chenyang Zhao, Tom\n\nWe sincerely thank the AtlasCloud and DataCrunch for their computing support.\n\nLinkedin: Lancert\n\n## Appendix\n\n\u003cdetails\u003e\n\u003csummary\u003eEngineering Implementation Details\u003c/summary\u003e\n\n### Context Parallel\n\nFSDP's CP is directly implemented via [ring flash attention](https://github.com/zhuzilin/ring-flash-attention) library. Compared to Megatron's complex chunk mechanism, FSDP only needs to implement simple continuous chunks, and the load balancing part is handed over to ring flash attn. We only need to focus on input data slicing and result aggregation.\n\n**Specific implementation flow is as follows:**\n\n1. **Device Mesh Setup:** Establish (DP, CP) 2D communication group in `setup_device_mesh`, and use `substitute_hf_flash_attn` to replace HuggingFace model's original Flash Attention operator with Ring Flash Attention implementation supporting CP.\n2. **Input Slicing:** In the `_get_model_inputs_args` stage before forward, we directly use `torch.chunk` to slice Data Packed `input_ids` and `position_ids` into `cp_size` parts on the sequence dimension. The current rank only loads its own part of data. Meanwhile, call `update_ring_flash_attn_params` to pass global `cu_seqlens` info to the underlying Attention operator.\n3. **Result Gathering**: When calculating Log Probs (`get_logprob_and_entropy_with_cp`), each rank calculates local shard's log_probs and entropy in parallel. Finally, splice the results distributed on different ranks back into a complete sequence via `all_gather`, and remove Padding filled to meet CP alignment requirements.\n\n### Data Packing\n\nTo avoid waste caused by large amounts of padding on each CP rank due to direct padding, we splice long sequences into continuous vectors and use `cu_seqlens` to record boundaries. We first reused megatron's `process_rollout_data()` to split rollout by DP rank, then `packed_data` estimates how many `micro_batch`es are needed to complete a `global_batch` based on rollout token count and DP size. The relationship between `global_batch` and `micro_batch` in miles is seen in Batch \u0026 Sample.\n\n- When `use_dynamic_batch_size` is enabled, the number of micro-batches needs to be dynamically calculated based on actual sequence length: Use First-Fit algorithm via `get_minimum_num_micro_batch_size()` to estimate the minimum number of micro-batches needed to accommodate all data based on each sequence's length and `max_tokens_per_gpu` limit. This number will be synchronized across all DP ranks via `all_reduce(MAX)` to ensure consistent gradient accumulation steps for each rank.\n- If dynamic batch size is not enabled, directly use static formula global_batch_size // (micro_batch_size * dp_size) to calculate fixed micro-batch count.\n\nNext, execute actual packing operation in `pack_sequences()`:\n\n- Calculate partition count `k_partitions = ceil(total_tokens / max_tokens_per_gpu)`\n- Call `get_seqlen_balanced_partitions()` to perform load balanced allocation using [Karmarkar-Karp](https://en.wikipedia.org/wiki/Largest_differencing_method) algorithm (Largest Differencing Method). This algorithm maintains partition states via priority queue, merging the two partitions with the largest token total difference each time, making the final token count of each pack highly balanced.\n- For each pack, splice assigned sequences into continuous `flat_tokens` vector, and construct `cu_seqlens` array to record each sequence's boundaries, e.g., `[0, 128, 384, 512]` means 3 sequences with lengths 128, 256, 128 respectively.\n\nIn Context Parallel mode (`cp_size \u003e 1`), `pad_packed_sequence_with_cp()` will perform minimum alignment padding (at most cp_size-1 tokens) on the spliced sequence, ensuring total length can be divisible by cp_size for cross-rank slicing. Although this is still naive direct padding, since padding ≤ cp_size -1, it will not cause visible overhead.\n\nDuring training, `cu_seqlens` is directly passed to Flash Attention to handle variable length sequences; when calculating loss, `unpack_sequences()` accurately restores indicators like log_probs, advantages for each sequence based on boundary information. This method basically avoids overhead caused by naive padding.\n\n### PPO KL Precision Error\n\nThe PPO training process involves three batch-related parameters: Batch, Micro batch size \u0026 Sample.\n\nIdeally, when `sample` count × `micro_batch_size` = `global_batch_size`, it means all samples generated in one rollout (sample count × prompts processed per batch) exactly equal one complete training batch. At this time, rollout phase and training phase use **the same unupdated actor weight version**.\n\n- Use weight `W_t` to generate responses during Rollout.\n- Still use weight `W_t` to calculate log probabilities during training.\n\nTherefore, theoretically PPO KL divergence should be 0. However, in actual operation (only when reference model is enabled), KL divergence maintains a small positive value instead of 0 starting from the first micro batch, indicating a numerical drift problem.\n\nThis problem is caused by precision errors in weight exchange logic. The original implementation referred to Megatron's way, manually exchanging ref and actor tensors between CPU and GPU. To be compatible with FSDP2's DTensor, we manually created DTensor for swap. However, manual weight exchange leads to slight numerical deviations during weight loading. Megatron uses this manual exchange because the offload process of distributed optimizer is very complex, so it simply exchanges weights directly.\n\nFinally, we switched to a cleaner solution: treat reference model as an independent FSDP model, use FSDP native CPU Offload for management, and load it to GPU only during forward. This method completely avoids manual weight exchange, fully utilizes FSDP native CPU/GPU transfer mechanism, eliminates numerical drift from the root cause, making PPO KL converge to theoretical value 0, while not introducing additional GPU memory overhead. \n\n### **True on policy**\n\nAfter the CP PR was merged, the true on policy of the main branch actually failed. After investigation, it was found that precision was autocast to bf16 after indentation. After fixing, training-infer mismatch was successfully restored to 0.\n\nTo avoid precision problems caused by improper application of auto cast, we finally chose [Mixed Precision](https://docs.pytorch.org/tutorials/intermediate/FSDP_advanced_tutorial.html#mixed-precision) newly supported by FSDP2, implementing clearer and cleaner precision management.\n\n### Batch \u0026 Sample\n\n- **Sample count (n-samples-per-prompt)**: Number of candidate responses generated per prompt\n- **Micro batch size**: Number of samples processed per forward/backward pass during training (limited by GPU VRAM)\n- **Global batch size**: Total samples for a complete training iteration, usually completed by gradient accumulation of multiple micro batches\n\n\u003c/details\u003e\n\n","slug":"2025-12-03-miles-fsdp"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2025-12-03-miles-fsdp"},"buildId":"iUALlhdQzCWTY537sUjtz","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>