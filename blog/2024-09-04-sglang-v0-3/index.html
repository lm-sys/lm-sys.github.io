<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>SGLang v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision | LMSYS Org</title><meta name="title" content="SGLang v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision | LMSYS Org"/><meta property="og:title" content="SGLang v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision | LMSYS Org"/><meta name="twitter:title" content="SGLang v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision | LMSYS Org"/><meta name="description" content="&lt;p&gt;We&#x27;re excited to announce the release of &lt;a href=&quot;https://github.com/sgl-project/sglang/tree/main&quot;&gt;SGLang v0.3&lt;/a&gt;, which brings significant performance e..."/><meta property="og:description" content="&lt;p&gt;We&#x27;re excited to announce the release of &lt;a href=&quot;https://github.com/sgl-project/sglang/tree/main&quot;&gt;SGLang v0.3&lt;/a&gt;, which brings significant performance e..."/><meta name="twitter:description" content="&lt;p&gt;We&#x27;re excited to announce the release of &lt;a href=&quot;https://github.com/sgl-project/sglang/tree/main&quot;&gt;SGLang v0.3&lt;/a&gt;, which brings significant performance e..."/><meta property="og:image" content="https://lmsys.org/images/blog/sglang_v0_3/preview.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/sglang_v0_3/preview.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2024-09-04-sglang-v0-3"/><meta name="twitter:url" content="https://lmsys.org/blog/2024-09-04-sglang-v0-3"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-0bb93d4b49319e30.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/Gi1AiNchWjqG3CsV-FQnk/_buildManifest.js" defer=""></script><script src="/_next/static/Gi1AiNchWjqG3CsV-FQnk/_ssgManifest.js" defer=""></script><script src="/_next/static/Gi1AiNchWjqG3CsV-FQnk/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">SGLang v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision</h1><p class="text-xl pt-2 pb-2">by: <!-- -->The SGLang Team<!-- -->,<!-- --> <!-- -->Sep 04, 2024<!-- --></p><hr/><div class="pt-2 article"><p>We're excited to announce the release of <a href="https://github.com/sgl-project/sglang/tree/main">SGLang v0.3</a>, which brings significant performance enhancements and expanded support for novel model architectures. Here are the key updates:</p>
<ul>
<li>Up to 7x higher throughput for DeepSeek Multi-head Latent Attention (MLA)</li>
<li>Up to 1.5x lower latency with <code>torch.compile</code> on small batch sizes</li>
<li>Support for interleaved text and multi-image/video in LLaVA-OneVision</li>
<li>Support for interleaved window attention and 2x longer context length in Gemma-2</li>
</ul>
<p>In this blog post, we'll walk you through these key features. Please do not hesitate to report any issues or contribute ideas and code.</p>
<h3><a id="deepseek-multi-head-latent-attention-mla-throughput-optimizations" class="anchor" href="#deepseek-multi-head-latent-attention-mla-throughput-optimizations" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>DeepSeek Multi-head Latent Attention (MLA) Throughput Optimizations</h3>
<p><a href="https://arxiv.org/pdf/2405.04434">Multi-head Latent Attention</a> (MLA) is a new attention variant introduced by the DeepSeek team to improve inference efficiency. Due to its differences from standard attention mechanisms, existing open-source libraries have not fully optimized this operation. In SGLang v0.3, we implemented various optimizations for MLA, including weight absorption, grouped decoding kernels, FP8 batched MatMul, and FP8 KV cache quantization. <strong>Benchmark results show that SGLang v0.3 with MLA optimizations achieves 3x to 7x higher throughput than the baseline system.</strong> The benchmark measures the peak output throughput of these models with BF16 and FP8 on H100 GPUs (tensor-parallelism=1 for lite models and tensor-parallelism=8 for big models) on the ShareGPT datasets. Reproducible instructions are in the appendix. While encouraging, there is still much room for improvement. We are actively working on more optimizations to fully reproduce the results from the DeepSeek paper. Related PRs:
<a href="https://github.com/sgl-project/sglang/pull/905">#905</a>,
<a href="https://github.com/sgl-project/sglang/pull/1060">#1060</a>,
<a href="https://github.com/sgl-project/sglang/pull/1138">#1138</a>,
<a href="https://github.com/flashinfer-ai/flashinfer/pull/469">#469</a>,
<a href="https://github.com/sgl-project/sglang/pull/1285">#1285</a>,
<a href="https://github.com/sgl-project/sglang/pull/1286">#1286</a>.</p>
<p><img src="/images/blog/sglang_v0_3/deepseek_mla.svg" style="display: flex; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 70%;"></img></p>
<h3><a id="torchcompile-latency-optimizations" class="anchor" href="#torchcompile-latency-optimizations" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Torch.compile Latency Optimizations</h3>
<p><a href="https://pytorch.org/assets/pytorch2-2.pdf">Torch.compile</a> is a major feature of PyTorch 2.0. On NVIDIA GPUs, it performs aggressive fusion and generates highly efficient Triton kernels. We've integrated torch.compile into SGLang for linear/norm/activation layers, combining it with FlashInfer attention and sampling kernels. We turn on torch.compile for batch sizes 1 to 32, where we observed the most acceleration. With this combination, SGLang is faster than <a href="https://github.com/pytorch-labs/gpt-fast">gpt-fast</a> at batch size 1 and supports all online serving features, including continuous batching and RadixAttention for prefix caching. We are actively collaborating with the torch.compile and <a href="https://github.com/pytorch/ao">torchao</a> teams to incorporate their latest optimizations into SGLang. To use torch.compile in SGLang, add <code>--enable-torch-compile</code> when launching the server. <strong>SGLang w/ torch.compile yields up to a 1.5x speedup in the following benchmark.</strong> Reproducible instructions are in the appendix.</p>
<p><img src="/images/blog/sglang_v0_3/torch_compile.svg" style="display: flex; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 70%;"></img></p>
<h3><a id="llava-onevision-support-with-interleaved-text-multi-image-and-video" class="anchor" href="#llava-onevision-support-with-interleaved-text-multi-image-and-video" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>LLaVA-OneVision Support with Interleaved Text, Multi-Image, and Video</h3>
<p><a href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/">LLaVA-OneVision</a> is the first open model to achieve state-of-the-art performance in three important computer vision scenarios: single-image, multi-image, and video tasks. We collaborated with the LLaVA team to integrate these capabilities into SGLang v0.3. You can launch a server and query it using the OpenAI-compatible vision API, which supports interleaved text, multi-image, and video formats. Usage details are available <a href="https://github.com/sgl-project/sglang/blob/c500f96bb16c686ee8ba5d5f1fc716a0bd8e5fff/README.md?plain=1#L241-L244">here</a>. The authors validated the model's accuracy and reported benchmark results on the VideoDetailDescriptions and LLaVA-in-the-wild datasets (see <a href="https://github.com/sgl-project/sglang/pull/1123#issuecomment-2301691452">#1123</a>). <strong>SGLang archives up to 4.5x speedup than the authors’ original implementation in HuggingFace/transformers.</strong></p>
<p><img src="/images/blog/sglang_v0_3/llava_onevision.svg" style="display: flex; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 70%;"></img></p>
<h3><a id="gemma-2-support-with-interleaved-window-attention" class="anchor" href="#gemma-2-support-with-interleaved-window-attention" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Gemma-2 Support with Interleaved Window Attention</h3>
<p>Google's <a href="https://arxiv.org/abs/2408.00118">Gemma-2 model</a> uses interleaved window attention to reduce computational complexity for long contexts, alternating between local sliding window attention (4K context length) and global attention (8K context length) in every other layer. We enhanced SGLang v0.3 to fully support the 8K context length by leveraging the optimized window attention kernel from FlashInfer kernels (which skips computation instead of masking) and refining our KV cache manager. Other libraries that lack this feature can only run with a 4K context length. You can launch the model with</p>
<pre><code class="hljs">python3 -m sglang<span class="hljs-selector-class">.launch_server</span> <span class="hljs-attr">--model-path</span> google/gemma-<span class="hljs-number">2</span>b   
</code></pre>
<p><img src="/images/blog/sglang_v0_3/gemma2.svg" style="display: flex; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 70%;"></img></p>
<h2><a id="acknowledgment" class="anchor" href="#acknowledgment" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgment</h2>
<p>The DeepSeek MLA optimizations were contributed by Ke Bao and Yineng Zhang. The torch.compile optimizations were contributed by Liangsheng Yin. The LLaVA-OneVision contributions were made by Kaichen Zhang and Bo Li. The interleaved window attention was contributed by Ying Sheng. We also thank all 90+ open-source <a href="https://github.com/sgl-project/sglang/graphs/contributors">contributors</a>.</p>
<h2><a id="appendix" class="anchor" href="#appendix" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Appendix</h2>
<h3><a id="benchmark-instructions-for-deepseek-mla" class="anchor" href="#benchmark-instructions-for-deepseek-mla" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Benchmark Instructions for DeepSeek MLA</h3>
<pre><code class="hljs"><span class="hljs-comment"># DeepSeekCoder-V2-Lite (BF16)</span>
<span class="hljs-comment">## Launch a server  </span>
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">sglang</span>.<span class="hljs-string">launch_server</span> <span class="hljs-built_in">--model</span> <span class="hljs-string">deepseek-ai</span>/<span class="hljs-string">DeepSeek-Coder-V2-Lite-Instruct</span> <span class="hljs-built_in">--enable-mla</span> <span class="hljs-built_in">--disable-radix</span> <span class="hljs-built_in">--trust-remote-code</span>  
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">vllm</span>.<span class="hljs-string">entrypoints</span>.<span class="hljs-string">openai</span>.<span class="hljs-string">api_server</span> <span class="hljs-built_in">--model</span> <span class="hljs-string">deepseek-ai</span>/<span class="hljs-string">DeepSeek-Coder-V2-Lite-Instruct</span> <span class="hljs-built_in">--disable-log-requests</span> <span class="hljs-built_in">--trust-remote-code</span> <span class="hljs-built_in">--max-model-len</span> <span class="hljs-string">4096</span>

<span class="hljs-comment">## Run benchmark  </span>
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">sglang</span>.<span class="hljs-string">bench_serving</span> <span class="hljs-built_in">--backend</span> <span class="hljs-string">sglang</span> <span class="hljs-built_in">--num-prompts</span> <span class="hljs-string">5000</span>  
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">sglang</span>.<span class="hljs-string">bench_serving</span> <span class="hljs-built_in">--backend</span> <span class="hljs-string">vllm</span> <span class="hljs-built_in">--num-prompts</span> <span class="hljs-string">5000</span>

<span class="hljs-comment"># DeepSeekCoder-V2 (BF16)  </span>
<span class="hljs-comment">## Launch a server  </span>
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">sglang</span>.<span class="hljs-string">launch_server</span> <span class="hljs-built_in">--model</span> <span class="hljs-string">deepseek-ai</span>/<span class="hljs-string">DeepSeek-Coder-V2-Instruct</span> <span class="hljs-built_in">--disable-radix</span> <span class="hljs-built_in">--tp</span> <span class="hljs-string">8</span> <span class="hljs-built_in">--trust-remote-code</span> <span class="hljs-built_in">--enable-mla</span>  
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">vllm</span>.<span class="hljs-string">entrypoints</span>.<span class="hljs-string">openai</span>.<span class="hljs-string">api_server</span> <span class="hljs-built_in">--model</span> <span class="hljs-string">deepseek-ai</span>/<span class="hljs-string">DeepSeek-Coder-V2-Instruct</span> <span class="hljs-built_in">--disable-log-requests</span> <span class="hljs-built_in">--tensor-parallel-size</span> <span class="hljs-string">8</span> <span class="hljs-built_in">--trust-remote-code</span> <span class="hljs-built_in">--max-model-len</span> <span class="hljs-string">4096</span>

<span class="hljs-comment">## Run benchmark  </span>
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">sglang</span>.<span class="hljs-string">bench_serving</span> <span class="hljs-built_in">--backend</span> <span class="hljs-string">sglang</span> <span class="hljs-built_in">--num-prompts</span> <span class="hljs-string">5000</span>  
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">sglang</span>.<span class="hljs-string">bench_serving</span> <span class="hljs-built_in">--backend</span> <span class="hljs-string">vllm</span> <span class="hljs-built_in">--num-prompts</span> <span class="hljs-string">5000</span>

<span class="hljs-comment"># DeepSeekCoder-V2 (FP8)  </span>
<span class="hljs-comment">## Launch a server  </span>
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">sglang</span>.<span class="hljs-string">launch_server</span> <span class="hljs-built_in">--model</span> <span class="hljs-string">neuralmagic</span>/<span class="hljs-string">DeepSeek-Coder-V2-Instruct-FP8</span> <span class="hljs-built_in">--enable-mla</span> <span class="hljs-built_in">--quantization</span> <span class="hljs-string">fp8</span> <span class="hljs-built_in">--kv-cache-dtype</span> <span class="hljs-string">fp8_e5m2</span> <span class="hljs-built_in">--disable-radix</span> <span class="hljs-built_in">--tp</span> <span class="hljs-string">8</span> <span class="hljs-built_in">--trust-remote-code</span>  
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">vllm</span>.<span class="hljs-string">entrypoints</span>.<span class="hljs-string">openai</span>.<span class="hljs-string">api_server</span> <span class="hljs-built_in">--model</span> <span class="hljs-string">neuralmagic</span>/<span class="hljs-string">DeepSeek-Coder-V2-Instruct-FP8</span> <span class="hljs-built_in">--quantization</span> <span class="hljs-string">fp8</span> <span class="hljs-built_in">--disable-log-requests</span> <span class="hljs-built_in">--tensor-parallel-size</span> <span class="hljs-string">8</span> <span class="hljs-built_in">--trust-remote-code</span> <span class="hljs-built_in">--max-model-len</span> <span class="hljs-string">4096</span>

<span class="hljs-comment">## Run benchmark  </span>
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">sglang</span>.<span class="hljs-string">bench_serving</span> <span class="hljs-built_in">--backend</span> <span class="hljs-string">sglang</span> <span class="hljs-built_in">--num-prompts</span> <span class="hljs-string">5000</span>  
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">sglang</span>.<span class="hljs-string">bench_serving</span> <span class="hljs-built_in">--backend</span> <span class="hljs-string">vllm</span> <span class="hljs-built_in">--num-prompts</span> <span class="hljs-string">5000</span>  
</code></pre>
<h3><a id="benchmark-instructions-for-torchcompile" class="anchor" href="#benchmark-instructions-for-torchcompile" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Benchmark Instructions for torch.compile</h3>
<pre><code class="hljs"><span class="hljs-comment"># SGLang  </span>
<span class="hljs-comment">## Launch a server  </span>
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">sglang</span>.<span class="hljs-string">launch_server</span> <span class="hljs-built_in">--model</span> <span class="hljs-string">meta-llama</span>/<span class="hljs-string">Meta-Llama-3-8B</span> <span class="hljs-built_in">--enable-torch-compile</span>

<span class="hljs-comment">## Run benchmark  </span>
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">sglang</span>.<span class="hljs-string">bench_serving</span> <span class="hljs-built_in">--backend</span> <span class="hljs-string">sglang</span> <span class="hljs-built_in">--dataset-name</span> <span class="hljs-string">random</span> <span class="hljs-built_in">--random-input-len</span> <span class="hljs-string">128</span> <span class="hljs-built_in">--random-output-len</span> <span class="hljs-string">512</span> <span class="hljs-built_in">--random-range-ratio</span> <span class="hljs-string">1</span> <span class="hljs-built_in">--num-prompts</span> <span class="hljs-string">1</span>

<span class="hljs-comment"># vLLM  </span>
<span class="hljs-comment">## Launch a server  </span>
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">vllm</span>.<span class="hljs-string">entrypoints</span>.<span class="hljs-string">openai</span>.<span class="hljs-string">api_server</span> <span class="hljs-built_in">--model</span> <span class="hljs-string">meta-llama</span>/<span class="hljs-string">Meta-Llama-3-8B</span> <span class="hljs-built_in">--disable-log-requests</span>

<span class="hljs-comment">## Run benchmark  </span>
<span class="hljs-string">python3</span> -<span class="hljs-string">m</span> <span class="hljs-string">sglang</span>.<span class="hljs-string">bench_serving</span> <span class="hljs-built_in">--backend</span> <span class="hljs-string">vllm</span> <span class="hljs-built_in">--dataset-name</span> <span class="hljs-string">random</span> <span class="hljs-built_in">--random-input-len</span> <span class="hljs-string">128</span> <span class="hljs-built_in">--random-output-len</span> <span class="hljs-string">512</span> <span class="hljs-built_in">--random-range-ratio</span> <span class="hljs-string">1</span> <span class="hljs-built_in">--num-prompts</span> <span class="hljs-string">1</span>  
</code></pre>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"SGLang v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision","author":"The SGLang Team","date":"September 4, 2024","previewImg":"/images/blog/sglang_v0_3/preview.png"},"content":"\nWe're excited to announce the release of [SGLang v0.3](https://github.com/sgl-project/sglang/tree/main), which brings significant performance enhancements and expanded support for novel model architectures. Here are the key updates:\n\n- Up to 7x higher throughput for DeepSeek Multi-head Latent Attention (MLA)  \n- Up to 1.5x lower latency with `torch.compile` on small batch sizes  \n- Support for interleaved text and multi-image/video in LLaVA-OneVision  \n- Support for interleaved window attention and 2x longer context length in Gemma-2\n\nIn this blog post, we'll walk you through these key features. Please do not hesitate to report any issues or contribute ideas and code.\n\n\n### DeepSeek Multi-head Latent Attention (MLA) Throughput Optimizations\n\n[Multi-head Latent Attention](https://arxiv.org/pdf/2405.04434) (MLA) is a new attention variant introduced by the DeepSeek team to improve inference efficiency. Due to its differences from standard attention mechanisms, existing open-source libraries have not fully optimized this operation. In SGLang v0.3, we implemented various optimizations for MLA, including weight absorption, grouped decoding kernels, FP8 batched MatMul, and FP8 KV cache quantization. **Benchmark results show that SGLang v0.3 with MLA optimizations achieves 3x to 7x higher throughput than the baseline system.** The benchmark measures the peak output throughput of these models with BF16 and FP8 on H100 GPUs (tensor-parallelism=1 for lite models and tensor-parallelism=8 for big models) on the ShareGPT datasets. Reproducible instructions are in the appendix. While encouraging, there is still much room for improvement. We are actively working on more optimizations to fully reproduce the results from the DeepSeek paper. Related PRs:\n[#905](https://github.com/sgl-project/sglang/pull/905),\n[#1060](https://github.com/sgl-project/sglang/pull/1060),\n[#1138](https://github.com/sgl-project/sglang/pull/1138),\n[#469](https://github.com/flashinfer-ai/flashinfer/pull/469),\n[#1285](https://github.com/sgl-project/sglang/pull/1285),\n[#1286](https://github.com/sgl-project/sglang/pull/1286).\n\n\u003cimg src=\"/images/blog/sglang_v0_3/deepseek_mla.svg\" style=\"display: flex; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 70%;\"\u003e\u003c/img\u003e\n\n### Torch.compile Latency Optimizations\n\n[Torch.compile](https://pytorch.org/assets/pytorch2-2.pdf) is a major feature of PyTorch 2.0. On NVIDIA GPUs, it performs aggressive fusion and generates highly efficient Triton kernels. We've integrated torch.compile into SGLang for linear/norm/activation layers, combining it with FlashInfer attention and sampling kernels. We turn on torch.compile for batch sizes 1 to 32, where we observed the most acceleration. With this combination, SGLang is faster than [gpt-fast](https://github.com/pytorch-labs/gpt-fast) at batch size 1 and supports all online serving features, including continuous batching and RadixAttention for prefix caching. We are actively collaborating with the torch.compile and [torchao](https://github.com/pytorch/ao) teams to incorporate their latest optimizations into SGLang. To use torch.compile in SGLang, add `--enable-torch-compile` when launching the server. **SGLang w/ torch.compile yields up to a 1.5x speedup in the following benchmark.** Reproducible instructions are in the appendix.  \n\n\u003cimg src=\"/images/blog/sglang_v0_3/torch_compile.svg\" style=\"display: flex; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 70%;\"\u003e\u003c/img\u003e\n\n### LLaVA-OneVision Support with Interleaved Text, Multi-Image, and Video \n\n[LLaVA-OneVision](https://llava-vl.github.io/blog/2024-08-05-llava-onevision/) is the first open model to achieve state-of-the-art performance in three important computer vision scenarios: single-image, multi-image, and video tasks. We collaborated with the LLaVA team to integrate these capabilities into SGLang v0.3. You can launch a server and query it using the OpenAI-compatible vision API, which supports interleaved text, multi-image, and video formats. Usage details are available [here](https://github.com/sgl-project/sglang/blob/c500f96bb16c686ee8ba5d5f1fc716a0bd8e5fff/README.md?plain=1#L241-L244). The authors validated the model's accuracy and reported benchmark results on the VideoDetailDescriptions and LLaVA-in-the-wild datasets (see [#1123](https://github.com/sgl-project/sglang/pull/1123#issuecomment-2301691452)). **SGLang archives up to 4.5x speedup than the authors’ original implementation in HuggingFace/transformers.**\n\n\u003cimg src=\"/images/blog/sglang_v0_3/llava_onevision.svg\" style=\"display: flex; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 70%;\"\u003e\u003c/img\u003e\n\n### Gemma-2 Support with Interleaved Window Attention\n\nGoogle's [Gemma-2 model](https://arxiv.org/abs/2408.00118) uses interleaved window attention to reduce computational complexity for long contexts, alternating between local sliding window attention (4K context length) and global attention (8K context length) in every other layer. We enhanced SGLang v0.3 to fully support the 8K context length by leveraging the optimized window attention kernel from FlashInfer kernels (which skips computation instead of masking) and refining our KV cache manager. Other libraries that lack this feature can only run with a 4K context length. You can launch the model with  \n```\npython3 -m sglang.launch_server --model-path google/gemma-2b   \n```\n\n\u003cimg src=\"/images/blog/sglang_v0_3/gemma2.svg\" style=\"display: flex; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 70%;\"\u003e\u003c/img\u003e\n\n## Acknowledgment\n\nThe DeepSeek MLA optimizations were contributed by Ke Bao and Yineng Zhang. The torch.compile optimizations were contributed by Liangsheng Yin. The LLaVA-OneVision contributions were made by Kaichen Zhang and Bo Li. The interleaved window attention was contributed by Ying Sheng. We also thank all 90+ open-source [contributors](https://github.com/sgl-project/sglang/graphs/contributors).\n\n## Appendix\n\n### Benchmark Instructions for DeepSeek MLA\n\n```  \n# DeepSeekCoder-V2-Lite (BF16)\n## Launch a server  \npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct --enable-mla --disable-radix --trust-remote-code  \npython3 -m vllm.entrypoints.openai.api_server --model deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct --disable-log-requests --trust-remote-code --max-model-len 4096\n\n## Run benchmark  \npython3 -m sglang.bench_serving --backend sglang --num-prompts 5000  \npython3 -m sglang.bench_serving --backend vllm --num-prompts 5000\n\n# DeepSeekCoder-V2 (BF16)  \n## Launch a server  \npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-Coder-V2-Instruct --disable-radix --tp 8 --trust-remote-code --enable-mla  \npython3 -m vllm.entrypoints.openai.api_server --model deepseek-ai/DeepSeek-Coder-V2-Instruct --disable-log-requests --tensor-parallel-size 8 --trust-remote-code --max-model-len 4096\n\n## Run benchmark  \npython3 -m sglang.bench_serving --backend sglang --num-prompts 5000  \npython3 -m sglang.bench_serving --backend vllm --num-prompts 5000\n\n# DeepSeekCoder-V2 (FP8)  \n## Launch a server  \npython3 -m sglang.launch_server --model neuralmagic/DeepSeek-Coder-V2-Instruct-FP8 --enable-mla --quantization fp8 --kv-cache-dtype fp8_e5m2 --disable-radix --tp 8 --trust-remote-code  \npython3 -m vllm.entrypoints.openai.api_server --model neuralmagic/DeepSeek-Coder-V2-Instruct-FP8 --quantization fp8 --disable-log-requests --tensor-parallel-size 8 --trust-remote-code --max-model-len 4096\n\n## Run benchmark  \npython3 -m sglang.bench_serving --backend sglang --num-prompts 5000  \npython3 -m sglang.bench_serving --backend vllm --num-prompts 5000  \n```\n\n### Benchmark Instructions for torch.compile\n\n```  \n# SGLang  \n## Launch a server  \npython3 -m sglang.launch_server --model meta-llama/Meta-Llama-3-8B --enable-torch-compile\n\n## Run benchmark  \npython3 -m sglang.bench_serving --backend sglang --dataset-name random --random-input-len 128 --random-output-len 512 --random-range-ratio 1 --num-prompts 1\n\n# vLLM  \n## Launch a server  \npython3 -m vllm.entrypoints.openai.api_server --model meta-llama/Meta-Llama-3-8B --disable-log-requests\n\n## Run benchmark  \npython3 -m sglang.bench_serving --backend vllm --dataset-name random --random-input-len 128 --random-output-len 512 --random-range-ratio 1 --num-prompts 1  \n```\n\n","slug":"2024-09-04-sglang-v0-3"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2024-09-04-sglang-v0-3"},"buildId":"Gi1AiNchWjqG3CsV-FQnk","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>