<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org</title><meta name="title" content="How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org"/><meta property="og:title" content="How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org"/><meta name="twitter:title" content="How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org"/><meta name="description" content="&lt;p&gt;In this blogpost, we introduce our latest series of chatbot models, LongChat-7B and LongChat-13B, featuring a new level of extended context length up to 1..."/><meta property="og:description" content="&lt;p&gt;In this blogpost, we introduce our latest series of chatbot models, LongChat-7B and LongChat-13B, featuring a new level of extended context length up to 1..."/><meta name="twitter:description" content="&lt;p&gt;In this blogpost, we introduce our latest series of chatbot models, LongChat-7B and LongChat-13B, featuring a new level of extended context length up to 1..."/><meta property="og:image" content="https://lmsys.org/images/blog/longchat/topic_retrieval_preview.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/longchat/topic_retrieval_preview.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2023-06-29-longchat"/><meta name="twitter:url" content="https://lmsys.org/blog/2023-06-29-longchat"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="19"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/9aa18d40847551db.css" as="style"/><link rel="stylesheet" href="/_next/static/css/9aa18d40847551db.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-a0cb9ddcff62d761.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-3d139df5a55b1694.js" defer=""></script><script src="/_next/static/EtN_-n3tZqyhAwOvuAgy8/_buildManifest.js" defer=""></script><script src="/_next/static/EtN_-n3tZqyhAwOvuAgy8/_ssgManifest.js" defer=""></script><script src="/_next/static/EtN_-n3tZqyhAwOvuAgy8/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://arena.lmsys.org" target="_blank" rel="noopener noreferrer">Chatbot Arena</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://arena.lmsys.org" target="_blank" rel="noopener noreferrer">Chatbot Arena</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">How Long Can Open-Source LLMs Truly Promise on Context Length?</h1><p class="text-xl pt-2 pb-2">by: <!-- -->The LongChat Team<!-- -->,<!-- --> <!-- -->Jun 29, 2023<!-- --></p><hr/><div class="pt-2 article"><p>In this blogpost, we introduce our latest series of chatbot models, LongChat-7B and LongChat-13B, featuring a new level of extended context length up to 16K tokens.
Evaluation results show that the long-range retrieval accuracy of LongChat-13B is up to 2x higher than other long-context open models such as MPT-7B-storywriter (84K), MPT-30B-chat (8K), and ChatGLM2-6B (8k).
LongChat shows promising results in closing the gap between open models and proprietary long context models such as Claude-100K and GPT-4-32K.</p>
<p><img src="/images/blog/longchat/topic_retrieval.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;"></img></p>
<p style="color:gray; text-align: center;">Figure 1: Comparing LongChat to other models on the long-range topic retrieval task.</p>
<p>Not only can LongChat models handle such a long context length, but they also precisely follow human instructions in dialogues and demonstrate strong performance in the human preference benchmark <a href="https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge">MT-Bench</a>.
Their preview versions are available at HuggingFace: <a href="https://huggingface.co/lmsys/longchat-13b-16k">lmsys/longchat-13b-16k</a> and <a href="https://huggingface.co/lmsys/longchat-7b-16k">lmsys/longchat-7b-16k</a>.
You can try them immediately in CLI or web interface using FastChat:</p>
<pre><code class="language-python">python3 -m fastchat.serve.cli --model-path lmsys/longchat-7b-16k
</code></pre>
<p>There has been a significant surge of interest within the open-source community in developing language models with longer context or extending the context length of existing models like LLaMA.
This trend has led to interesting observations and extensive discussions in various sources, such as <a href="https://kaiokendev.github.io/context">Kaiokendev’s blog</a> and this <a href="https://arxiv.org/pdf/2306.15595.pdf">arXiv manuscript</a>;
meanwhile, several notable models have been released claiming to support much longer context than LLaMA, notable ones include:</p>
<ul>
<li><a href="https://huggingface.co/mosaicml/mpt-7b-storywriter">MPT-7B-storywriter</a> supports 65K context length and extrapolates to 84K.</li>
<li><a href="https://huggingface.co/spaces/mosaicml/mpt-30b-chat">MPT-30B-chat</a> supports 8K context length.</li>
<li><a href="https://huggingface.co/THUDM/chatglm2-6b">ChatGLM2-6B</a> supports 8K context.</li>
</ul>
<p>At LMSYS Org, we have been concurrently exploring various techniques to lengthen the context of our models like <a href="https://huggingface.co/lmsys/vicuna-13b-v1.3">Vicuna</a>.
In this blogpost, alongside the release of the LongChat series, we share our <a href="https://github.com/DachengLi1/LongChat">evaluation tools</a> to verify the long-context capability of LLMs.</p>
<p>Using our evaluation tools in combination with various academic long-context evaluation benchmarks, we conduct a thorough comparison of several open-source and commercial models that claim to support long context.
Through this analysis, we examine how well these models deliver on their promised context length.
We found that <em>while commercial models like GPT-3.5-turbo performs well on our tests, many open source models do not deliver the expected results on their promised context length</em>.</p>
<p>The data and code used to reproduce the results in the blog post are available in our LongChat <a href="https://github.com/DachengLi1/LongChat/tree/longeval">repo</a>.
We provide a visualization in this <a href="https://github.com/DachengLi1/LongChat/blob/longeval/longeval/topics_lines_demo.ipynb">notebook</a>.</p>
<h2><a id="longchat-training-recipe" class="anchor" href="#longchat-training-recipe" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>LongChat Training Recipe</h2>
<p>LongChat is finetuned from LLaMA models, which were originally pretrained with 2048 context length.
The training recipe can be conceptually described in two steps:</p>
<h3><a id="step-1-condensing-rotary-embeddings" class="anchor" href="#step-1-condensing-rotary-embeddings" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 1: Condensing rotary embeddings</h3>
<p><a href="https://arxiv.org/abs/2104.09864v4">Rotary position embedding</a> is a type of positional embedding that injects the information of position in Transformer.
It is implemented in Hugging Face transformer by:</p>
<pre><code class="language-python">query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
</code></pre>
<p>Where position_ids are indices such as 1, 2, 3, ... that denote the position of a token in the sentence.
For instance, the token &quot;today&quot; in the sentence &quot;today is a good day&quot; has position_ids 1.
The <code>apply_rotary_pos_emb()</code> function then applies a <a href="https://arxiv.org/pdf/2104.09864.pdf">transformation</a> based on the provided position_ids.</p>
<p>The LLaMA model is pre-trained with rotary embedding on sequence length 2048, which means that it has not observed scenarios where position_ids &gt; 2048 during the pre-training phase.
Instead of forcing the LLaMA model to adapt to position_ids &gt; 2048, we condense position_ids &gt; 2048 to be within 0 to 2048.
Intuitively, we conjecture this condensation can maximally reuse the model weights learned in the pre-training stage. See more insights from <a href="https://kaiokendev.github.io/context">Kaiokendev’s blog</a>.</p>
<p>We define the term <code>condensation ratio</code> by dividing the target new context length <code>y</code> by 2048. We then divide every position_ids by this ratio and feed it into the <code>apply_rotary_pos_emb()</code> function.</p>
<pre><code class="language-python">query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids / ratio)
</code></pre>
<p>In this release, we fine-tune the model to a context length of 16384, and thus the condensation ratio is 8. For instance, a token with position_ids = 10000 becomes position_ids = 10000 / 8 = 1250, and the neighboring token 10001 becomes 10001 / 8 = 1250.125.
This step requires no training.</p>
<h3><a id="step-2-finetuning-on-curated-conversation-data" class="anchor" href="#step-2-finetuning-on-curated-conversation-data" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Step 2: Finetuning on Curated Conversation Data</h3>
<p>After condensing the embedding, we perform the finetuning procedure on our curated conversation dataset.
We reuse our collected user-shared conversations previously used for training Vicuna.
We clean the data using FastChat data pipeline, and truncate these conversations so they are no longer than 16K.
We finetune the model using standard next-token prediction loss. We fine-tune the 7B and 13B models with 80k and 18k conversations, respectively.
To save memory, we use Pytorch FSDP and Flash Attention. Assume A100 is $3/hour on Cloud, the 7B model costs ~$300, and the 13B model costs ~$700.</p>
<h2><a id="evaluation-toolkits-longeval" class="anchor" href="#evaluation-toolkits-longeval" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Evaluation toolkits: LongEval</h2>
<p>Recently, commercial and open-source models have continued to tout their abilities to support expanded context length (from 8K, 32K, 84K, to 100K) in their latest releases, but how can we verify these claims?
The term &quot;long-context capability&quot; can mean different things for different model providers. For instance, does <a href="https://huggingface.co/mosaicml/mpt-7b-storywriter">MPT-7B-StoryWriter's</a> advertised 84K context length operate at the same capacity as OpenAI’s ChatGPT at 16K?
This issue is also prevalent in our LongChat models development: how do we swiftly and effectively confirm if a freshly trained model can handle the intended context length?</p>
<p>To address this, we can base our evaluations on tasks that necessitate LLMs to process lengthy contexts, such as text generation, retrieval, summarization, and information association in long text sequences.
Inspired by <a href="https://twitter.com/DimitrisPapail/status/1658091355632189440">recent discussions</a>, we've devised, <a href="https://github.com/DachengLi1/LongChat.git">LongEval</a>, a long context test suite.
This suite incorporates two tasks of varying degrees of difficulty, providing a simple and swift way to measure and compare long-context performance.</p>
<h3><a id="task-1-coarse-grained-topic-retrieval" class="anchor" href="#task-1-coarse-grained-topic-retrieval" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Task 1: Coarse-grained Topic Retrieval</h3>
<p>In real-world long conversations, users usually talk about and jump between several topics with the chatbot. The Topic Retrieval task mimics this scenario by asking the chatbot to retrieve the first topic in a long conversation consisting of multiple topics. An example task is:</p>
<pre><code class="language-python">… (instruction of the task)
USER: I would like to discuss &lt;TOPIC-1&gt;
ASSISTANT: Sure! What about xxx of &lt;TOPIC-1&gt;?
… (a multi-turn conversation of &lt;TOPIC-1&gt;)
USER: I would like to discuss  &lt;TOPIC-2&gt;
…
USER: I would like to discuss &lt;TOPIC-k&gt;
… 
USER: What is the first topic we discussed?
ASSISTANT: 
</code></pre>
<p>This task tests whether the model can locate a chunk of text and associate it with the right topic name. We design a conversation to be 400 ~ 600 tokens long. Thus, this task is considered coarse-grained because the model may give correct predictions when it locates positions not too far away (&lt;500 token distance) from the right ones.</p>
<h3><a id="task-2-fine-grained-line-retrieval" class="anchor" href="#task-2-fine-grained-line-retrieval" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Task 2: Fine-grained Line Retrieval</h3>
<p>To further test the model ability to locate and associate texts from a long conversation, we introduce a finer-grained Line Retrieval test. In this test, the chatbot needs to precisely retrieve a number from a long document, instead of a topic from long multi-round conversations. Below is an example:</p>
<pre><code class="language-python">line torpid-kid: REGISTER_CONTENT is &lt;24169&gt;
line moaning-conversation: REGISTER_CONTENT is &lt;10310&gt;
…
line tacit-colonial: REGISTER_CONTENT is &lt;14564&gt;
What is the &lt;REGISTER_CONTENT&gt; in line moaning-conversation?
</code></pre>
<p>The task was originally proposed in <a href="https://github.com/anadim/the-little-retrieval-test">Little Retrieval Test</a>.
The original testcase uses numbers to denote a line, which we found smaller LLMs usually cannot comprehend well.
To disentangle these factors and make them more suitable for testing open-source chatbots at various sizes, we improve it by using random natural language (e.g., torpid-kid) instead.</p>
<p>We found these two tasks behave with the expected characteristics:</p>
<ol>
<li>The task can effectively capture the abilities of text generation, retrieval, and information association at long context, reflected by the retrieving accuracy.</li>
<li>It is easy to extend the tests to arbitrary lengths to test models’ capacity under different context lengths.</li>
<li>We have run sanity checks of both tasks and observed the expected results. For example, the vanilla LLaMA models, pretrained with a 2K context length, can achieve perfect accuracy on both tasks when the test inputs length is &lt;2K, but will immediately fail (nearly 0 accuracy) on any test inputs beyond 2K.</li>
</ol>
<p>More details and example usage of LongEval can be found in this <a href="https://github.com/DachengLi1/LongChat/blob/longeval/longeval/topics_lines_demo.ipynb">notebook</a>.</p>
<h2><a id="results-and-findings" class="anchor" href="#results-and-findings" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Results and findings</h2>
<p>In this section, we share our evaluation and findings.
<br></p>
<p style="color:gray; text-align: center;">Table 1. Model Specifications.</p>
<div style="display: flex; justify-content: center;">
<table id="Table1">
<tbody>
<tr> <th>Model</th> <th>Size</th> <th>Instruction-tuned?</th> <th>Pretrained Context Length</th> <th>Finetune Context Length</th> <th>Claimed Context Length</th> <th>Open Source?</th></tr>
<tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-30b-chat">MPT-30-chat</a></td>  <td>30B</td>  <td>Yes</td>  <td>8K</td>  <td>-</td> <td>8K</td> <td>Yes</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-7b-storywriter">MPT-7b-storywriter</a></td>  <td>7B</td> <td>Yes</td>  <td>2K</td>  <td>65K</td>  <td>84K</td> <td>Yes</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/THUDM/chatglm2-6b">ChatGLM2-6b</a></td>  <td>6B</td>  <td>Yes</td>  <td>32K</td>  <td>8K</td> <td>8K</td> <td>Yes</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/lmsys/longchat-13b-16k">LongChat-13b-16k (ours)</a></td>  <td>13B</td>  <td>Yes</td> <td>2K</td>  <td>16K</td>  <td>16K</td> <td>Yes</td> </tr>
<tr> <td><a target="_blank" href="https://chat.openai.com/">GPT-3.5-turbo</a></td>  <td>-</td>  <td>-</td>  <td>-</td> <td>-</td> <td>16K</td>  <td>No</td> </tr>
<tr> <td><a target="_blank" href="https://www.anthropic.com/index/introducing-claude">Anthropic Claude-1.3</a></td>  <td>-</td>  <td>-</td>  <td>-</td> <td>-</td> <td>100K</td>  <td>No</td> </tr>
</tbody>
</table>
</div>
<p>­</p>
<p>In particular, we consider four open-sourced models and two proprietary models, listed in Table 1.</p>
<h3><a id="longeval-results" class="anchor" href="#longeval-results" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>LongEval results</h3>
<p>From the coarse-grained topic retrieval test results (Figure 2 at the beginning), we observe the problematic performance of open-source long-context models. For instance, MPT-7B-storywriter claims to have a context length of 84K but barely achieves 50% accuracy even at one-fifth of its claimed context length (16K).
ChatGLM2-6B cannot reliably retrieve the first topic at the length of 6K (46% accuracy). On the other hand, LongChat-13B-16K model reliably retrieves the first topic, with comparable accuracy to GPT-3.5-turbo.</p>
<p><img src="/images/blog/longchat/line_retrieval.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;"></img></p>
<p style="color:gray; text-align: center;">Figure 3: Accuracy on the long-range line retrieval task.</p>
<p>In the fine-grained line retrieval test, MPT-7B-storywriter performs even worse -- the accuracy drops from ~50% to ~30%. ChatGLM2-6B also observes degradation and does not perform well at 5K context length (32%).
We notice that ChatGLM2-6B states that it has not been yet fully optimized for single-turn long document understanding, which could explain its current performance on LongEval.
LongChat-13B-16K performs closely to GPT-3.5 and Claude-v3 within 12K context length. However, we also find the preview versions are not perfect at 12K-16K, see the <a href="https://lmsys.org/blog/2023-06-29-longchat/#discussion">discussion section</a>.</p>
<p><strong>Disentangle irrelevant LLM abilities in LongEval</strong></p>
<p>In topics and line retrieval tests, we observe mistakes caused by factors irrelevant to long-context ability, such as the instruction-following ability. For instance, in the Line Retrieval test, the model may simply respond “sure, I will tell you the number” instead of returning an actual number.
To give a fair comparison, we took two actions to avoid factors irrespective of long-context capabilities: prompt engineering and estimating accuracy only based on cases in which the models correctly follow instructions. Check our codes for details.</p>
<h3><a id="human-preference-benchmark-mt-bench" class="anchor" href="#human-preference-benchmark-mt-bench" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Human preference benchmark (MT-bench)</h3>
<p>In the previous section, we observed that LongChat models perform well on long-range retrieval tasks, but does this come with a significant drop in human preference? To test whether it still follows human preferences, we use GPT-4 graded <a href="https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge">MT-bench</a>, a set of challenging multi-turn conversation questions.</p>
<p style="color:gray; text-align: center;">Table 2. MT-bench scores comparing LongChat-13B to other models of similar sizes.</p>
<div style="display: flex; justify-content: center;">
<table id="Table1" style="max-width: 400px;">
<tbody>
<tr> <th>Model</th> <th>MT-bench (score)</th></tr>
<tr> <td><a target="_blank" href="https://huggingface.co/lmsys/longchat-13b-16k">LongChat-13B-16K</a></td>  <td>5.95</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/lmsys/vicuna-13b-v1.3">Vicuna-13B </a></td>  <td>6.39</td>  </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/WizardLM/WizardLM-13B-V1.0"> WizardLM-13B</a></td>  <td>6.35</td>  </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/project-baize/baize-v2-13b"> Baize-v2-13B </a></td>  <td>5.75</td>  </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/NousResearch/Nous-Hermes-13b"> Nous-Hermes-13B </a></td>  <td>5.51</td>   </tr>
<tr> <td><a target="_blank" href="https://crfm.stanford.edu/2023/03/13/alpaca.html"> Alpaca-13B</a></td>  <td>4.53</td>  </tr>
</tbody>
</table>
</div>
<p>We find that LongChat-13B-16K is comparable to its closest alternative -- Vicuna-13B, which indicates that this long-range ability does not come with a significant sacrifice of its short-range ability.
At the same time, LongChat-13B-16K is competitive compared to other models of similar sizes.
­</p>
<h3><a id="long-sequence-question-answer-benchmark" class="anchor" href="#long-sequence-question-answer-benchmark" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Long sequence question answer benchmark</h3>
<p>In the previous sections, we tested models on our long-range retrieval tasks and human preference tasks.
But how do these models perform on more complex academic long-range reasoning tasks?  In this section, we study this by running the Qasper question answering dataset. We use the validation set selection and prompts from the <a href="https://www.zero.scrolls-benchmark.com/">ZeroScrolls</a> long sequence benchmark.</p>
<br>
<p style="color:gray; text-align: center;">Table 3. ZeroScrolls benchmark (validation set)</p>
<div style="display: flex; justify-content: center;">
<table>
<tbody>
<tr> <th>Benchmark</th> <th>LongChat-13B-16K</th> <th>LongChat-7B-16k</th> <th>Vicuna-13B-v1.3</th> <th>Vicuna-7B-v1.3</th> <th>GPT-4-8k</th></tr>
<tr> <td>Qasper (F1)</td>  <td>0.286</td> <td>0.275</td> <td>0.220</td> <td>0.190</td> <td>0.356</td> </tr>
</tbody>
</table>
</div>
<p>­</p>
<p>We find that LongChat significantly outperforms Vicuna due to its extended context length. We leave more rigorous analysis on academic benchmarks for future work.</p>
<h2><a id="discussion" class="anchor" href="#discussion" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Discussion</h2>
<p>We find that LongChat-13B-16K experiences an accuracy drop when the context length is near 16K on the fine-grained line retrieval task. In our preliminary attempts, we conjecture that this is because it is near the maximal fine-tuning length. For instance, training on even longer (e.g., 32K) documents can alleviate this problem.
We are actively address this issue in a near-future release.</p>
<h2><a id="conclusion" class="anchor" href="#conclusion" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>In our evaluations, commercial long-context models always fulfill their promises: GPT-3.5-16K and Anthropic Claude-v3 (almost) achieve perfect performance in both benchmarks.
However, existing open-source models often do not perform well in their claimed context length.</p>
<p style="color:gray; text-align: center;">Table 4. Ability levels of open source models supporting long context</p>
<div style="display: flex; justify-content: center;">
<table>
<tbody>
<tr> <th></th> <th>Claimed Context Length</th> <th>Text generation</th> <th>Coarse Retrieval</th> <th>Fine-grained Retrieval</th></tr>
<tr> <td>Ability Description at claimed context length</td> <td>-</td> <td>Faithfully generate natural languages</td> <td>Retrieve information in a coarse granularity</td> <td>Retrieve information precisely in a fine-grained granularity</td> </tr>
<tr> <td><a target="_blank" href="https://huggingface.co/lmsys/longchat-13b-16k">LongChat-13B-16K </a> <td>16K</td> <td>⭐⭐⭐</td> <td>⭐⭐⭐</td> <td>⭐⭐</td></tr>
<tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-30b-chat">MPT-30B-chat</a></td> <td>8K</td> <td>⭐⭐⭐</td> <td>⭐⭐⭐</td> <td>⭐⭐</td></tr>
<tr> <td><a target="_blank" href="https://huggingface.co/mosaicml/mpt-7b-storywriter">MPT-7B-storywriter</a></td> <td>80K</td> <td>⭐⭐⭐</td> <td>⭐⭐</td> <td>⭐</td></tr>
<tr> <td><a target="_blank" href="https://huggingface.co/THUDM/chatglm2-6b">ChatGLM2-6B</a></td> <td>8K</td>  <td>⭐⭐⭐</td> <td>⭐⭐</td> <td>⭐</td></tr>
<tr> <td><a target="_blank" href="https://chat.openai.com/">GPT-3.5-turbo</a></td> <td>16K</td> <td>⭐⭐⭐</td> <td>⭐⭐⭐</td> <td>⭐⭐⭐</td></tr>
<tr> <td><a target="_blank" href="https://www.anthropic.com/index/introducing-claude">Anthropic Claude-1.3</a></td> <td>100K</td> <td>⭐⭐⭐</td> <td>⭐⭐⭐</td> <td>⭐⭐⭐</td></tr>
</tbody>
</table>
</div>
<p>­</p>
<p>We qualitatively illustrate the level of performance in Table 4, and we would like to make our final thoughts -- There are gaps between being able to generate coherent text and being able to retrieve or reason on long context.
We call for the community to contribute to more evaluation benchmarks of long-context chatbots and further understand and bridge the gap.</p>
<h2><a id="next-steps" class="anchor" href="#next-steps" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Next Steps</h2>
<p>Inspired by the promising performance and the simple training recipe of our 16K models, we would like to explore how to build chatbots with even longer context.
We have observed many efficiency issues (e.g., memory and throughput) during training and inference using chatbots with much longer context length.
We plan to develop new system technologies to improve LLMs' performance at long context.</p>
<h2><a id="disclaimer" class="anchor" href="#disclaimer" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Disclaimer</h2>
<p>The benchmark LongEval introduced in this blogpost is not yet a comprehensive benchmark that should be used as the only indicator.
We are actively working on more systematic benchmarking.</p>
<h2><a id="the-team" class="anchor" href="#the-team" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Team</h2>
<p>The LongChat models and this blog post are developed, evaluated, and maintained by the following members:
Dacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, Hao Zhang.</p>
<p>(* Joint first author)</p>
<h2><a id="citation" class="anchor" href="#citation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Citation</h2>
<p>If you find our LongChat models or LongEval tools helpful, please consider citing this blog post via:</p>
<pre><code>@misc{longchat2023,
    title = {How Long Can Open-Source LLMs Truly Promise on Context Length?},
    url = {https://lmsys.org/blog/2023-06-29-longchat},
    author = {Dacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang},
    month = {June},
    year = {2023}
}
</code></pre>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"How Long Can Open-Source LLMs Truly Promise on Context Length?","author":"The LongChat Team","date":"June 29, 2023","previewImg":"/images/blog/longchat/topic_retrieval_preview.png"},"content":"\nIn this blogpost, we introduce our latest series of chatbot models, LongChat-7B and LongChat-13B, featuring a new level of extended context length up to 16K tokens.\nEvaluation results show that the long-range retrieval accuracy of LongChat-13B is up to 2x higher than other long-context open models such as MPT-7B-storywriter (84K), MPT-30B-chat (8K), and ChatGLM2-6B (8k).\nLongChat shows promising results in closing the gap between open models and proprietary long context models such as Claude-100K and GPT-4-32K.\n\n\u003cimg src=\"/images/blog/longchat/topic_retrieval.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 1: Comparing LongChat to other models on the long-range topic retrieval task.\u003c/p\u003e\n\n\n\nNot only can LongChat models handle such a long context length, but they also precisely follow human instructions in dialogues and demonstrate strong performance in the human preference benchmark [MT-Bench](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge). \nTheir preview versions are available at HuggingFace: [lmsys/longchat-13b-16k](https://huggingface.co/lmsys/longchat-13b-16k) and [lmsys/longchat-7b-16k](https://huggingface.co/lmsys/longchat-7b-16k).\nYou can try them immediately in CLI or web interface using FastChat:\n\n```python\npython3 -m fastchat.serve.cli --model-path lmsys/longchat-7b-16k\n```\n\nThere has been a significant surge of interest within the open-source community in developing language models with longer context or extending the context length of existing models like LLaMA. \nThis trend has led to interesting observations and extensive discussions in various sources, such as [Kaiokendev’s blog](https://kaiokendev.github.io/context) and this [arXiv manuscript](https://arxiv.org/pdf/2306.15595.pdf); \nmeanwhile, several notable models have been released claiming to support much longer context than LLaMA, notable ones include:\n- [MPT-7B-storywriter](https://huggingface.co/mosaicml/mpt-7b-storywriter) supports 65K context length and extrapolates to 84K. \n- [MPT-30B-chat](https://huggingface.co/spaces/mosaicml/mpt-30b-chat) supports 8K context length.\n- [ChatGLM2-6B](https://huggingface.co/THUDM/chatglm2-6b) supports 8K context.\n\nAt LMSYS Org, we have been concurrently exploring various techniques to lengthen the context of our models like [Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.3). \nIn this blogpost, alongside the release of the LongChat series, we share our [evaluation tools](https://github.com/DachengLi1/LongChat) to verify the long-context capability of LLMs. \n\nUsing our evaluation tools in combination with various academic long-context evaluation benchmarks, we conduct a thorough comparison of several open-source and commercial models that claim to support long context. \nThrough this analysis, we examine how well these models deliver on their promised context length.\nWe found that *while commercial models like GPT-3.5-turbo performs well on our tests, many open source models do not deliver the expected results on their promised context length*.\n\nThe data and code used to reproduce the results in the blog post are available in our LongChat [repo](https://github.com/DachengLi1/LongChat/tree/longeval). \nWe provide a visualization in this [notebook](https://github.com/DachengLi1/LongChat/blob/longeval/longeval/topics_lines_demo.ipynb).\n\n## LongChat Training Recipe\n\nLongChat is finetuned from LLaMA models, which were originally pretrained with 2048 context length. \nThe training recipe can be conceptually described in two steps:\n\n### Step 1: Condensing rotary embeddings\n[Rotary position embedding](https://arxiv.org/abs/2104.09864v4) is a type of positional embedding that injects the information of position in Transformer. \nIt is implemented in Hugging Face transformer by:\n```python\nquery_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n```\nWhere position_ids are indices such as 1, 2, 3, ... that denote the position of a token in the sentence. \nFor instance, the token \"today\" in the sentence \"today is a good day\" has position_ids 1. \nThe `apply_rotary_pos_emb()` function then applies a [transformation](https://arxiv.org/pdf/2104.09864.pdf) based on the provided position_ids.\n\nThe LLaMA model is pre-trained with rotary embedding on sequence length 2048, which means that it has not observed scenarios where position_ids \u003e 2048 during the pre-training phase. \nInstead of forcing the LLaMA model to adapt to position_ids \u003e 2048, we condense position_ids \u003e 2048 to be within 0 to 2048. \nIntuitively, we conjecture this condensation can maximally reuse the model weights learned in the pre-training stage. See more insights from [Kaiokendev’s blog](https://kaiokendev.github.io/context).\n\nWe define the term `condensation ratio` by dividing the target new context length `y` by 2048. We then divide every position_ids by this ratio and feed it into the `apply_rotary_pos_emb()` function.\n```python\nquery_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids / ratio)\n```\nIn this release, we fine-tune the model to a context length of 16384, and thus the condensation ratio is 8. For instance, a token with position_ids = 10000 becomes position_ids = 10000 / 8 = 1250, and the neighboring token 10001 becomes 10001 / 8 = 1250.125. \nThis step requires no training.\n\n### Step 2: Finetuning on Curated Conversation Data\nAfter condensing the embedding, we perform the finetuning procedure on our curated conversation dataset. \nWe reuse our collected user-shared conversations previously used for training Vicuna. \nWe clean the data using FastChat data pipeline, and truncate these conversations so they are no longer than 16K. \nWe finetune the model using standard next-token prediction loss. We fine-tune the 7B and 13B models with 80k and 18k conversations, respectively. \nTo save memory, we use Pytorch FSDP and Flash Attention. Assume A100 is $3/hour on Cloud, the 7B model costs ~$300, and the 13B model costs ~$700. \n\n## Evaluation toolkits: LongEval\nRecently, commercial and open-source models have continued to tout their abilities to support expanded context length (from 8K, 32K, 84K, to 100K) in their latest releases, but how can we verify these claims?\nThe term \"long-context capability\" can mean different things for different model providers. For instance, does [MPT-7B-StoryWriter's](https://huggingface.co/mosaicml/mpt-7b-storywriter) advertised 84K context length operate at the same capacity as OpenAI’s ChatGPT at 16K? \nThis issue is also prevalent in our LongChat models development: how do we swiftly and effectively confirm if a freshly trained model can handle the intended context length?\n\nTo address this, we can base our evaluations on tasks that necessitate LLMs to process lengthy contexts, such as text generation, retrieval, summarization, and information association in long text sequences. \nInspired by [recent discussions](https://twitter.com/DimitrisPapail/status/1658091355632189440), we've devised, [LongEval](https://github.com/DachengLi1/LongChat.git), a long context test suite. \nThis suite incorporates two tasks of varying degrees of difficulty, providing a simple and swift way to measure and compare long-context performance.\n\n### Task 1: Coarse-grained Topic Retrieval\nIn real-world long conversations, users usually talk about and jump between several topics with the chatbot. The Topic Retrieval task mimics this scenario by asking the chatbot to retrieve the first topic in a long conversation consisting of multiple topics. An example task is:\n```python\n… (instruction of the task)\nUSER: I would like to discuss \u003cTOPIC-1\u003e\nASSISTANT: Sure! What about xxx of \u003cTOPIC-1\u003e?\n… (a multi-turn conversation of \u003cTOPIC-1\u003e)\nUSER: I would like to discuss  \u003cTOPIC-2\u003e\n…\nUSER: I would like to discuss \u003cTOPIC-k\u003e\n… \nUSER: What is the first topic we discussed?\nASSISTANT: \n```\nThis task tests whether the model can locate a chunk of text and associate it with the right topic name. We design a conversation to be 400 ~ 600 tokens long. Thus, this task is considered coarse-grained because the model may give correct predictions when it locates positions not too far away (\u003c500 token distance) from the right ones.\n\n### Task 2: Fine-grained Line Retrieval\nTo further test the model ability to locate and associate texts from a long conversation, we introduce a finer-grained Line Retrieval test. In this test, the chatbot needs to precisely retrieve a number from a long document, instead of a topic from long multi-round conversations. Below is an example:\n```python\nline torpid-kid: REGISTER_CONTENT is \u003c24169\u003e\nline moaning-conversation: REGISTER_CONTENT is \u003c10310\u003e\n…\nline tacit-colonial: REGISTER_CONTENT is \u003c14564\u003e\nWhat is the \u003cREGISTER_CONTENT\u003e in line moaning-conversation?\n```\n\nThe task was originally proposed in [Little Retrieval Test](https://github.com/anadim/the-little-retrieval-test). \nThe original testcase uses numbers to denote a line, which we found smaller LLMs usually cannot comprehend well. \nTo disentangle these factors and make them more suitable for testing open-source chatbots at various sizes, we improve it by using random natural language (e.g., torpid-kid) instead.\n\nWe found these two tasks behave with the expected characteristics:\n1. The task can effectively capture the abilities of text generation, retrieval, and information association at long context, reflected by the retrieving accuracy.\n2. It is easy to extend the tests to arbitrary lengths to test models’ capacity under different context lengths.\n3. We have run sanity checks of both tasks and observed the expected results. For example, the vanilla LLaMA models, pretrained with a 2K context length, can achieve perfect accuracy on both tasks when the test inputs length is \u003c2K, but will immediately fail (nearly 0 accuracy) on any test inputs beyond 2K.\n\nMore details and example usage of LongEval can be found in this [notebook](https://github.com/DachengLi1/LongChat/blob/longeval/longeval/topics_lines_demo.ipynb).\n\n\n## Results and findings\nIn this section, we share our evaluation and findings.\n\u003cbr\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 1. Model Specifications.\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: center;\"\u003e\n\u003ctable id=\"Table1\"\u003e\n\u003ctbody\u003e\n\u003ctr\u003e \u003cth\u003eModel\u003c/th\u003e \u003cth\u003eSize\u003c/th\u003e \u003cth\u003eInstruction-tuned?\u003c/th\u003e \u003cth\u003ePretrained Context Length\u003c/th\u003e \u003cth\u003eFinetune Context Length\u003c/th\u003e \u003cth\u003eClaimed Context Length\u003c/th\u003e \u003cth\u003eOpen Source?\u003c/th\u003e\u003c/tr\u003e\n\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-30b-chat\"\u003eMPT-30-chat\u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e30B\u003c/td\u003e  \u003ctd\u003eYes\u003c/td\u003e  \u003ctd\u003e8K\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e \u003ctd\u003e8K\u003c/td\u003e \u003ctd\u003eYes\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-7b-storywriter\"\u003eMPT-7b-storywriter\u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e7B\u003c/td\u003e \u003ctd\u003eYes\u003c/td\u003e  \u003ctd\u003e2K\u003c/td\u003e  \u003ctd\u003e65K\u003c/td\u003e  \u003ctd\u003e84K\u003c/td\u003e \u003ctd\u003eYes\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/THUDM/chatglm2-6b\"\u003eChatGLM2-6b\u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e6B\u003c/td\u003e  \u003ctd\u003eYes\u003c/td\u003e  \u003ctd\u003e32K\u003c/td\u003e  \u003ctd\u003e8K\u003c/td\u003e \u003ctd\u003e8K\u003c/td\u003e \u003ctd\u003eYes\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/lmsys/longchat-13b-16k\"\u003eLongChat-13b-16k (ours)\u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e13B\u003c/td\u003e  \u003ctd\u003eYes\u003c/td\u003e \u003ctd\u003e2K\u003c/td\u003e  \u003ctd\u003e16K\u003c/td\u003e  \u003ctd\u003e16K\u003c/td\u003e \u003ctd\u003eYes\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://chat.openai.com/\"\u003eGPT-3.5-turbo\u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e \u003ctd\u003e-\u003c/td\u003e \u003ctd\u003e16K\u003c/td\u003e  \u003ctd\u003eNo\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://www.anthropic.com/index/introducing-claude\"\u003eAnthropic Claude-1.3\u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e  \u003ctd\u003e-\u003c/td\u003e \u003ctd\u003e-\u003c/td\u003e \u003ctd\u003e100K\u003c/td\u003e  \u003ctd\u003eNo\u003c/td\u003e \u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\n\u0026shy;\n\n\nIn particular, we consider four open-sourced models and two proprietary models, listed in Table 1.\n\n\n### LongEval results\nFrom the coarse-grained topic retrieval test results (Figure 2 at the beginning), we observe the problematic performance of open-source long-context models. For instance, MPT-7B-storywriter claims to have a context length of 84K but barely achieves 50% accuracy even at one-fifth of its claimed context length (16K). \nChatGLM2-6B cannot reliably retrieve the first topic at the length of 6K (46% accuracy). On the other hand, LongChat-13B-16K model reliably retrieves the first topic, with comparable accuracy to GPT-3.5-turbo.\n\n\u003cimg src=\"/images/blog/longchat/line_retrieval.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto;\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 3: Accuracy on the long-range line retrieval task.\u003c/p\u003e\n\nIn the fine-grained line retrieval test, MPT-7B-storywriter performs even worse -- the accuracy drops from ~50% to ~30%. ChatGLM2-6B also observes degradation and does not perform well at 5K context length (32%). \nWe notice that ChatGLM2-6B states that it has not been yet fully optimized for single-turn long document understanding, which could explain its current performance on LongEval. \nLongChat-13B-16K performs closely to GPT-3.5 and Claude-v3 within 12K context length. However, we also find the preview versions are not perfect at 12K-16K, see the [discussion section](https://lmsys.org/blog/2023-06-29-longchat/#discussion).\n\n\n**Disentangle irrelevant LLM abilities in LongEval**\n\nIn topics and line retrieval tests, we observe mistakes caused by factors irrelevant to long-context ability, such as the instruction-following ability. For instance, in the Line Retrieval test, the model may simply respond “sure, I will tell you the number” instead of returning an actual number. \nTo give a fair comparison, we took two actions to avoid factors irrespective of long-context capabilities: prompt engineering and estimating accuracy only based on cases in which the models correctly follow instructions. Check our codes for details.\n\n### Human preference benchmark (MT-bench)\nIn the previous section, we observed that LongChat models perform well on long-range retrieval tasks, but does this come with a significant drop in human preference? To test whether it still follows human preferences, we use GPT-4 graded [MT-bench](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge), a set of challenging multi-turn conversation questions.\n\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 2. MT-bench scores comparing LongChat-13B to other models of similar sizes.\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: center;\"\u003e\n\u003ctable id=\"Table1\" style=\"max-width: 400px;\"\u003e\n\u003ctbody\u003e\n\u003ctr\u003e \u003cth\u003eModel\u003c/th\u003e \u003cth\u003eMT-bench (score)\u003c/th\u003e\u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/lmsys/longchat-13b-16k\"\u003eLongChat-13B-16K\u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e5.95\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/lmsys/vicuna-13b-v1.3\"\u003eVicuna-13B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e6.39\u003c/td\u003e  \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/WizardLM/WizardLM-13B-V1.0\"\u003e WizardLM-13B\u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e6.35\u003c/td\u003e  \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/project-baize/baize-v2-13b\"\u003e Baize-v2-13B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e5.75\u003c/td\u003e  \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/NousResearch/Nous-Hermes-13b\"\u003e Nous-Hermes-13B \u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e5.51\u003c/td\u003e   \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\"\u003e Alpaca-13B\u003c/a\u003e\u003c/td\u003e  \u003ctd\u003e4.53\u003c/td\u003e  \u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\nWe find that LongChat-13B-16K is comparable to its closest alternative -- Vicuna-13B, which indicates that this long-range ability does not come with a significant sacrifice of its short-range ability. \nAt the same time, LongChat-13B-16K is competitive compared to other models of similar sizes.\n\u0026shy;\n\n### Long sequence question answer benchmark \nIn the previous sections, we tested models on our long-range retrieval tasks and human preference tasks. \nBut how do these models perform on more complex academic long-range reasoning tasks?  In this section, we study this by running the Qasper question answering dataset. We use the validation set selection and prompts from the [ZeroScrolls](https://www.zero.scrolls-benchmark.com/) long sequence benchmark.\n\n\u003cbr\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 3. ZeroScrolls benchmark (validation set)\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: center;\"\u003e\n\u003ctable\u003e\n\u003ctbody\u003e\n\u003ctr\u003e \u003cth\u003eBenchmark\u003c/th\u003e \u003cth\u003eLongChat-13B-16K\u003c/th\u003e \u003cth\u003eLongChat-7B-16k\u003c/th\u003e \u003cth\u003eVicuna-13B-v1.3\u003c/th\u003e \u003cth\u003eVicuna-7B-v1.3\u003c/th\u003e \u003cth\u003eGPT-4-8k\u003c/th\u003e\u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003eQasper (F1)\u003c/td\u003e  \u003ctd\u003e0.286\u003c/td\u003e \u003ctd\u003e0.275\u003c/td\u003e \u003ctd\u003e0.220\u003c/td\u003e \u003ctd\u003e0.190\u003c/td\u003e \u003ctd\u003e0.356\u003c/td\u003e \u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\n\u0026shy;\n\nWe find that LongChat significantly outperforms Vicuna due to its extended context length. We leave more rigorous analysis on academic benchmarks for future work.\n\n## Discussion\nWe find that LongChat-13B-16K experiences an accuracy drop when the context length is near 16K on the fine-grained line retrieval task. In our preliminary attempts, we conjecture that this is because it is near the maximal fine-tuning length. For instance, training on even longer (e.g., 32K) documents can alleviate this problem. \nWe are actively address this issue in a near-future release.\n\n## Conclusion\nIn our evaluations, commercial long-context models always fulfill their promises: GPT-3.5-16K and Anthropic Claude-v3 (almost) achieve perfect performance in both benchmarks. \nHowever, existing open-source models often do not perform well in their claimed context length.\n\n\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 4. Ability levels of open source models supporting long context\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: center;\"\u003e\n\u003ctable\u003e\n\u003ctbody\u003e\n\u003ctr\u003e \u003cth\u003e\u003c/th\u003e \u003cth\u003eClaimed Context Length\u003c/th\u003e \u003cth\u003eText generation\u003c/th\u003e \u003cth\u003eCoarse Retrieval\u003c/th\u003e \u003cth\u003eFine-grained Retrieval\u003c/th\u003e\u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003eAbility Description at claimed context length\u003c/td\u003e \u003ctd\u003e-\u003c/td\u003e \u003ctd\u003eFaithfully generate natural languages\u003c/td\u003e \u003ctd\u003eRetrieve information in a coarse granularity\u003c/td\u003e \u003ctd\u003eRetrieve information precisely in a fine-grained granularity\u003c/td\u003e \u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/lmsys/longchat-13b-16k\"\u003eLongChat-13B-16K \u003c/a\u003e \u003ctd\u003e16K\u003c/td\u003e \u003ctd\u003e⭐⭐⭐\u003c/td\u003e \u003ctd\u003e⭐⭐⭐\u003c/td\u003e \u003ctd\u003e⭐⭐\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-30b-chat\"\u003eMPT-30B-chat\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e8K\u003c/td\u003e \u003ctd\u003e⭐⭐⭐\u003c/td\u003e \u003ctd\u003e⭐⭐⭐\u003c/td\u003e \u003ctd\u003e⭐⭐\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-7b-storywriter\"\u003eMPT-7B-storywriter\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e80K\u003c/td\u003e \u003ctd\u003e⭐⭐⭐\u003c/td\u003e \u003ctd\u003e⭐⭐\u003c/td\u003e \u003ctd\u003e⭐\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://huggingface.co/THUDM/chatglm2-6b\"\u003eChatGLM2-6B\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e8K\u003c/td\u003e  \u003ctd\u003e⭐⭐⭐\u003c/td\u003e \u003ctd\u003e⭐⭐\u003c/td\u003e \u003ctd\u003e⭐\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://chat.openai.com/\"\u003eGPT-3.5-turbo\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e16K\u003c/td\u003e \u003ctd\u003e⭐⭐⭐\u003c/td\u003e \u003ctd\u003e⭐⭐⭐\u003c/td\u003e \u003ctd\u003e⭐⭐⭐\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://www.anthropic.com/index/introducing-claude\"\u003eAnthropic Claude-1.3\u003c/a\u003e\u003c/td\u003e \u003ctd\u003e100K\u003c/td\u003e \u003ctd\u003e⭐⭐⭐\u003c/td\u003e \u003ctd\u003e⭐⭐⭐\u003c/td\u003e \u003ctd\u003e⭐⭐⭐\u003c/td\u003e\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\n\u0026shy;\n\nWe qualitatively illustrate the level of performance in Table 4, and we would like to make our final thoughts -- There are gaps between being able to generate coherent text and being able to retrieve or reason on long context.\nWe call for the community to contribute to more evaluation benchmarks of long-context chatbots and further understand and bridge the gap. \n\n## Next Steps\nInspired by the promising performance and the simple training recipe of our 16K models, we would like to explore how to build chatbots with even longer context. \nWe have observed many efficiency issues (e.g., memory and throughput) during training and inference using chatbots with much longer context length. \nWe plan to develop new system technologies to improve LLMs' performance at long context.\n\n## Disclaimer\nThe benchmark LongEval introduced in this blogpost is not yet a comprehensive benchmark that should be used as the only indicator. \nWe are actively working on more systematic benchmarking.\n\n## The Team\nThe LongChat models and this blog post are developed, evaluated, and maintained by the following members:\nDacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, Hao Zhang.\n\n(* Joint first author)\n\n## Citation\nIf you find our LongChat models or LongEval tools helpful, please consider citing this blog post via:\n```\n@misc{longchat2023,\n    title = {How Long Can Open-Source LLMs Truly Promise on Context Length?},\n    url = {https://lmsys.org/blog/2023-06-29-longchat},\n    author = {Dacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang},\n    month = {June},\n    year = {2023}\n}\n```\n","slug":"2023-06-29-longchat"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2023-06-29-longchat"},"buildId":"EtN_-n3tZqyhAwOvuAgy8","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>