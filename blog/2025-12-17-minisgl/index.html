<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>Mini-SGLang: Efficient Inference Engine in a Nutshell | LMSYS Org</title><meta name="title" content="Mini-SGLang: Efficient Inference Engine in a Nutshell | LMSYS Org"/><meta property="og:title" content="Mini-SGLang: Efficient Inference Engine in a Nutshell | LMSYS Org"/><meta name="twitter:title" content="Mini-SGLang: Efficient Inference Engine in a Nutshell | LMSYS Org"/><meta name="description" content="&lt;p&gt;We&#x27;re excited to introduce &lt;strong&gt;Mini-SGLang&lt;/strong&gt;, a lightweight yet high-performance inference framework for Large Language Models (LLMs). Derived ..."/><meta property="og:description" content="&lt;p&gt;We&#x27;re excited to introduce &lt;strong&gt;Mini-SGLang&lt;/strong&gt;, a lightweight yet high-performance inference framework for Large Language Models (LLMs). Derived ..."/><meta name="twitter:description" content="&lt;p&gt;We&#x27;re excited to introduce &lt;strong&gt;Mini-SGLang&lt;/strong&gt;, a lightweight yet high-performance inference framework for Large Language Models (LLMs). Derived ..."/><meta property="og:image" content="https://lmsys.org/images/blog/minisgl/logo.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/minisgl/logo.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2025-12-17-minisgl"/><meta name="twitter:url" content="https://lmsys.org/blog/2025-12-17-minisgl"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d62cc293bc63f5ee.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/lxEPiI5h6ilqv5JE5PHNj/_buildManifest.js" defer=""></script><script src="/_next/static/lxEPiI5h6ilqv5JE5PHNj/_ssgManifest.js" defer=""></script><script src="/_next/static/lxEPiI5h6ilqv5JE5PHNj/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.io" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">Mini-SGLang: Efficient Inference Engine in a Nutshell</h1><p class="text-xl pt-2 pb-2">by: <!-- -->Ziyi Xu<!-- -->,<!-- --> <!-- -->Dec 17, 2025<!-- --></p><hr/><div class="pt-2 article"><p>We're excited to introduce <strong>Mini-SGLang</strong>, a lightweight yet high-performance inference framework for Large Language Models (LLMs). Derived from the <a href="https://github.com/sgl-project/sglang">SGLang</a> project, Mini-SGLang is designed to demystify the complexities of modern serving systems. Despite its compact codebase, it retains the advanced features that define state-of-the-art performance, including <strong>Radix Attention</strong> for efficient KV cache reuse, <strong>Chunked Prefill</strong> for controlled memory footprint, <strong>Overlap Scheduling</strong> for reduced CPU overhead, and <strong>Tensor Parallelism</strong> for scalable distributed serving. With an OpenAI-compatible API and out-of-the-box support for models like Llama-3 and Qwen-3, Mini-SGLang serves as both a capable inference engine and a transparent reference implementation for researchers and developers.</p>
<p>The source code is available at <a href="https://github.com/sgl-project/mini-sglang">https://github.com/sgl-project/mini-sglang</a>.</p>
<!-- ![Header](/images/blog/minisgl/logo.png) -->
<h2><a id="motivation-why-mini-sglang" class="anchor" href="#motivation-why-mini-sglang" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Motivation: Why Mini-SGLang?</h2>
<p>Although SGLang has achieved state-of-the-art inference performance with a comprehensive feature set, its codebase has grown massive, reaching nearly 300k lines of Python code. To address the complexity barrier for learners and researchers, we built Mini-SGLang, focusing on two main objectives: providing learning resources and enabling fast prototyping for research.</p>
<h3><a id="educational-purposes" class="anchor" href="#educational-purposes" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Educational Purposes</h3>
<p>Mini-SGLang features a clean, highly modular codebase of only <strong>5k lines of Python code</strong>, which makes it significantly easier for beginners to understand the core components of a modern LLM serving engine.</p>
<p>Despite its simplicity, Mini-SGLang supports both online and offline inference and implements essential modern optimizations, including <strong>Tensor Parallelism</strong>, <strong>Overlap Scheduling</strong>, <strong>Chunked Prefill</strong>, <strong>Radix Cache</strong>, and <strong>JIT CUDA kernels</strong>. This makes it a comprehensive learning resource.</p>
<h3><a id="quick-research-prototype" class="anchor" href="#quick-research-prototype" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Quick Research Prototype</h3>
<p>Many ML and system researchers struggle to integrate their optimizations into existing framework. On one hand, injecting new logic into complex frameworks like SGLang is risky: you may easily break implicit invariants of the system, which gives rise to subtle bugs. On the other hand, building an inference engine from scratch is tedious, requiring significant effort to handle infrastructure details (e.g., frontend servers, tokenization, NCCL communication) just to match state-of-the-art baselines.</p>
<p>Mini-SGLang strikes a balance. It started as a research prototype we used to validate new system ideas quickly, without spending weeks handling a full-scale codebase or re-implementing infrastructure from scratch. It offers an out-of-the-box, high-performance framework that is easy to inspect, extend and optimize. It handles the heavy lifting of infrastructure while being flexible enough for rapid prototyping. Additionally, Mini-SGLang provides <strong>OpenAI-compatible benchmark utilities</strong>, facilitating end-to-end performance analysis and comparison against various serving engines, such as <a href="https://github.com/sgl-project/sglang">SGLang</a>, <a href="https://github.com/vllm-project/vllm">vLLM</a> and <a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a>. For kernel developers, Mini-SGLang also provides fine-grained <strong>NVTX annotations</strong>, which are very valuable for kernel debugging and performance profiling.</p>
<h2><a id="features" class="anchor" href="#features" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Features</h2>
<p>Mini-SGLang shares the same high-level system architecture as SGLang, consisting of a frontend API server, a tokenizer server, and a backend scheduler for each GPU.</p>
<p><img src="/images/blog/minisgl/design.drawio.png" alt="system-design"></p>
<h3><a id="overlap-scheduling" class="anchor" href="#overlap-scheduling" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Overlap Scheduling</h3>
<p>LLM inference is not just about GPU computation; a significant amount of work is handled by the CPU, including batch scheduling, memory management, and token processing. Without optimization, this CPU overhead can lead to GPU idling, hurting overall performance.</p>
<p>Mini-SGLang implements an <strong>overlap scheduling</strong> mechanism, similar to the one in SGLang, to mitigate this. By preparing the next batch of requests on the CPU while the GPU is busy with the current batch, it effectively hides the CPU overhead. As the Nsight-Systems profile below shows, this keeps the GPU consistently utilized, eliminating GPU idleness and maximizing throughput. More technical details are available in our <a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/">previous blog post</a>.</p>
<p><img src="/images/blog/minisgl/overlap.png" alt="overlap"></p>
<blockquote>
<p>An example of overlapped execution. CPU execution overhead is fully overlapped.</p>
</blockquote>
<p><img src="/images/blog/minisgl/no-overlap.png" alt="no-overlap"></p>
<blockquote>
<p>An example of non-overlapped execution. CPU execution overhead leads to substantial GPU stalls.</p>
</blockquote>
<p>To run an ablation study without overlap scheduling, set the environment variable <code>MINISGL_DISABLE_OVERLAP_SCHEDULING=1</code>.</p>
<h3><a id="high-performance-kernels" class="anchor" href="#high-performance-kernels" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>High-Performance Kernels</h3>
<p>Mini-SGLang integrates state-of-the-art attention kernels to ensure top performance. It leverages <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention-3</a> for prefill kernel and <a href="https://github.com/flashinfer-ai/flashinfer">FlashInfer</a> for decode kernel on NVIDIA Hopper architecture.</p>
<p>Following <a href="https://github.com/flashinfer-ai/flashinfer">FlashInfer</a> and <a href="https://github.com/sgl-project/sglang">SGLang</a>, Mini-SGLang also integrates just-in-time (JIT) compiled kernel for better runtime performance. We adopt <a href="https://github.com/apache/tvm-ffi">TVM FFI</a> for Python binding, which is much faster than the default PyTorch interface due to its lightweight design.</p>
<h3><a id="interactive-shell-mode" class="anchor" href="#interactive-shell-mode" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Interactive Shell Mode</h3>
<p>For direct interaction and testing, Mini-SGLang includes a simple shell mode. This allows users to chat with LLMs directly from the command line, providing a convenient way to test models and observe their behavior without needing a separate client.</p>
<p><img src="/images/blog/minisgl/shell.png" alt="Shell Example"></p>
<h2><a id="performance-benchmark" class="anchor" href="#performance-benchmark" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Performance Benchmark</h2>
<p>To evaluate the performance of Mini-SGLang, we conducted comprehensive experiments covering both offline throughput and online serving latency.</p>
<h3><a id="offline-inference-throughput" class="anchor" href="#offline-inference-throughput" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Offline Inference Throughput</h3>
<p>We evaluated Mini-SGLang's offline throughput against Nano-vLLM on a single NVIDIA H200 GPU. Following the methodology from <a href="https://github.com/GeeeekExplorer/nano-vllm/">Nano-vLLM</a>, we used the <a href="https://huggingface.co/Qwen/Qwen3-0.6B/">Qwen3-0.6B</a> model and also tested the larger <a href="https://huggingface.co/Qwen/Qwen3-14B/">Qwen3-14B</a> model to assess performance at scale. We focused on Qwen3 models due to the current limitations of the Nano-vLLM baseline.</p>
<p>The throughput results (in tokens per second) are shown below:</p>
<p><img src="/images/blog/minisgl/offline.png" alt="Offline-Benchmark"></p>
<p>The results show that Mini-SGLang consistently outperforms Nano-vLLM baseline on both Qwen3 models, thanks to our <strong>overlap scheduling</strong> mechanism that effectively hides CPU overhead.</p>
<p><strong>Reproducibility</strong>: The offline benchmark script is available at <a href="https://github.com/sgl-project/mini-sglang/blob/main/benchmark/offline/bench.py">this link</a>.</p>
<h3><a id="online-serving-latency" class="anchor" href="#online-serving-latency" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Online Serving Latency</h3>
<p>To assess real-world serving performance, we benchmarked Mini-SGLang against SGLang using a realistic workload from the <a href="https://github.com/alibaba-edu/qwen-bailian-usagetraces-anon/blob/main/qwen_traceA_blksz_16.jsonl">Qwen trace</a>. We replayed 1,000 requests to a <a href="https://huggingface.co/Qwen/Qwen3-32B">Qwen3-32B</a> model deployed with 4-way tensor parallelism on 4 H200 GPUs. We measured throughput, 90th percentile (P90) Time To First Token (TTFT), and Time Between Tokens (TBT).</p>
<p><img src="/images/blog/minisgl/online.png" alt="Online-Benchmark"></p>
<p>The results demonstrate that Mini-SGLang achieves nearly identical performance to SGLang, confirming that its lightweight design does not compromise on throughput or latency.</p>
<p><strong>Reproducibility</strong>: Use the following commands to launch each system:</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># Mini-SGLang</span>
python -m minisgl --model <span class="hljs-string">&quot;Qwen/Qwen3-32B&quot;</span> --tp 4 --cache naive 

<span class="hljs-comment"># SGLang</span>
python3 -m sglang.launch_server --model <span class="hljs-string">&quot;Qwen/Qwen3-32B&quot;</span> --tp 4 \
    --disable-radix --port 1919 --decode-attention flashinfer
</code></pre>
<p>The online benchmark script is available at <a href="https://github.com/sgl-project/mini-sglang/blob/main/benchmark/online/bench_qwen.py">this link</a>.</p>
<h2><a id="conclusion" class="anchor" href="#conclusion" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>Mini-SGLang successfully distills the power of a state-of-the-art inference engine into a compact and understandable codebase. By retaining key optimizations like overlap scheduling and high-performance attention kernels, it delivers impressive performance while serving as an invaluable educational tool and a flexible platform for research.</p>
<p>We invite you to explore the <a href="https://github.com/sgl-project/mini-sglang">source code</a>, run the benchmarks, and see for yourself how Mini-SGLang makes high-performance LLM inference more accessible than ever.</p>
<h2><a id="acknowledgements" class="anchor" href="#acknowledgements" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgements</h2>
<ul>
<li>We would like to thank the SGLang team and community for their generous support and feedback, especially Liangsheng Yin, Lianmin Zheng and many others.</li>
<li>We would like to thank <a href="https://github.com/MisakaVan">MisakaVan</a> for his prominent contribution in testing, documentation, code improvement, and <a href="https://github.com/Conless">Yi Pan</a> for the initial PyTorch implementation of C++ NCCL communicator.</li>
<li>We would like to thank <a href="https://peterzheng98.github.io/">Wenxin Zheng</a> from SJTU for serving as a teaching assistant for the experimental lab course based on Mini-SGLang during the 2025 summer term, and for his support in course organization and student guidance.</li>
<li>We learn a lot from the system design of SGLang, FlashInfer, vLLM and Nano-vLLM, which jointly help make Mini-SGLang a clean yet robust system.</li>
</ul>
</div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"Mini-SGLang: Efficient Inference Engine in a Nutshell","author":"Ziyi Xu","date":"December 17, 2025","previewImg":"/images/blog/minisgl/logo.png"},"content":"\nWe're excited to introduce **Mini-SGLang**, a lightweight yet high-performance inference framework for Large Language Models (LLMs). Derived from the [SGLang](https://github.com/sgl-project/sglang) project, Mini-SGLang is designed to demystify the complexities of modern serving systems. Despite its compact codebase, it retains the advanced features that define state-of-the-art performance, including **Radix Attention** for efficient KV cache reuse, **Chunked Prefill** for controlled memory footprint, **Overlap Scheduling** for reduced CPU overhead, and **Tensor Parallelism** for scalable distributed serving. With an OpenAI-compatible API and out-of-the-box support for models like Llama-3 and Qwen-3, Mini-SGLang serves as both a capable inference engine and a transparent reference implementation for researchers and developers.\n\nThe source code is available at [https://github.com/sgl-project/mini-sglang](https://github.com/sgl-project/mini-sglang).\n\n\u003c!-- ![Header](/images/blog/minisgl/logo.png) --\u003e\n\n## Motivation: Why Mini-SGLang?\n\nAlthough SGLang has achieved state-of-the-art inference performance with a comprehensive feature set, its codebase has grown massive, reaching nearly 300k lines of Python code. To address the complexity barrier for learners and researchers, we built Mini-SGLang, focusing on two main objectives: providing learning resources and enabling fast prototyping for research.\n\n### Educational Purposes\n\nMini-SGLang features a clean, highly modular codebase of only **5k lines of Python code**, which makes it significantly easier for beginners to understand the core components of a modern LLM serving engine.\n\nDespite its simplicity, Mini-SGLang supports both online and offline inference and implements essential modern optimizations, including **Tensor Parallelism**, **Overlap Scheduling**, **Chunked Prefill**, **Radix Cache**, and **JIT CUDA kernels**. This makes it a comprehensive learning resource.\n\n### Quick Research Prototype\n\nMany ML and system researchers struggle to integrate their optimizations into existing framework. On one hand, injecting new logic into complex frameworks like SGLang is risky: you may easily break implicit invariants of the system, which gives rise to subtle bugs. On the other hand, building an inference engine from scratch is tedious, requiring significant effort to handle infrastructure details (e.g., frontend servers, tokenization, NCCL communication) just to match state-of-the-art baselines.\n\nMini-SGLang strikes a balance. It started as a research prototype we used to validate new system ideas quickly, without spending weeks handling a full-scale codebase or re-implementing infrastructure from scratch. It offers an out-of-the-box, high-performance framework that is easy to inspect, extend and optimize. It handles the heavy lifting of infrastructure while being flexible enough for rapid prototyping. Additionally, Mini-SGLang provides **OpenAI-compatible benchmark utilities**, facilitating end-to-end performance analysis and comparison against various serving engines, such as [SGLang](https://github.com/sgl-project/sglang), [vLLM](https://github.com/vllm-project/vllm) and [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM). For kernel developers, Mini-SGLang also provides fine-grained **NVTX annotations**, which are very valuable for kernel debugging and performance profiling.\n\n## Features\n\nMini-SGLang shares the same high-level system architecture as SGLang, consisting of a frontend API server, a tokenizer server, and a backend scheduler for each GPU.\n\n![system-design](/images/blog/minisgl/design.drawio.png)\n\n### Overlap Scheduling\n\nLLM inference is not just about GPU computation; a significant amount of work is handled by the CPU, including batch scheduling, memory management, and token processing. Without optimization, this CPU overhead can lead to GPU idling, hurting overall performance.\n\nMini-SGLang implements an **overlap scheduling** mechanism, similar to the one in SGLang, to mitigate this. By preparing the next batch of requests on the CPU while the GPU is busy with the current batch, it effectively hides the CPU overhead. As the Nsight-Systems profile below shows, this keeps the GPU consistently utilized, eliminating GPU idleness and maximizing throughput. More technical details are available in our [previous blog post](https://lmsys.org/blog/2024-12-04-sglang-v0-4/).\n\n![overlap](/images/blog/minisgl/overlap.png)\n\n\u003e An example of overlapped execution. CPU execution overhead is fully overlapped.\n\n![no-overlap](/images/blog/minisgl/no-overlap.png)\n\n\u003e An example of non-overlapped execution. CPU execution overhead leads to substantial GPU stalls.\n\nTo run an ablation study without overlap scheduling, set the environment variable `MINISGL_DISABLE_OVERLAP_SCHEDULING=1`.\n\n### High-Performance Kernels\n\nMini-SGLang integrates state-of-the-art attention kernels to ensure top performance. It leverages [FlashAttention-3](https://github.com/Dao-AILab/flash-attention) for prefill kernel and [FlashInfer](https://github.com/flashinfer-ai/flashinfer) for decode kernel on NVIDIA Hopper architecture.\n\nFollowing [FlashInfer](https://github.com/flashinfer-ai/flashinfer) and [SGLang](https://github.com/sgl-project/sglang), Mini-SGLang also integrates just-in-time (JIT) compiled kernel for better runtime performance. We adopt [TVM FFI](https://github.com/apache/tvm-ffi) for Python binding, which is much faster than the default PyTorch interface due to its lightweight design.\n\n### Interactive Shell Mode\n\nFor direct interaction and testing, Mini-SGLang includes a simple shell mode. This allows users to chat with LLMs directly from the command line, providing a convenient way to test models and observe their behavior without needing a separate client.\n\n![Shell Example](/images/blog/minisgl/shell.png)\n\n## Performance Benchmark\n\nTo evaluate the performance of Mini-SGLang, we conducted comprehensive experiments covering both offline throughput and online serving latency.\n\n### Offline Inference Throughput\n\nWe evaluated Mini-SGLang's offline throughput against Nano-vLLM on a single NVIDIA H200 GPU. Following the methodology from [Nano-vLLM](https://github.com/GeeeekExplorer/nano-vllm/), we used the [Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B/) model and also tested the larger [Qwen3-14B](https://huggingface.co/Qwen/Qwen3-14B/) model to assess performance at scale. We focused on Qwen3 models due to the current limitations of the Nano-vLLM baseline.\n\nThe throughput results (in tokens per second) are shown below:\n\n![Offline-Benchmark](/images/blog/minisgl/offline.png)\n\nThe results show that Mini-SGLang consistently outperforms Nano-vLLM baseline on both Qwen3 models, thanks to our **overlap scheduling** mechanism that effectively hides CPU overhead.\n\n**Reproducibility**: The offline benchmark script is available at [this link](https://github.com/sgl-project/mini-sglang/blob/main/benchmark/offline/bench.py).\n\n### Online Serving Latency\n\nTo assess real-world serving performance, we benchmarked Mini-SGLang against SGLang using a realistic workload from the [Qwen trace](https://github.com/alibaba-edu/qwen-bailian-usagetraces-anon/blob/main/qwen_traceA_blksz_16.jsonl). We replayed 1,000 requests to a [Qwen3-32B](https://huggingface.co/Qwen/Qwen3-32B) model deployed with 4-way tensor parallelism on 4 H200 GPUs. We measured throughput, 90th percentile (P90) Time To First Token (TTFT), and Time Between Tokens (TBT).\n\n![Online-Benchmark](/images/blog/minisgl/online.png)\n\nThe results demonstrate that Mini-SGLang achieves nearly identical performance to SGLang, confirming that its lightweight design does not compromise on throughput or latency.\n\n**Reproducibility**: Use the following commands to launch each system:\n\n```bash\n# Mini-SGLang\npython -m minisgl --model \"Qwen/Qwen3-32B\" --tp 4 --cache naive \n\n# SGLang\npython3 -m sglang.launch_server --model \"Qwen/Qwen3-32B\" --tp 4 \\\n    --disable-radix --port 1919 --decode-attention flashinfer\n```\n\nThe online benchmark script is available at [this link](https://github.com/sgl-project/mini-sglang/blob/main/benchmark/online/bench_qwen.py).\n\n## Conclusion\n\nMini-SGLang successfully distills the power of a state-of-the-art inference engine into a compact and understandable codebase. By retaining key optimizations like overlap scheduling and high-performance attention kernels, it delivers impressive performance while serving as an invaluable educational tool and a flexible platform for research.\n\nWe invite you to explore the [source code](https://github.com/sgl-project/mini-sglang), run the benchmarks, and see for yourself how Mini-SGLang makes high-performance LLM inference more accessible than ever.\n\n## Acknowledgements\n\n- We would like to thank the SGLang team and community for their generous support and feedback, especially Liangsheng Yin, Lianmin Zheng and many others.\n- We would like to thank [MisakaVan](https://github.com/MisakaVan) for his prominent contribution in testing, documentation, code improvement, and [Yi Pan](https://github.com/Conless) for the initial PyTorch implementation of C++ NCCL communicator.\n- We would like to thank [Wenxin Zheng](https://peterzheng98.github.io/) from SJTU for serving as a teaching assistant for the experimental lab course based on Mini-SGLang during the 2025 summer term, and for his support in course organization and student guidance.\n- We learn a lot from the system design of SGLang, FlashInfer, vLLM and Nano-vLLM, which jointly help make Mini-SGLang a clean yet robust system.\n","slug":"2025-12-17-minisgl"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2025-12-17-minisgl"},"buildId":"lxEPiI5h6ilqv5JE5PHNj","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>