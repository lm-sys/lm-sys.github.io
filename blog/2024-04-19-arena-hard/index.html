<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/styles/atom-one-light.min.css"/><title>From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline | LMSYS Org</title><meta name="title" content="From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline | LMSYS Org"/><meta property="og:title" content="From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline | LMSYS Org"/><meta name="twitter:title" content="From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline | LMSYS Org"/><meta name="description" content="&lt;p&gt;Building an affordable and reliable benchmark for LLM chatbots has become a critical challenge. A high-quality benchmark should 1) robustly separate model..."/><meta property="og:description" content="&lt;p&gt;Building an affordable and reliable benchmark for LLM chatbots has become a critical challenge. A high-quality benchmark should 1) robustly separate model..."/><meta name="twitter:description" content="&lt;p&gt;Building an affordable and reliable benchmark for LLM chatbots has become a critical challenge. A high-quality benchmark should 1) robustly separate model..."/><meta property="og:image" content="https://lmsys.org/images/blog/arena_hard/arena_hard.png"/><meta name="twitter:image" content="https://lmsys.org/images/blog/arena_hard/arena_hard.png"/><meta name="twitter:image:alt" content="The text: LLMSYS Org, Large Model Systems Organization."/><meta property="og:type" content="website"/><meta property="og:url" content="https://lmsys.org/blog/2024-04-19-arena-hard"/><meta name="twitter:url" content="https://lmsys.org/blog/2024-04-19-arena-hard"/><meta name="twitter:card" content="summary_large_image"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="theme-color" content="#1d1d1f"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon.jpeg"/><link rel="icon" href="/favicon.jpeg" type="image/jpg"/><meta name="next-head-count" content="20"/><script src="/mathjax.js" defer=""></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js" defer=""></script><link rel="preload" href="/_next/static/css/664049df48ded145.css" as="style"/><link rel="stylesheet" href="/_next/static/css/664049df48ded145.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-5752944655d749a0.js" defer=""></script><script src="/_next/static/chunks/framework-a87821de553db91d.js" defer=""></script><script src="/_next/static/chunks/main-6a269cfcb9446759.js" defer=""></script><script src="/_next/static/chunks/pages/_app-0bb93d4b49319e30.js" defer=""></script><script src="/_next/static/chunks/286-48ebbaba72d91976.js" defer=""></script><script src="/_next/static/chunks/807-a4eae1dfa8bfbe9f.js" defer=""></script><script src="/_next/static/chunks/138-efd6d72a198878c6.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-c9fe776063d24120.js" defer=""></script><script src="/_next/static/WjcjWncQ9yTR-Zzk1k6tG/_buildManifest.js" defer=""></script><script src="/_next/static/WjcjWncQ9yTR-Zzk1k6tG/_ssgManifest.js" defer=""></script><script src="/_next/static/WjcjWncQ9yTR-Zzk1k6tG/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="w-screen full-container flex-col md:flex-row flex "><div class="md:basis-1/5 "><div class="navbar fixed w-full flex md:flex-col px-4 md:px-6 py-2 md:py-6 md:pb-7 z-30 bg-sky text-paper md:h-full items-center justify-between md:static md:w-auto md:bg-sky md:text-paper md:max-h-screen md:justify-between child:pl-2 child:md:pl-0 child:text-lg "><div><p class="text-4xl md:text-7xl cursor-pointer font-bold pl-0 md:pb-3">LMSYS ORG</p><div class="md:flex child:pl-3 md:text-xl child:md:pl-1 child:md:pt-2 hidden md:flex-col child:brightness-100 child:transition"><a href="/projects/">Projects</a><a href="/blog/">Blog</a><a href="/about/">About</a><a href="/donations/">Donations</a><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></div></div><div class="child:mr-3 -ml-0.5 child:w-8 child:brightness-100 child:transition hidden md:flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://slack.sglang.ai" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div><div class="md:hidden"><div><div class="bm-overlay" style="position:fixed;z-index:1000;width:100%;height:100%;background:rgba(0, 0, 0, 0.3);opacity:0;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:opacity 0.3s, transform 0s 0.3s;top:0px;left:0px"></div><div><div class="bm-burger-button" style="z-index:1000;position:fixed;width:1.2em;height:1.0em;right:1.2rem;top:1em"><button type="button" id="react-burger-menu-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer">Open Menu</button><span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:0%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:40%;opacity:1;background:#fff"></span><span class="bm-burger-bars" style="position:absolute;height:20%;left:0;right:0;top:80%;opacity:1;background:#fff"></span></span></div></div><div id="" class="bm-menu-wrap" style="position:fixed;right:0;z-index:1100;width:300px;height:100%;-moz-transform:translate3d(100%, 0, 0);-ms-transform:translate3d(100%, 0, 0);-o-transform:translate3d(100%, 0, 0);-webkit-transform:translate3d(100%, 0, 0);transform:translate3d(100%, 0, 0);transition:all 0.5s;top:0px" aria-hidden="true"><div class="bm-menu" style="height:100%;box-sizing:border-box;overflow:auto;background:#1d1d1f;padding:2.5em 1.5em 0"><nav class="bm-item-list" style="height:100%;color:#fff;padding:0.8em"><div class="bm-item" style="display:inline-block" tabindex="-1"><div class="child:pb-2 child:child:text-2xl"><p><a href="/projects/">Projects</a></p><p><a href="/blog/">Blog</a></p><p><a href="/about/">About</a></p><p><a href="/donations/">Donations</a></p><p><a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">Chatbot Arena (graduated)</a></p></div><div class="child:mr-3 pt-4 child:w-8 child:brightness-100 hover:child:brightness-90 child:transition flex"><a href="mailto:lmsys.org@gmail.com" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://discord.gg/HSWAKCrnFx" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a><a href="https://github.com/lm-sys" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://twitter.com/lmsysorg" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="https://lmsys.org/rss.xml" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a></div></div></nav></div><div><div class="bm-cross-button" style="position:absolute;width:24px;height:24px;right:8px;top:8px"><button type="button" id="react-burger-cross-btn" style="position:absolute;left:0;top:0;z-index:1;width:100%;height:100%;margin:0;padding:0;border:none;font-size:0;background:transparent;cursor:pointer" tabindex="-1">Close Menu</button><span style="position:absolute;top:6px;right:14px"><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(45deg);background:#fff"></span><span class="bm-cross" style="position:absolute;width:3px;height:14px;transform:rotate(-45deg);background:#fff"></span></span></div></div></div></div></div></div></div><div id="content" class="md:overflow-y-scroll md:max-h-screen md:z-50 md:shadow-lg shadow-neutral-600/70 text-sky grow md:grow-0 md:basis-4/5 flex child:grow flex-col "><div class="" style="opacity:0"><div class="w-full flex justify-center py-5 pt-16 md:pt-5"><div class="container px-5" lang="en"><h1 lang="en" style="hyphens:auto" class="text-4xl md:text-4xl w-full font-bold break-words">From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline</h1><p class="text-xl pt-2 pb-2">by: <!-- -->Tianle Li*, Wei-Lin Chiang*, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, Ion Stoica<!-- -->,<!-- --> <!-- -->Apr 19, 2024<!-- --></p><hr/><div class="pt-2 article"><p>Building an affordable and reliable benchmark for LLM chatbots has become a critical challenge. A high-quality benchmark should 1) robustly separate model capability, 2) reflect human preference in real-world use cases, and 3) frequently update to avoid over-fitting or test set leakage.</p>
<p>Traditional benchmarks are often static or close-ended (e.g., MMLU multi-choice QA), which do not satisfy the above requirements. On the other hand, models are evolving faster than ever, underscoring the need to build benchmarks with high separability.</p>
<p>We introduce Arena-Hard – a data pipeline to build high-quality benchmarks from live data in <a href="https://arxiv.org/abs/2403.04132">Chatbot Arena</a>, which is a crowd-sourced platform for LLM evals. To measure its quality, we propose two key metrics:</p>
<ol>
<li>Agreement to Human preference: whether the benchmark score has high agreement to human preference.</li>
<li>Separability: whether the benchmark can confidently separate models.</li>
</ol>
<p>We compare our new benchmark, Arena Hard Auto v0.1, to a current leading chat LLM benchmark, MT Bench. In Figure 1, we show Arena Hard Auto v0.1 offers significantly stronger separability against MT Bench with tighter confidence intervals. It also has a higher agreement (89.1%, see Table 1) with the human preference ranking by Chatbot Arena (english-only). We expect to see this benchmark useful for model developers to differentiate their model checkpoints.</p>
<style>
.tg  {border-collapse:collapse;border-spacing:0;margin:0px auto;}
.tg td{border-color:#ccc;border-style:solid;border-width:1px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-head{background-color:#c0c0c0;border-color:#ccc;text-align:left;vertical-align:top;}
.tg .tg-body{text-align:left;vertical-align:top;}

table {
  border-collapse: collapse;
  width: 100%;
}
</style>
<style>
th {text-align: left}
td {text-align: left}

table {
  border-collapse: collapse;
  width: 100%;
}


th {
  cursor: pointer;
}

th:hover {
  background-color: #ddd;
}

.arrow {
  display: inline-block;
  width: 0;
  height: 0;
  vertical-align: middle;
  margin-left: 5px;
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
}

.arrow-up {
  border-bottom: 5px solid #000;
}

.arrow-down {
  border-top: 5px solid #000;
}

/* Initially sort arrow for descending order */
th:nth-child(1) .arrow-down {
  border-top: 5px solid #000;
}

ul {
    list-style-type: disc !important; /* or 'circle' or 'square', depending on the bullet style you want */
    padding-left: 20px;
}

ul ul {
    list-style-type: circle !important; /* for nested lists, to distinguish from the parent list */
}

li::before {
    content: normal !important; /* This will remove any content added before the list item */
}
</style>
<style>
  iframe {
    display: block;
    width: 100%;
    height: 950px;
    border: none;
    overflow: hidden;
  }
</style>
<p><img src="/images/blog/arena_hard/arena-hard-vs-mt_bench.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%"></img></p>
<p style="color:gray; text-align: left;">Figure 1: Comparison between MT-bench and Arena Hard Auto v0.1. The latter offers significantly better separability between models and tighter confidence intervals. GPT-4-0314 has no variance in Arena-hard-Auto-v0.1 because it's used as the anchor model.</p>
<p>Links:</p>
<ul>
<li>Evaluate your model on Arena-Hard-Auto-v0.1: <a href="https://github.com/lm-sys/arena-hard-auto">Link</a></li>
<li>Browse Arena-Hard-Auto-v0.1 prompts: <a href="https://huggingface.co/spaces/lmsys/arena-hard-browser">Link</a></li>
<li>Statistic Notebook Google Colab: <a href="https://colab.research.google.com/drive/1ar6XLWREN_dXEh404WNOxroFVUe_4njp?usp=sharing">Link</a></li>
<li>Full leaderboard at the Result section: <a href="#full-leaderboard-with-gpt-4-turbo-as-judge">Skip</a></li>
</ul>
<p>We explain more technical details in the following sections.</p>
<h2><a id="key-objectives-of-llm-benchmarks" class="anchor" href="#key-objectives-of-llm-benchmarks" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Key Objectives of LLM benchmarks</h2>
<p>We outline a few key properties that an LLM chatbot benchmark should possess to provide a meaningful measurement of capabilities between models:</p>
<ol>
<li>Agreement to human preference: It should correlate with human preference in real-world use cases</li>
<li>Separability: It should provide confidence interval on benchmark score and separate models with high confidence</li>
<li>Freshness: It should use new, unseen prompts to avoid potential test leakage</li>
</ol>
<p>We define <strong>agreement</strong> of Benchmark A with respect to a reference Benchmark B by the below formulation:</p>
<p>For a given model pair (which B can separate with confidence)</p>
  <ul>
      <li>If A can confidently separate the 2 given models</li>
      <ul>
          <li>+1.0 if the rank order agrees with B.</li>
          <li>-1.0 if the rank order disagrees with B.</li>
      </ul>
      <li>+0.0 if A cannot separate the 2 given models with confidence</li>
  </ul>
<p>An agreement score of 1 implies benchmark A confidently agrees on the preference of every single unique models pair. On the other hand, an agreement score of -1 implies benchmark B confidently disagrees on the preference of every single unique models pair instead.</p>
<p>We define <strong>separability</strong> by whether a benchmark can separate given model pairs with derived confidence intervals (via bootstrapping). This metric can also serve to measure the variances in ranking outputs provided by a benchmark. We quantify this metric by the percentage of model pairs which have non-overlapping confidence intervals of the benchmark scores.</p>
<p>We use a set of top-20 models* on <a href="https://lmarena.ai/?leaderboard">Chatbot Arena</a> (April 13, 2024) that are presented on <a href="https://tatsu-lab.github.io/alpaca_eval/">AlpacaEval leaderboard</a> to calculate separability and agreement per benchmark. We consider the human preference ranking by Chatbot Arena (English only) as the reference to calculate agreement.</p>
<p>In Table 1, Arena-hard-Auto-v0.1 shows the highest separability (87.4%) against widely adopted LLM benchmarks and offers highest agreement (89.1%) to Chatbot Arena. It is also cheap and fast to run ($25).</p>
<p>Interestingly, we find Spearman Correlation, a popular metric for measuring correlations between rankings, may be an unreliable metric for ranking correlation as it does not consider variance of the rankings, and therefore fails to adequately punish essential ranking granularities of the top models we care about most. For example, when considering 95% CI, MT-bench’s agreement to Chatbot Arena drops from 91.3% to 22.6%.</p>
<p>You can find full statistics in the result section.</p>
<p style="color:gray; text-align: center;">Table 1. Separability and agreement per benchmark.</p>
<table class="tg" style="justify-content: center;">
  <colgroup>
    <col style="width: 20%;">
    <col style="width: 20%;">
    <col style="width: 20%;">
    <col style="width: 20%;"> <!-- narrower -->
    <col style="width: 20%;"> <!-- wider -->
  </colgroup>
  <tbody>
    <tr>
      <th class="tg-head"><span style="font-weight:bold;"></span></th>
      <th class="tg-head"><span style="font-weight:bold;">Chatbot Arena<br>(English-only)</span></th>
      <th class="tg-head"><span style="font-weight:bold;">MT-bench</span></th>
      <th class="tg-head"><span style="font-weight:bold;">AlpacaEval 2.0 LC<br>(Length Controlled)</span></th>
      <th class="tg-head"><span style="font-weight:bold;">Arena-Hard-Auto-v0.1</span></th>
    </tr>
    <tr>
      <td class="tg-body">Avg #prompts per model eval</td>
      <td class="tg-body">10,000+</td>
      <td class="tg-body">160</td>
      <td class="tg-body">800</td>
      <td class="tg-body">1,000</td>
    </tr>
    <tr>
      <td class="tg-body"><b>Agreement to Chatbot Arena with 95% CI</b></td>
      <td class="tg-body">N/A</td>
      <td class="tg-body" style="color:red">26.1%</td>
      <td class="tg-body">81.2%</td>
      <td class="tg-body" style="color:green"><b>89.1%</b></td>
    </tr>
    <tr>
      <td class="tg-body">Spearman Correlation</td>
      <td class="tg-body">N/A</td>
      <td class="tg-body">91.3%</td>
      <td class="tg-body">90.8%</td>
      <td class="tg-body" style="color:green"><b>94.1%</b></td>
    </tr>
    <tr>
      <td class="tg-body"><b>Separability with 95% CI</b></td>
      <td class="tg-body">85.8%</td>
      <td class="tg-body" style="color:red">22.6%</td>
      <td class="tg-body">83.2%</td>
      <td class="tg-body" style="color:green"><b>87.4%</b></td>
    </tr>
    <tr>
      <td class="tg-body">Real-world</td>
      <td class="tg-body">Yes</td>
      <td class="tg-body">Mixed</td>
      <td class="tg-body">Mixed</td>
      <td class="tg-body" style="color:green"><b>Yes</b></td>
    </tr>
    <tr>
      <td class="tg-body">Freshness</td>
      <td class="tg-body">Live</td>
      <td class="tg-body">Static</td>
      <td class="tg-body">Static</td>
      <td class="tg-body" style="color:green"><b>Frequent Updates</b></td>
    </tr>
    <tr>
      <td class="tg-body">Eval cost per model</td>
      <td class="tg-body">Very High</td>
      <td class="tg-body">$10</td>
      <td class="tg-body">$10</td>
      <td class="tg-body">$25</td>
    </tr>
    <tr>
      <td class="tg-body">Judge</td>
      <td class="tg-body">Human</td>
      <td class="tg-body">LLM</td>
      <td class="tg-body">LLM</td>
      <td class="tg-body">LLM</td>
    </tr>
</tbody>
</table>
<details close style="text-align: left; font-family: monospace; font-size: 15px;">
<summary>*Results based on 20 top models from Chatbot Arena that are also presented on Alpaca Eval</summary>
gpt-4-turbo-2024-04-09, claude-3-opus-20240229, claude-3-sonnet-20240229, gpt-4-0314, gpt-4-0613, mistral-large-2402, qwen1.5-72b-chat, mistral-medium, claude-2.0, gpt-3.5-turbo-0613, claude-2.1, gemini-pro, mixtral-8x7b-instruct-v0.1, gpt-3.5-turbo-0314, yi-34b-chat, tulu-2-dpo-70b, dbrx-instruct-preview, vicuna-33b, starling-lm-7b-alpha, llama-2-70b-chat
</details>
<p>Next, we elaborate how to build the prompt selection pipeline to ensure data quality.</p>
<h2><a id="arena-hard-pipeline" class="anchor" href="#arena-hard-pipeline" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Arena-Hard Pipeline</h2>
<p>We build a pipeline that automatically extracts quality prompts from a dataset of 200,000 user queries collected via Chatbot Arena. This process involves ensuring:</p>
<ul>
<li>Diversity: Prompt set should cover a wide range of real-world topics</li>
<li>Prompt quality: Each prompt should possess high quality to benchmark LLMs. we define several key criteria below (see Table 2)</li>
</ul>
<p><img src="/images/blog/arena_hard/method.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%"></img></p>
<p style="color:gray; text-align: center;">Figure 2: Arena-Hard Pipeline</p>
<p>To ensure prompt diversity, we adopt a topic modeling pipeline in <a href="https://github.com/MaartenGr/BERTopic">BERTopic</a> by first converting each prompt with OpenAI’s embedding (text-embedding-3-small), reducing dimension with UMAP, and using a hierarchical-based clustering algorithm (HDBSCAN) to identify clusters which are then summarized using GPT-4-turbo. This helps us identify over 4000 topics covering a wide range of domains. However, topic clusters come with varying quality and separability in benchmarking LLMs. We then develop a calibrated system prompt for LLMs to help us select high quality user queries by seven key criteria (e.g., specificity, domain knowledge, problem-solving, etc).</p>
<table style="width:100%; border-collapse: collapse; border: 1px solid black;">
  <tr style="background-color: black; color: white;">
    <th style="border: 1px solid black; padding: 10px; text-align: left;">Table 2: 7 Key Criteria</th>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>1. Specificity:</strong> Does the prompt ask for a specific output?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>2. Domain Knowledge:</strong> Does the prompt cover one or more specific domains?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>3. Complexity:</strong> Does the prompt have multiple levels of reasoning, components, or variables?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>4. Problem-Solving:</strong> Does the prompt directly involve the AI to demonstrate active problem-solving skills?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>5. Creativity:</strong> Does the prompt involve a level of creativity in approaching the problem?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>6. Technical Accuracy:</strong> Does the prompt require technical accuracy in the response?</td>
  </tr>
  <tr>
    <td style="border: 1px solid black; padding: 10px; text-align: left;"><strong>7. Real-world Application:</strong> Does the prompt relate to real-world applications?</td>
  </tr>
</table>
<p>An LLM Judge (GPT-3.5-Turbo, GPT-4-Turbo) annotates each prompt from 0 to 7 to indicate how many criteria are met. We then score each cluster by the average score of its prompts. Below, we show examples of topic clusters ranging from low to high mean scores. We can observe clusters with higher scores often correlate to challenging topics or tasks for LLMs like game development or mathematical proofs. On the other hand, clusters with lower scores point to trivial or ambiguous questions like &quot;Design Styles and Influences&quot;.</p>
<p><img src="/images/blog/arena_hard/cluster_distribution.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%"></img></p>
<p style="color:gray; text-align: center;">Figure 3: Chatbot Arena clusters sorted by their scores.</p>
<p>To see whether the prompt score correlates with separability, we sample 50 prompts per score and compare the responses from GPT-4 and Llama-70b, with GPT-4-Turbo as judge. We observe a strong correlation between high potential score and the win-rate of GPT-4 over Llama-70b. A similar trend is also observed in other model pairs such as Claude Sonnet vs Haiku and Mistral-large vs Mixtral.</p>
<p><img src="/images/blog/arena_hard/hard_score_line.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%"></img></p>
<p style="color:gray; text-align: center;">Figure 4: Win-rate between model pairs becomes more separable as the "7 Key Criteria" score increases.</p>
<h2><a id="results" class="anchor" href="#results" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Results</h2>
<h3><a id="arena-hard-auto-v01" class="anchor" href="#arena-hard-auto-v01" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Arena-Hard-Auto-v0.1</h3>
<p>Using the above pipeline, we identify 250 high-quality topic clusters with mean score &gt;=6 out of 7. We then randomly sample 2 prompts per cluster to construct 500 high-quality benchmark prompts, Arena-Hard-Auto-v0.1. This benchmark set contains mostly well-defined, technical problem-solving queries as required in the above key criteria. You can browse all the prompts at this <a href="https://huggingface.co/spaces/lmsys/arena-hard-browser">link</a>.</p>
<p>However, evaluating models on challenging queries such as Arena-Hard-Auto-v0.1 is a non-trivial task. Most queries involve deep domain knowledge and problem solving skills, requiring expert-level judgment to evaluate the answer quality. Unfortunately, this is prohibitively expensive and time consuming. Following <a href="https://arxiv.org/abs/2306.05685">LLM-as-a-Judge</a> and <a href="https://arxiv.org/abs/2305.14387">AlpacaFarm</a>, we employ LLM as a judge framework to approximate human preference.</p>
<p>We consider the pairwise comparison setup against a strong baseline model (GPT-4-0314), and ask a strong judge model (e.g., GPT-4-Turbo or Claude-3-Opus) to categorize the preference into five labels: A &gt;&gt; B, A &gt; B, A~=B, .. B&gt;&gt;A. This way, a model will be penalized more in big losses than small losses, which we find to be effective in separating models. We also employ CoT to prompt the LLM judge to generate answers first before giving judgments. Full judge prompt can be found <a href="https://github.com/lm-sys/arena-hard-auto/blob/main/config/judge_config.yaml">here</a>.</p>
<p>To avoid potential position bias, we adopt a two-game setup – per query we swap the models on the first &amp; second position. This results in 500x2=1000 judgments per model evaluation. Following Chatbot Arena, we adopt the Bradley-Terry model to produce model’s the final model scores. By bootstrapping the comparisons from all models, we find it to be statistically stable compared to only considering win-rate against the baseline model.</p>
<h3><a id="full-leaderboard-with-gpt-4-turbo-as-judge" class="anchor" href="#full-leaderboard-with-gpt-4-turbo-as-judge" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Full Leaderboard with GPT-4-Turbo as judge</h3>
<p>We use gpt-4-1106-preview as the judge model to generate judgment for the model response against baseline. We take all the comparisons and compute each model’s Bradley-Terry coefficient. We then transform it to win-rate against the baseline as the final score. The 95% confidence interval is computed via 100 rounds of bootstrapping.</p>
<p style="color:gray; text-align: center;">Arena Hard Auto v0.1 Leaderboard (baseline: GPT-4-0314)</p>
<div style="display: flex; justify-content: center; font-family: Consolas, monospace;">
<table style="line-height: 1; font-size: 1.0em;">
  <caption style="text-align: left; color: red">*Note: GPT-4-Turbo’s high score can be due to the GPT-4 judge favoring GPT-4 outputs.</caption>
  <thead>
    <tr style="border-bottom: thin solid #ccc;">
      <th style="width: 40%;">Model Name</th>
      <th style="width: 20%;">Score</th>
      <th style="width: 20%;">95% CI</th>
      <th style="width: 20%;">Average #Tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left;">gpt-4-turbo-2024-04-09*</td>
      <td>82.6</td>
      <td>-1.8/+1.6</td>
      <td>662</td>
    </tr>
    <tr>
      <td style="text-align: left;">gpt-4-0125-preview*</td>
      <td>78.0</td>
      <td>-2.2/+2.4</td>
      <td>619</td>
    </tr>
    <tr>
      <td style="text-align: left;">claude-3-opus-20240229</td>
      <td>60.4</td>
      <td>-3.3/+2.4</td>
      <td>541</td>
    </tr>
    <tr>
      <td style="text-align: left;">gpt-4-0314</td>
      <td>50.0</td>
      <td>-0.0/+0.0</td>
      <td>423</td>
    </tr>
    <tr>
  <td style="text-align: left;">claude-3-sonnet-20240229</td>
  <td>46.8</td>
  <td>-2.1/+2.2</td>
  <td>552</td>
</tr>
<tr>
  <td style="text-align: left;">claude-3-haiku-20240307</td>
  <td>41.5</td>
  <td>-2.8/+2.5</td>
  <td>505</td>
</tr>
<tr>
  <td style="text-align: left;">llama-3-70b-instruct</td>
  <td>41.1</td>
  <td>-2.5/+2.4</td>
  <td>583</td>
</tr>
<tr>
  <td style="text-align: left;">gpt-4-0613</td>
  <td>37.9</td>
  <td>-2.2/+2.0</td>
  <td>354</td>
</tr>
<tr>
  <td style="text-align: left;">mistral-large-2402</td>
  <td>37.7</td>
  <td>-1.9/+2.6</td>
  <td>400</td>
</tr>
<tr>
  <td style="text-align: left;">mixtral-8x22b-instruct-v0.1</td>
  <td>36.4</td>
  <td>-2.7/+2.9</td>
  <td>430</td>
</tr>
<tr>
  <td style="text-align: left;">Qwen1.5-72B-Chat</td>
  <td>36.1</td>
  <td>-2.5/+2.2</td>
  <td>474</td>
</tr>
<tr>
  <td style="text-align: left;">command-r-plus</td>
  <td>33.1</td>
  <td>-2.1/+2.2</td>
  <td>541</td>
</tr>
<tr>
  <td style="text-align: left;">mistral-medium</td>
  <td>31.9</td>
  <td>-2.3/+2.4</td>
  <td>485</td>
</tr>
<tr>
  <td style="text-align: left;">mistral-next</td>
  <td>27.4</td>
  <td>-2.1/+1.7</td>
  <td>297</td>
</tr>
<tr>
  <td style="text-align: left;">gpt-3.5-turbo-0613</td>
  <td>24.8</td>
  <td>-1.6/+2.0</td>
  <td>401</td>
</tr>
<tr>
  <td style="text-align: left;">claude-2.0</td>
  <td>24.0</td>
  <td>-2.5/+2.5</td>
  <td>295</td>
</tr>
<tr>
  <td style="text-align: left;">dbrx-instruct</td>
  <td>23.9</td>
  <td>-1.4/+1.5</td>
  <td>415</td>
</tr>
<tr>
  <td style="text-align: left;">Mixtral-8x7B-Instruct-v0.1</td>
  <td>23.4</td>
  <td>-2.3/+1.7</td>
  <td>457</td>
</tr>
<tr>
  <td style="text-align: left;">gpt-3.5-turbo-0125</td>
  <td>23.3</td>
  <td>-2.2/+2.3</td>
  <td>329</td>
</tr>
<tr>
  <td style="text-align: left;">Yi-34B-Chat</td>
  <td>23.1</td>
  <td>-1.8/+2.0</td>
  <td>611</td>
</tr>
<tr>
  <td style="text-align: left;">Starling-LM-7B-beta</td>
  <td>23.0</td>
  <td>-1.9/+2.2</td>
  <td>530</td>
</tr>
<tr>
  <td style="text-align: left;">claude-2.1</td>
  <td>22.8</td>
  <td>-1.6/+2.1</td>
  <td>290</td>
</tr>
<tr>
  <td style="text-align: left;">Snorkel-Mistral-PairRM-DPO</td>
  <td>20.7</td>
  <td>-2.2/+1.5</td>
  <td>564</td>
</tr>
<tr>
  <td style="text-align: left;">llama-3-8b-instruct</td>
  <td>20.6</td>
  <td>-2.5/+1.8</td>
  <td>585</td>
</tr>
<tr>
  <td style="text-align: left;">gpt-3.5-turbo-1106</td>
  <td>18.9</td>
  <td>-1.6/+2.1</td>
  <td>285</td>
</tr>
<tr>
  <td style="text-align: left;">gpt-3.5-turbo-0301</td>
  <td>18.1</td>
  <td>-1.7/+1.2</td>
  <td>334</td>
</tr>
<tr>
  <td style="text-align: left;">gemini-1.0-pro</td>
  <td>17.8</td>
  <td>-1.7/+1.7</td>
  <td>322</td>
</tr>
<tr>
  <td style="text-align: left;">command-r</td>
  <td>17.0</td>
  <td>-1.9/+1.7</td>
  <td>432</td>
</tr>
<tr>
  <td style="text-align: left;">tulu-2-dpo-70b</td>
  <td>15.0</td>
  <td>-1.4/+1.2</td>
  <td>550</td>
</tr>
<tr>
  <td style="text-align: left;">Starling-LM-7B-alpha</td>
  <td>12.8</td>
  <td>-1.4/+1.4</td>
  <td>483</td>
</tr>
<tr>
  <td style="text-align: left;">mistral-7b-instruct-v0.2</td>
  <td>12.6</td>
  <td>-1.6/+1.3</td>
  <td>541</td>
</tr>
<tr>
  <td style="text-align: left;">Llama-2-70b-chat-hf</td>
  <td>11.6</td>
  <td>-1.6/+1.4</td>
  <td>595</td>
</tr>
<tr>
  <td style="text-align: left;">vicuna-33b-v1.3</td>
  <td>8.6</td>
  <td>-1.3/+1.0</td>
  <td>451</td>
</tr>
<tr>
  <td style="text-align: left;">gemma-7b-it</td>
  <td>7.5</td>
  <td>-1.1/+1.2</td>
  <td>378</td>
</tr>
<tr>
  <td style="text-align: left;">Llama-2-7b-chat-hf</td>
  <td>4.6</td>
  <td>-0.8/+0.8</td>
  <td>561</td>
</tr>
<tr>
  <td style="text-align: left;">gemma-2b-it</td>
  <td>3.0</td>
  <td>-0.6/+0.7</td>
  <td>369</td>
</tr>
</tbody>
</table>
</div>
<h3><a id="gpt-4-turbo-or-claude-as-judge" class="anchor" href="#gpt-4-turbo-or-claude-as-judge" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>GPT-4-Turbo or Claude as Judge?</h3>
<p>We also compare two strongest LLMs: GPT-4-1106-Preview and Claude-3 Opus as the judge mode in Table 3. When GPT-4 Judge is used, we observe higher separability across models (ranging from 23.0 to 78.0). When Claude Judge is used, we find the Claude family of models scores in general go up, despite it still favoring gpt-4-0125-preview over itself. Surprisingly, it favors several open models (Mixtral, Yi, Starling) or even gpt-3.5-turbo over gpt-4-0613.</p>
<p style="color:gray; text-align: center;">Table 3. Leaderboard Comparison Between GPT and Claude as Judge</p>
<div style="display: flex; justify-content: center; font-family: Consolas, monospace;">
<table style="line-height: 1; font-size: 1.0em;">
  <thead>
    <tr style="border-bottom: thin solid #ccc;">
      <th style="width: 30%;">Model Name</th>
      <th style="width: 25%;">GPT-4-1106-Preview Judge</th>
      <th style="width: 25%;">Claude-3-Opus<br>Judge</th>
      <th style="width: 20%;">Diff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left;">gpt-4-0125-preview</td>
      <td>78.0</td>
      <td>76.3 <span style="color: red;">(↓)</span></td>
      <td style="color: red;">-1.7</td>
    </tr>
    <tr>
      <td style="text-align: left;">claude-3-opus-20240229</td>
      <td>60.4</td>
      <td>71.8 <span style="color: green;">(↑)</span></td>
      <td style="color: green;">+11.4</td>
    </tr>
    <tr>
      <td style="text-align: left;">claude-3-sonnet-20240229</td>
      <td>46.8</td>
      <td>63.6 <span style="color: green;">(↑)</span></td>
      <td style="color: green;">+16.8</td>
    </tr>
    <tr>
      <td style="text-align: left;">claude-3-haiku-20240307</td>
      <td>41.5</td>
      <td>56.1 <span style="color: green;">(↑)</span></td>
      <td style="color: green;">+14.6</td>
    </tr>
    <tr>
      <td style="text-align: left;">gpt-4-0613</td>
      <td>37.9</td>
      <td>30.6 <span style="color: red;">(↓)</span></td>
      <td style="color: red;">-7.3</td>
    </tr>
    <tr>
      <td style="text-align: left;">gpt-3.5-0613</td>
      <td>24.8</td>
      <td>34.7 <span style="color: green;">(↑)</span></td>
      <td style="color: green;">+9.9</td>
    </tr>
    <tr>
      <td style="text-align: left;">mixtral-8x22b-instruct-v0.1</td>
      <td>23.4</td>
      <td>34.8 <span style="color: green;">(↑)</span></td>
      <td style="color: green;">+11.4</td>
    </tr>
    <tr>
      <td style="text-align: left;">yi-34b-chat</td>
      <td>23.1</td>
      <td>46.6 <span style="color: green;">(↑)</span></td>
      <td style="color: green;">+23.5</td>
    </tr>
    <tr>
      <td style="text-align: left;">starling-lm-7b-beta</td>
      <td>23.0</td>
      <td>45.0 <span style="color: green;">(↑)</span></td>
      <td style="color: green;">+22</td>
    </tr>
  </tbody>
</table>
</div>
<p>We further compare GPT-4 and Claude Judges using our proposed metrics of separability and agreement in Table 4, and find that the GPT-4-turbo Judge is significantly better across all metrics.</p>
<table style="border-collapse: collapse; border: 1px solid black">
  <caption>Table 4: Statistical comparisons between LLM Judges and Human</caption>
  <tr>
    <td style="border: 1px solid black"></td>
    <td style="border: 1px solid black">Arena-Hard-Auto-v0.1 (GPT-4-1106-Preview Judge)</td>
    <td style="border: 1px solid black">Arena-Hard-Auto-v0.1 (Claude-3 Judge)</td>
  </tr>
  <tr>
    <td style="border: 1px solid black">Agreement to Chatbot Arena with 95% CI</td>
    <td style="border: 1px solid black"><b>89.1%</b></td>
    <td style="border: 1px solid black">66.7%</td>
  </tr>
  <tr>
    <td style="border: 1px solid black">Separability with 95% confidence intervals</td>
    <td style="border: 1px solid black"><b>87.4%</b></td>
    <td style="border: 1px solid black">83.7%</td>
  </tr>
  <tr>
    <td style="border: 1px solid black">Spearman Correlation</td>
    <td style="border: 1px solid black"><b>94.2%</b></td>
    <td style="border: 1px solid black">77.0%</td>
  </tr>
    <tr>
    <td style="border: 1px solid black">Brier Score*</td>
    <td style="border: 1px solid black"><b>0.07</b></td>
    <td style="border: 1px solid black">0.17</td>
  </tr>
</table>
<caption>*Brier Score (lower is better), a statistical scoring function for measuring the accuracy of probabilistic accuracy. (see section View Benchmarking as a Forecasting Problem for more information)</caption>
<p>We manually compared different judgment examples between GPT-4-Turbo and Claude as a judge. We found that when the two judges disagreed, it could usually be broken down into two main categories:</p>
<ol>
<li>Conservative scoring</li>
<li>Differing perspectives on the user's prompt</li>
</ol>
<p>We find that Claude-3-Opus is much less likely to give harsh scores – it is particularly hesitant to proclaim one response as &quot;significantly better&quot; than another. In contrast, GPT-4-Turbo will identify errors in a model's response that led to an incorrect answer and penalize the model with a significantly lower score. On the other hand, Claude-3-Opus sometimes overlooks smaller errors. Even when Claude-3-Opus does identify these errors, it tends to treat them as minor issues and shows leniency during scoring. This effect is particularly present in coding and math problems, where small mistakes are more likely to completely derail the final answer; these scorings are still given leniency from Claude-3-Opus but not GPT-4-Turbo. See the appendix below for specific examples of differing judgments, many of which exhibit this phenomenon.</p>
<p><img src="/images/blog/arena_hard/score_strength.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%"></img></p>
<p style="color:gray; text-align: center;">Figure 5: Score Strength</p>
<p>There is also a small subset of prompts in which Claude-3-Opus and GPT-4-Turbo judge with fundamentally different perspectives. For example, given a coding question, Claude-3-Opus may choose the response that provides the most educational value to the user, offering a simplistic structure without relying on external libraries. GPT-4-Turbo, however, may prioritize the response that provides the most practical answer, regardless of its educational value to the user.  While both interpretations are valid judging criteria, we find GPT-4-Turbo’s perspective may be more correlated with the average user.</p>
<p>Despite the observed differences between Claude-3-Opus and GPT-4-Turbo judgment styles, we find the judges have an overall soft agreement rate of 80%. Two judgments “soft agree” if they are at most distance one apart, or in other words they do not contradict.</p>
<h2><a id="limitations" class="anchor" href="#limitations" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Limitations</h2>
<h3><a id="verbosity-does-the-llm-judge-prefer-longer-responses" class="anchor" href="#verbosity-does-the-llm-judge-prefer-longer-responses" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Verbosity: does the LLM Judge prefer longer responses?</h3>
<p>LLM as judges are known to suffer from verbosity bias (<a href="https://arxiv.org/abs/2404.04475">Length-Controlled AlpacaEval</a>). Below we plot the avg token length and score per model for both MT-Bench and Arena-Hard-Auto-v0.1. Visually, there isn't a strong correlation between score and length.</p>
<p><img src="/images/blog/arena_hard/verbose_scatterplot.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 60%"></img></p>
<p style="color:gray; text-align: center;">Figure 6: Verbosity scatterplot comparing Arena-Hard-Auto-v0.1 and MT Bench.</p>
<p>To further examine potential verbosity bias, we conduct an ablation on three different system prompts (original, chatty, detailed) with GPT-3.5-Turbo. We observe that both GPT-4-Turbo and Claude-3-Opus judges may be affected by longer outputs, while Claude being significantly more impacted with a “more detailed” system prompt as GPT-3.5-Turbo reaches a win-rate of over 40% against GPT-4-0314.</p>
<p>Interestingly, the “chatty” system prompt doesn’t affect much on the win-rate by both judges, despite the longer average #tokens. This suggests output length is not the only factor. It is possible that more detailed answers are also more helpful and thus preferred by LLM judges.</p>
<p style="color:gray; text-align: center;">Table 5. Length Bias Comparison Between GPT and Claude as Judge</p>
<div style="display: flex; justify-content: center; font-family: Consolas, monospace;">
<table style="line-height: 1; font-size: 1.0em;">
  <thead>
    <tr style="border-bottom: thin solid #ccc;">
      <th style="width: 40%;">Model Name</th>
      <th style="width: 30%;">Win Rate</th>
      <th style="width: 30%;">Average Token #</th>
    </tr>
  </thead>
  <tbody>
    <tr style="border: 1px solid black;">
      <td style="text-align: left;"><b>GPT-4-1106-Preview</b></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td style="text-align: left;">gpt-3.5-turbo-0125-detailed</td>
      <td>29.86</td>
      <td>421</td>
    </tr>
<tr>
  <td style="text-align: left;">gpt-3.5-turbo-0125-chatty</td>
  <td>23.89</td>
  <td>361</td>
</tr>
<tr>
  <td style="text-align: left;">gpt-3.5-turbo-0125</td>
  <td>23.2</td>
  <td>328</td>
</tr>
<tr style="border: 1px solid black;">
  <td style="text-align: left;"></td>
  <td></td>
  <td></td>
</tr>
<tr style="border: 1px solid black;">
  <td style="text-align: left;"><b>Claude-3-Opus</b></td>
  <td></td>
  <td></td>
</tr>
<tr>
  <td style="text-align: left;">gpt-3.5-turbo-0125-detailed</td>
  <td>40.78</td>
  <td>421</td>
</tr>
<tr>
  <td style="text-align: left;">gpt-3.5-turbo-0125-chatty</td>
  <td>28.49</td>
  <td>375</td>
</tr>
<tr>
  <td style="text-align: left;">gpt-3.5-turbo-0125</td>
  <td>27.97</td>
  <td>328</td>
</tr>
</tbody>
</table>
</div>
<caption style="font-family: Consolas, monospace; font-size: 15px;">
System Prompt:<br>detailed: “You are a helpful assistant who thoroughly explains things with as much detail as possible.”<br>chatty: “You are a helpful assistant who is chatty.”
</caption>
<h3><a id="variance-in-gpt-4-judgments" class="anchor" href="#variance-in-gpt-4-judgments" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Variance in GPT-4 judgments</h3>
<p>We find that even with temperature=0, GPT-4-Turbo may still generate slightly different judgments. Here we repeat the judgments for gpt-3.5-turbo-0125 three times and report its variance. Due to limited budget, we can only evaluate all the models once. We recommend using the confidence intervals to determine model separation.</p>
<p style="color:gray; text-align: center;">Table 6. Variances between 3 separate runs of Arena Hard Auto v0.1.</p>
<div style="display: flex; justify-content: center; font-family: Consolas, monospace;">
<table style="line-height: 1; font-size: 1.0em;">
  <thead>
    <tr style="border-bottom: thin solid #ccc;">
      <th style="width: 40%;">Model Name</th>
      <th style="width: 30%;">Win Rate</th>
      <th style="width: 30%;">Average Token #</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left;">gpt-3.5-turbo-0125-1</td>
      <td>23.05</td>
      <td>328</td>
    </tr>
    <tr>
      <td style="text-align: left;">gpt-3.5-turbo-0125-2</td>
      <td>22.93</td>
      <td>328</td>
    </tr>
        <tr>
      <td style="text-align: left;">gpt-3.5-turbo-0125-3</td>
      <td>22.75</td>
      <td>328</td>
    </tr>
</tbody>
</table>
</div>
<h3><a id="potential-self-bias--prompt-selection-bias" class="anchor" href="#potential-self-bias--prompt-selection-bias" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Potential self-bias &amp; prompt selection bias</h3>
<p>We also observe potential self-bias in LLM judges (e.g., Claude Judge prefers Claude answers).
In addition, the prompt selection process could be biased by the LLMs. The benchmark also does not evaluate multi-turn interactions.</p>
<h2><a id="viewing-benchmarking-as-a-forecasting-problem" class="anchor" href="#viewing-benchmarking-as-a-forecasting-problem" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Viewing Benchmarking as a Forecasting Problem</h2>
<p>In this section we attempt to combine both confidence and correlation into one standardized metric for benchmarking.</p>
<table style="border-collapse: collapse; border: 1px solid black">
  <caption>Correlation of Brier Score with Overall Chatbot Arena Score Across Different Models</caption>
  <tr>
    <td style="border: 1px solid black">Arena Hard Auto v0.1</td>
    <td style="border: 1px solid black">Chabot Arena* (20K Votes)</td>
    <td style="border: 1px solid black">MT Bench</td>
    <td style="border: 1px solid black">Alpaca 2.0 LC</td>
  </tr>
  <tr>
    <td style="border: 1px solid black"><b>0.07</b></td>
    <td style="border: 1px solid black">0.08</td>
    <td style="border: 1px solid black">0.09</td>
    <td style="border: 1px solid black">0.11</td>
  </tr>
</table>
<caption>*20K human preference battles randomly sampled from Chatbot Arena between the 20 top models.</caption>
<p>Model developers generally use benchmarks for model selection, not ground truth certification of performance.  Benchmarks serve as a cheap and lightweight proxy for more expensive and complex evaluations like ground truth Bradley Terry Coefficients derived from human preference. Thus, we expect benchmarks to tell us, as model developers, some confidence bound on what a model’s real world performance will be. In this sense, a benchmark serves as a forecast for true long-run performance.</p>
<p>Forecasting is a delicate balance between confidence and uncertainty. Therefore, a good benchmark should show confidence when separating clearly unequal models, but should demonstrate uncertainty when ranking differences between legitimately similar models. One might argue we only need to look at how confident a given benchmark is at separating model pairs. A good benchmark is not necessarily always confident at separating models– you don’t want your benchmark to be confidently incorrect. For example, given a pair of models A and B and benchmark 1 and 2. Let’s assume ground truth is model A is better than model B. We bootstrap both benchmark 1 and 2 and retrieve their confidence intervals for both model’s performances. Benchmark 1 confidently predicts model B is better than A while Benchmark 2 predicts model B is better than A with low confidence. In this case, we should say Benchmark 2 is actually better than Benchmark 1 at predicting this pair of models. This is to say, high confidence should be rewarded only when the answer is correct, and low confidence is better when incorrect.</p>
<p>In this problem context, we introduce the prediction criteria as simply the binary indicator <strong>1</strong>$(\pi_a &lt; \pi_b)$ for some model pair ($\pi_a$ and $\pi_b$).  The forecast gives a probability that this indicator is true, $P(\pi_a &lt; \pi_b)$.  A higher probability forecast indicates greater confidence that <strong>1</strong>$(\pi_a &lt; \pi_b)$ will be true.  We can generate these probability predictions using bootstrapped score mean and variance, which in turn define a gaussian distribution. We then resolve the ground truth label for <strong>1</strong>$(\pi_a &lt; \pi_b)$ using Chatbot Arena's Bradley Terry coefficients.</p>
<p>A well-defined fair-in-expectation loss for forecasting is <a href="https://en.wikipedia.org/wiki/Brier_score">Brier Score</a>. Brier score rewards confidence when forecasts are correct while punishing confident errors. We can calculate the loss over a benchmark prediction of <strong>1</strong>$(\pi_a &lt; \pi_b)$ for each model pair with respect to the Chatbot Area ground truth scores to quantify a benchmark’s forecasting performance. Here we assume Chatbot Arena as “ground truth” as both Alpaca 2.0 LC and Arena Hard Auto are advertised as an inexpensive alternative to Chatbot Arena as an evaluation pipeline. We will conduct future study on correlation comparison where we instead use Chatbot Arena's Bradley Terry coefficient derived from similar distributions as the given benchmark.</p>
<p>We find that Arena Hard Auto averages much lower forecasting loss, demonstrating that it is both accurate in score, and accurate in confidence level.</p>
<div style="display: flex; gap: 10px;">
  <div style="width: 48%;">
    <img src="/images/blog/arena_hard/forecast_arena_20k.png">
  </div>
  <div style="width: 48%;">
    <img src="/images/blog/arena_hard/forecast_arena_hard.png">
  </div>
</div>
<div style="display: flex; gap: 10px;">
  <div style="width: 48%;">
    <img src="/images/blog/arena_hard/forecast_alpaca.png">
  </div>
  <div style="width: 48%;">
    <img src="/images/blog/arena_hard/forecast_mt_bench.png">
  </div>
</div>
<p>Above is the predicted model predicted probability against the bootstrapped arena “ground truth” probability (jittered to show clusters).  While both Alpaca eval and Arena Hard Auto have large clusters around (0,0) and (1,1) signifying good forecasting, Arena Hard Auto has lighter clusters on (0,1) and (1,0), if any, revealing less overconfidence. MT Bench has heavy tails along the top and bottom, revealing underconfidence. However, none of these benchmarks show an “ideal” y=x curve (with dense ends) expected with a perfectly calibrated forecast, signifying room for future research.</p>
<h2><a id="future" class="anchor" href="#future" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Future</h2>
<p>We hope to study deeper into the above limitations and biases in the later technical report. We are also working on diving deeper into the statistics for more studies on how to measure the quality of benchmarks. Lastly, we also hope to upgrade Arena-Hard frequently. So expect frequent new benchmarks!</p>
<h2><a id="acknowledgment" class="anchor" href="#acknowledgment" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgment</h2>
<p>We thank Matei Zaharia, Yann Dubois, Anastasios Angelopoulos, Lianmin Zheng, Lewis Tunstall, Nathan Lambert, Xuechen Li, Naman Jain, Ying Sheng, Maarten Grootendorst for their valuable feedback. We thank Siyuan Zhuang and Dacheng Li for the valuable review and debug of the code. We thank Microsoft <a href="https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/">AFMR</a> for Azure OpenAI credits support. We also thank Together.ai &amp; Anyscale for open model endpoint support.</p>
<h2><a id="citation" class="anchor" href="#citation" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Citation</h2>
<p>If you find Arena-Hard-Auto or BenchBuilder useful, please cite our papers below.</p>
<pre><code class="hljs"><span class="hljs-comment">@misc{li2024crowdsourced,</span>
      title={From Crowdsourced <span class="hljs-meta">Data</span> to High-Quality Benchmarks: Arena-Hard <span class="hljs-keyword">and</span> BenchBuilder Pipeline}, 
      author={Tianle Li <span class="hljs-keyword">and</span> Wei-Lin Chiang <span class="hljs-keyword">and</span> Evan Frick <span class="hljs-keyword">and</span> Lisa Dunlap <span class="hljs-keyword">and</span> Tianhao Wu <span class="hljs-keyword">and</span> Banghua Zhu <span class="hljs-keyword">and</span> Joseph E. Gonzalez <span class="hljs-keyword">and</span> Ion Stoica},
      year={<span class="hljs-number">2024</span>},
      eprint={<span class="hljs-number">2406</span>.<span class="hljs-number">11939</span>},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

<span class="hljs-comment">@misc{chiang2024chatbot,</span>
    title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},
    author={Wei-Lin Chiang <span class="hljs-keyword">and</span> Lianmin Zheng <span class="hljs-keyword">and</span> Ying Sheng <span class="hljs-keyword">and</span> Anastasios Nikolas Angelopoulos <span class="hljs-keyword">and</span> Tianle Li <span class="hljs-keyword">and</span> Dacheng Li <span class="hljs-keyword">and</span> Hao Zhang <span class="hljs-keyword">and</span> Banghua Zhu <span class="hljs-keyword">and</span> Michael Jordan <span class="hljs-keyword">and</span> Joseph E. Gonzalez <span class="hljs-keyword">and</span> Ion Stoica},
    year={<span class="hljs-number">2024</span>},
    eprint={<span class="hljs-number">2403</span>.<span class="hljs-number">04132</span>},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

<span class="hljs-comment">@misc{arenahard2024,</span>
    title = {From Live <span class="hljs-meta">Data</span> to High-Quality Benchmarks: The Arena-Hard Pipeline},
    url = {https:<span class="hljs-comment">//lmsys.org/blog/2024-04-19-arena-hard/},</span>
    author = {Tianle Li*, Wei-Lin Chiang*, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, Ion Stoica},
    month = {April},
    year = {<span class="hljs-number">2024</span>}
}
</code></pre>
<h2><a id="appendix" class="anchor" href="#appendix" aria-hidden="true"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Appendix</h2>
<p><img src="/images/blog/arena_hard/heatmap.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 120%"></img></p>
<p style="color:gray; text-align: center;">Appendix Figure 1: Similarity Heatmap of 50 Arena Hard Auto v0.1 Clusters</p>
<p><img src="/images/blog/arena_hard/clustering_filtered_small_64.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 120%"></img></p>
<p style="color:gray; text-align: center;">Appendix Figure 2: Top-64 clusters visualized in hierarchy. x-axis represents the cosine similarity distance. y-axis shows the topic title per cluster summarized by gpt-4-turbo.</p></div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline","author":"Tianle Li*, Wei-Lin Chiang*, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, Ion Stoica","date":"April 19, 2024","previewImg":"/images/blog/arena_hard/arena_hard.png"},"content":"\nBuilding an affordable and reliable benchmark for LLM chatbots has become a critical challenge. A high-quality benchmark should 1) robustly separate model capability, 2) reflect human preference in real-world use cases, and 3) frequently update to avoid over-fitting or test set leakage.\n\nTraditional benchmarks are often static or close-ended (e.g., MMLU multi-choice QA), which do not satisfy the above requirements. On the other hand, models are evolving faster than ever, underscoring the need to build benchmarks with high separability.\n\nWe introduce Arena-Hard – a data pipeline to build high-quality benchmarks from live data in [Chatbot Arena](https://arxiv.org/abs/2403.04132), which is a crowd-sourced platform for LLM evals. To measure its quality, we propose two key metrics:\n1. Agreement to Human preference: whether the benchmark score has high agreement to human preference.\n2. Separability: whether the benchmark can confidently separate models.\n\nWe compare our new benchmark, Arena Hard Auto v0.1, to a current leading chat LLM benchmark, MT Bench. In Figure 1, we show Arena Hard Auto v0.1 offers significantly stronger separability against MT Bench with tighter confidence intervals. It also has a higher agreement (89.1%, see Table 1) with the human preference ranking by Chatbot Arena (english-only). We expect to see this benchmark useful for model developers to differentiate their model checkpoints.\n\n\u003cstyle\u003e\n.tg  {border-collapse:collapse;border-spacing:0;margin:0px auto;}\n.tg td{border-color:#ccc;border-style:solid;border-width:1px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-head{background-color:#c0c0c0;border-color:#ccc;text-align:left;vertical-align:top;}\n.tg .tg-body{text-align:left;vertical-align:top;}\n\ntable {\n  border-collapse: collapse;\n  width: 100%;\n}\n\u003c/style\u003e\n\n\u003cstyle\u003e\nth {text-align: left}\ntd {text-align: left}\n\ntable {\n  border-collapse: collapse;\n  width: 100%;\n}\n\n\nth {\n  cursor: pointer;\n}\n\nth:hover {\n  background-color: #ddd;\n}\n\n.arrow {\n  display: inline-block;\n  width: 0;\n  height: 0;\n  vertical-align: middle;\n  margin-left: 5px;\n  border-left: 5px solid transparent;\n  border-right: 5px solid transparent;\n}\n\n.arrow-up {\n  border-bottom: 5px solid #000;\n}\n\n.arrow-down {\n  border-top: 5px solid #000;\n}\n\n/* Initially sort arrow for descending order */\nth:nth-child(1) .arrow-down {\n  border-top: 5px solid #000;\n}\n\nul {\n    list-style-type: disc !important; /* or 'circle' or 'square', depending on the bullet style you want */\n    padding-left: 20px;\n}\n\nul ul {\n    list-style-type: circle !important; /* for nested lists, to distinguish from the parent list */\n}\n\nli::before {\n    content: normal !important; /* This will remove any content added before the list item */\n}\n\u003c/style\u003e\n\n\u003cstyle\u003e\n  iframe {\n    display: block;\n    width: 100%;\n    height: 950px;\n    border: none;\n    overflow: hidden;\n  }\n\u003c/style\u003e\n\n\n\u003cimg src=\"/images/blog/arena_hard/arena-hard-vs-mt_bench.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: left;\"\u003eFigure 1: Comparison between MT-bench and Arena Hard Auto v0.1. The latter offers significantly better separability between models and tighter confidence intervals. GPT-4-0314 has no variance in Arena-hard-Auto-v0.1 because it's used as the anchor model.\u003c/p\u003e\n\nLinks:\n- Evaluate your model on Arena-Hard-Auto-v0.1: [Link](https://github.com/lm-sys/arena-hard-auto)\n- Browse Arena-Hard-Auto-v0.1 prompts: [Link](https://huggingface.co/spaces/lmsys/arena-hard-browser)\n- Statistic Notebook Google Colab: [Link](https://colab.research.google.com/drive/1ar6XLWREN_dXEh404WNOxroFVUe_4njp?usp=sharing)\n- Full leaderboard at the Result section: [Skip](#full-leaderboard-with-gpt-4-turbo-as-judge)\n\nWe explain more technical details in the following sections.\n\n## Key Objectives of LLM benchmarks\n\nWe outline a few key properties that an LLM chatbot benchmark should possess to provide a meaningful measurement of capabilities between models:\n1. Agreement to human preference: It should correlate with human preference in real-world use cases\n2. Separability: It should provide confidence interval on benchmark score and separate models with high confidence\n3. Freshness: It should use new, unseen prompts to avoid potential test leakage\n\n\nWe define **agreement** of Benchmark A with respect to a reference Benchmark B by the below formulation:\n\nFor a given model pair (which B can separate with confidence)\n  \u003cul\u003e\n      \u003cli\u003eIf A can confidently separate the 2 given models\u003c/li\u003e\n      \u003cul\u003e\n          \u003cli\u003e+1.0 if the rank order agrees with B.\u003c/li\u003e\n          \u003cli\u003e-1.0 if the rank order disagrees with B.\u003c/li\u003e\n      \u003c/ul\u003e\n      \u003cli\u003e+0.0 if A cannot separate the 2 given models with confidence\u003c/li\u003e\n  \u003c/ul\u003e\n\nAn agreement score of 1 implies benchmark A confidently agrees on the preference of every single unique models pair. On the other hand, an agreement score of -1 implies benchmark B confidently disagrees on the preference of every single unique models pair instead.\n\nWe define **separability** by whether a benchmark can separate given model pairs with derived confidence intervals (via bootstrapping). This metric can also serve to measure the variances in ranking outputs provided by a benchmark. We quantify this metric by the percentage of model pairs which have non-overlapping confidence intervals of the benchmark scores.\n\nWe use a set of top-20 models* on [Chatbot Arena](https://lmarena.ai/?leaderboard) (April 13, 2024) that are presented on [AlpacaEval leaderboard](https://tatsu-lab.github.io/alpaca_eval/) to calculate separability and agreement per benchmark. We consider the human preference ranking by Chatbot Arena (English only) as the reference to calculate agreement.\n\nIn Table 1, Arena-hard-Auto-v0.1 shows the highest separability (87.4%) against widely adopted LLM benchmarks and offers highest agreement (89.1%) to Chatbot Arena. It is also cheap and fast to run ($25).\n\nInterestingly, we find Spearman Correlation, a popular metric for measuring correlations between rankings, may be an unreliable metric for ranking correlation as it does not consider variance of the rankings, and therefore fails to adequately punish essential ranking granularities of the top models we care about most. For example, when considering 95% CI, MT-bench’s agreement to Chatbot Arena drops from 91.3% to 22.6%.\n\nYou can find full statistics in the result section. \n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 1. Separability and agreement per benchmark.\u003c/p\u003e\n\n\u003ctable class=\"tg\" style=\"justify-content: center;\"\u003e\n  \u003ccolgroup\u003e\n    \u003ccol style=\"width: 20%;\"\u003e\n    \u003ccol style=\"width: 20%;\"\u003e\n    \u003ccol style=\"width: 20%;\"\u003e\n    \u003ccol style=\"width: 20%;\"\u003e \u003c!-- narrower --\u003e\n    \u003ccol style=\"width: 20%;\"\u003e \u003c!-- wider --\u003e\n  \u003c/colgroup\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth class=\"tg-head\"\u003e\u003cspan style=\"font-weight:bold;\"\u003e\u003c/span\u003e\u003c/th\u003e\n      \u003cth class=\"tg-head\"\u003e\u003cspan style=\"font-weight:bold;\"\u003eChatbot Arena\u003cbr\u003e(English-only)\u003c/span\u003e\u003c/th\u003e\n      \u003cth class=\"tg-head\"\u003e\u003cspan style=\"font-weight:bold;\"\u003eMT-bench\u003c/span\u003e\u003c/th\u003e\n      \u003cth class=\"tg-head\"\u003e\u003cspan style=\"font-weight:bold;\"\u003eAlpacaEval 2.0 LC\u003cbr\u003e(Length Controlled)\u003c/span\u003e\u003c/th\u003e\n      \u003cth class=\"tg-head\"\u003e\u003cspan style=\"font-weight:bold;\"\u003eArena-Hard-Auto-v0.1\u003c/span\u003e\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd class=\"tg-body\"\u003eAvg #prompts per model eval\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003e10,000+\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003e160\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003e800\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003e1,000\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd class=\"tg-body\"\u003e\u003cb\u003eAgreement to Chatbot Arena with 95% CI\u003c/b\u003e\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003eN/A\u003c/td\u003e\n      \u003ctd class=\"tg-body\" style=\"color:red\"\u003e26.1%\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003e81.2%\u003c/td\u003e\n      \u003ctd class=\"tg-body\" style=\"color:green\"\u003e\u003cb\u003e89.1%\u003c/b\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd class=\"tg-body\"\u003eSpearman Correlation\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003eN/A\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003e91.3%\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003e90.8%\u003c/td\u003e\n      \u003ctd class=\"tg-body\" style=\"color:green\"\u003e\u003cb\u003e94.1%\u003c/b\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd class=\"tg-body\"\u003e\u003cb\u003eSeparability with 95% CI\u003c/b\u003e\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003e85.8%\u003c/td\u003e\n      \u003ctd class=\"tg-body\" style=\"color:red\"\u003e22.6%\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003e83.2%\u003c/td\u003e\n      \u003ctd class=\"tg-body\" style=\"color:green\"\u003e\u003cb\u003e87.4%\u003c/b\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd class=\"tg-body\"\u003eReal-world\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003eYes\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003eMixed\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003eMixed\u003c/td\u003e\n      \u003ctd class=\"tg-body\" style=\"color:green\"\u003e\u003cb\u003eYes\u003c/b\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd class=\"tg-body\"\u003eFreshness\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003eLive\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003eStatic\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003eStatic\u003c/td\u003e\n      \u003ctd class=\"tg-body\" style=\"color:green\"\u003e\u003cb\u003eFrequent Updates\u003c/b\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd class=\"tg-body\"\u003eEval cost per model\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003eVery High\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003e$10\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003e$10\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003e$25\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd class=\"tg-body\"\u003eJudge\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003eHuman\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003eLLM\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003eLLM\u003c/td\u003e\n      \u003ctd class=\"tg-body\"\u003eLLM\u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cdetails close style=\"text-align: left; font-family: monospace; font-size: 15px;\"\u003e\n\u003csummary\u003e*Results based on 20 top models from Chatbot Arena that are also presented on Alpaca Eval\u003c/summary\u003e\ngpt-4-turbo-2024-04-09, claude-3-opus-20240229, claude-3-sonnet-20240229, gpt-4-0314, gpt-4-0613, mistral-large-2402, qwen1.5-72b-chat, mistral-medium, claude-2.0, gpt-3.5-turbo-0613, claude-2.1, gemini-pro, mixtral-8x7b-instruct-v0.1, gpt-3.5-turbo-0314, yi-34b-chat, tulu-2-dpo-70b, dbrx-instruct-preview, vicuna-33b, starling-lm-7b-alpha, llama-2-70b-chat\n\u003c/details\u003e\n\nNext, we elaborate how to build the prompt selection pipeline to ensure data quality.\n\n## Arena-Hard Pipeline\n\nWe build a pipeline that automatically extracts quality prompts from a dataset of 200,000 user queries collected via Chatbot Arena. This process involves ensuring:\n- Diversity: Prompt set should cover a wide range of real-world topics\n- Prompt quality: Each prompt should possess high quality to benchmark LLMs. we define several key criteria below (see Table 2)\n\n\u003cimg src=\"/images/blog/arena_hard/method.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 2: Arena-Hard Pipeline\u003c/p\u003e\n\nTo ensure prompt diversity, we adopt a topic modeling pipeline in [BERTopic](https://github.com/MaartenGr/BERTopic) by first converting each prompt with OpenAI’s embedding (text-embedding-3-small), reducing dimension with UMAP, and using a hierarchical-based clustering algorithm (HDBSCAN) to identify clusters which are then summarized using GPT-4-turbo. This helps us identify over 4000 topics covering a wide range of domains. However, topic clusters come with varying quality and separability in benchmarking LLMs. We then develop a calibrated system prompt for LLMs to help us select high quality user queries by seven key criteria (e.g., specificity, domain knowledge, problem-solving, etc).\n\n\u003ctable style=\"width:100%; border-collapse: collapse; border: 1px solid black;\"\u003e\n  \u003ctr style=\"background-color: black; color: white;\"\u003e\n    \u003cth style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003eTable 2: 7 Key Criteria\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003e\u003cstrong\u003e1. Specificity:\u003c/strong\u003e Does the prompt ask for a specific output?\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003e\u003cstrong\u003e2. Domain Knowledge:\u003c/strong\u003e Does the prompt cover one or more specific domains?\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003e\u003cstrong\u003e3. Complexity:\u003c/strong\u003e Does the prompt have multiple levels of reasoning, components, or variables?\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003e\u003cstrong\u003e4. Problem-Solving:\u003c/strong\u003e Does the prompt directly involve the AI to demonstrate active problem-solving skills?\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003e\u003cstrong\u003e5. Creativity:\u003c/strong\u003e Does the prompt involve a level of creativity in approaching the problem?\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003e\u003cstrong\u003e6. Technical Accuracy:\u003c/strong\u003e Does the prompt require technical accuracy in the response?\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black; padding: 10px; text-align: left;\"\u003e\u003cstrong\u003e7. Real-world Application:\u003c/strong\u003e Does the prompt relate to real-world applications?\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n\nAn LLM Judge (GPT-3.5-Turbo, GPT-4-Turbo) annotates each prompt from 0 to 7 to indicate how many criteria are met. We then score each cluster by the average score of its prompts. Below, we show examples of topic clusters ranging from low to high mean scores. We can observe clusters with higher scores often correlate to challenging topics or tasks for LLMs like game development or mathematical proofs. On the other hand, clusters with lower scores point to trivial or ambiguous questions like \"Design Styles and Influences\".\n\n\u003cimg src=\"/images/blog/arena_hard/cluster_distribution.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 3: Chatbot Arena clusters sorted by their scores.\u003c/p\u003e\n\nTo see whether the prompt score correlates with separability, we sample 50 prompts per score and compare the responses from GPT-4 and Llama-70b, with GPT-4-Turbo as judge. We observe a strong correlation between high potential score and the win-rate of GPT-4 over Llama-70b. A similar trend is also observed in other model pairs such as Claude Sonnet vs Haiku and Mistral-large vs Mixtral.\n\n\n\n\u003cimg src=\"/images/blog/arena_hard/hard_score_line.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 4: Win-rate between model pairs becomes more separable as the \"7 Key Criteria\" score increases.\u003c/p\u003e\n\n## Results\n\n### Arena-Hard-Auto-v0.1\n\nUsing the above pipeline, we identify 250 high-quality topic clusters with mean score \u003e=6 out of 7. We then randomly sample 2 prompts per cluster to construct 500 high-quality benchmark prompts, Arena-Hard-Auto-v0.1. This benchmark set contains mostly well-defined, technical problem-solving queries as required in the above key criteria. You can browse all the prompts at this [link](https://huggingface.co/spaces/lmsys/arena-hard-browser).\n\nHowever, evaluating models on challenging queries such as Arena-Hard-Auto-v0.1 is a non-trivial task. Most queries involve deep domain knowledge and problem solving skills, requiring expert-level judgment to evaluate the answer quality. Unfortunately, this is prohibitively expensive and time consuming. Following [LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) and [AlpacaFarm](https://arxiv.org/abs/2305.14387), we employ LLM as a judge framework to approximate human preference.\n\nWe consider the pairwise comparison setup against a strong baseline model (GPT-4-0314), and ask a strong judge model (e.g., GPT-4-Turbo or Claude-3-Opus) to categorize the preference into five labels: A \u003e\u003e B, A \u003e B, A~=B, .. B\u003e\u003eA. This way, a model will be penalized more in big losses than small losses, which we find to be effective in separating models. We also employ CoT to prompt the LLM judge to generate answers first before giving judgments. Full judge prompt can be found [here](https://github.com/lm-sys/arena-hard-auto/blob/main/config/judge_config.yaml).\n\nTo avoid potential position bias, we adopt a two-game setup – per query we swap the models on the first \u0026 second position. This results in 500x2=1000 judgments per model evaluation. Following Chatbot Arena, we adopt the Bradley-Terry model to produce model’s the final model scores. By bootstrapping the comparisons from all models, we find it to be statistically stable compared to only considering win-rate against the baseline model.\n\n### Full Leaderboard with GPT-4-Turbo as judge\n\nWe use gpt-4-1106-preview as the judge model to generate judgment for the model response against baseline. We take all the comparisons and compute each model’s Bradley-Terry coefficient. We then transform it to win-rate against the baseline as the final score. The 95% confidence interval is computed via 100 rounds of bootstrapping.\n\n\u003cp style=\"color:gray; text-align: center;\"\u003eArena Hard Auto v0.1 Leaderboard (baseline: GPT-4-0314)\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: center; font-family: Consolas, monospace;\"\u003e\n\u003ctable style=\"line-height: 1; font-size: 1.0em;\"\u003e\n  \u003ccaption style=\"text-align: left; color: red\"\u003e*Note: GPT-4-Turbo’s high score can be due to the GPT-4 judge favoring GPT-4 outputs.\u003c/caption\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"border-bottom: thin solid #ccc;\"\u003e\n      \u003cth style=\"width: 40%;\"\u003eModel Name\u003c/th\u003e\n      \u003cth style=\"width: 20%;\"\u003eScore\u003c/th\u003e\n      \u003cth style=\"width: 20%;\"\u003e95% CI\u003c/th\u003e\n      \u003cth style=\"width: 20%;\"\u003eAverage #Tokens\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003egpt-4-turbo-2024-04-09*\u003c/td\u003e\n      \u003ctd\u003e82.6\u003c/td\u003e\n      \u003ctd\u003e-1.8/+1.6\u003c/td\u003e\n      \u003ctd\u003e662\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003egpt-4-0125-preview*\u003c/td\u003e\n      \u003ctd\u003e78.0\u003c/td\u003e\n      \u003ctd\u003e-2.2/+2.4\u003c/td\u003e\n      \u003ctd\u003e619\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003eclaude-3-opus-20240229\u003c/td\u003e\n      \u003ctd\u003e60.4\u003c/td\u003e\n      \u003ctd\u003e-3.3/+2.4\u003c/td\u003e\n      \u003ctd\u003e541\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003egpt-4-0314\u003c/td\u003e\n      \u003ctd\u003e50.0\u003c/td\u003e\n      \u003ctd\u003e-0.0/+0.0\u003c/td\u003e\n      \u003ctd\u003e423\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003eclaude-3-sonnet-20240229\u003c/td\u003e\n  \u003ctd\u003e46.8\u003c/td\u003e\n  \u003ctd\u003e-2.1/+2.2\u003c/td\u003e\n  \u003ctd\u003e552\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003eclaude-3-haiku-20240307\u003c/td\u003e\n  \u003ctd\u003e41.5\u003c/td\u003e\n  \u003ctd\u003e-2.8/+2.5\u003c/td\u003e\n  \u003ctd\u003e505\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003ellama-3-70b-instruct\u003c/td\u003e\n  \u003ctd\u003e41.1\u003c/td\u003e\n  \u003ctd\u003e-2.5/+2.4\u003c/td\u003e\n  \u003ctd\u003e583\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003egpt-4-0613\u003c/td\u003e\n  \u003ctd\u003e37.9\u003c/td\u003e\n  \u003ctd\u003e-2.2/+2.0\u003c/td\u003e\n  \u003ctd\u003e354\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003emistral-large-2402\u003c/td\u003e\n  \u003ctd\u003e37.7\u003c/td\u003e\n  \u003ctd\u003e-1.9/+2.6\u003c/td\u003e\n  \u003ctd\u003e400\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003emixtral-8x22b-instruct-v0.1\u003c/td\u003e\n  \u003ctd\u003e36.4\u003c/td\u003e\n  \u003ctd\u003e-2.7/+2.9\u003c/td\u003e\n  \u003ctd\u003e430\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003eQwen1.5-72B-Chat\u003c/td\u003e\n  \u003ctd\u003e36.1\u003c/td\u003e\n  \u003ctd\u003e-2.5/+2.2\u003c/td\u003e\n  \u003ctd\u003e474\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003ecommand-r-plus\u003c/td\u003e\n  \u003ctd\u003e33.1\u003c/td\u003e\n  \u003ctd\u003e-2.1/+2.2\u003c/td\u003e\n  \u003ctd\u003e541\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003emistral-medium\u003c/td\u003e\n  \u003ctd\u003e31.9\u003c/td\u003e\n  \u003ctd\u003e-2.3/+2.4\u003c/td\u003e\n  \u003ctd\u003e485\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003emistral-next\u003c/td\u003e\n  \u003ctd\u003e27.4\u003c/td\u003e\n  \u003ctd\u003e-2.1/+1.7\u003c/td\u003e\n  \u003ctd\u003e297\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003egpt-3.5-turbo-0613\u003c/td\u003e\n  \u003ctd\u003e24.8\u003c/td\u003e\n  \u003ctd\u003e-1.6/+2.0\u003c/td\u003e\n  \u003ctd\u003e401\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003eclaude-2.0\u003c/td\u003e\n  \u003ctd\u003e24.0\u003c/td\u003e\n  \u003ctd\u003e-2.5/+2.5\u003c/td\u003e\n  \u003ctd\u003e295\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003edbrx-instruct\u003c/td\u003e\n  \u003ctd\u003e23.9\u003c/td\u003e\n  \u003ctd\u003e-1.4/+1.5\u003c/td\u003e\n  \u003ctd\u003e415\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003eMixtral-8x7B-Instruct-v0.1\u003c/td\u003e\n  \u003ctd\u003e23.4\u003c/td\u003e\n  \u003ctd\u003e-2.3/+1.7\u003c/td\u003e\n  \u003ctd\u003e457\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003egpt-3.5-turbo-0125\u003c/td\u003e\n  \u003ctd\u003e23.3\u003c/td\u003e\n  \u003ctd\u003e-2.2/+2.3\u003c/td\u003e\n  \u003ctd\u003e329\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003eYi-34B-Chat\u003c/td\u003e\n  \u003ctd\u003e23.1\u003c/td\u003e\n  \u003ctd\u003e-1.8/+2.0\u003c/td\u003e\n  \u003ctd\u003e611\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003eStarling-LM-7B-beta\u003c/td\u003e\n  \u003ctd\u003e23.0\u003c/td\u003e\n  \u003ctd\u003e-1.9/+2.2\u003c/td\u003e\n  \u003ctd\u003e530\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003eclaude-2.1\u003c/td\u003e\n  \u003ctd\u003e22.8\u003c/td\u003e\n  \u003ctd\u003e-1.6/+2.1\u003c/td\u003e\n  \u003ctd\u003e290\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003eSnorkel-Mistral-PairRM-DPO\u003c/td\u003e\n  \u003ctd\u003e20.7\u003c/td\u003e\n  \u003ctd\u003e-2.2/+1.5\u003c/td\u003e\n  \u003ctd\u003e564\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003ellama-3-8b-instruct\u003c/td\u003e\n  \u003ctd\u003e20.6\u003c/td\u003e\n  \u003ctd\u003e-2.5/+1.8\u003c/td\u003e\n  \u003ctd\u003e585\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003egpt-3.5-turbo-1106\u003c/td\u003e\n  \u003ctd\u003e18.9\u003c/td\u003e\n  \u003ctd\u003e-1.6/+2.1\u003c/td\u003e\n  \u003ctd\u003e285\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003egpt-3.5-turbo-0301\u003c/td\u003e\n  \u003ctd\u003e18.1\u003c/td\u003e\n  \u003ctd\u003e-1.7/+1.2\u003c/td\u003e\n  \u003ctd\u003e334\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003egemini-1.0-pro\u003c/td\u003e\n  \u003ctd\u003e17.8\u003c/td\u003e\n  \u003ctd\u003e-1.7/+1.7\u003c/td\u003e\n  \u003ctd\u003e322\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003ecommand-r\u003c/td\u003e\n  \u003ctd\u003e17.0\u003c/td\u003e\n  \u003ctd\u003e-1.9/+1.7\u003c/td\u003e\n  \u003ctd\u003e432\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003etulu-2-dpo-70b\u003c/td\u003e\n  \u003ctd\u003e15.0\u003c/td\u003e\n  \u003ctd\u003e-1.4/+1.2\u003c/td\u003e\n  \u003ctd\u003e550\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003eStarling-LM-7B-alpha\u003c/td\u003e\n  \u003ctd\u003e12.8\u003c/td\u003e\n  \u003ctd\u003e-1.4/+1.4\u003c/td\u003e\n  \u003ctd\u003e483\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003emistral-7b-instruct-v0.2\u003c/td\u003e\n  \u003ctd\u003e12.6\u003c/td\u003e\n  \u003ctd\u003e-1.6/+1.3\u003c/td\u003e\n  \u003ctd\u003e541\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003eLlama-2-70b-chat-hf\u003c/td\u003e\n  \u003ctd\u003e11.6\u003c/td\u003e\n  \u003ctd\u003e-1.6/+1.4\u003c/td\u003e\n  \u003ctd\u003e595\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003evicuna-33b-v1.3\u003c/td\u003e\n  \u003ctd\u003e8.6\u003c/td\u003e\n  \u003ctd\u003e-1.3/+1.0\u003c/td\u003e\n  \u003ctd\u003e451\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003egemma-7b-it\u003c/td\u003e\n  \u003ctd\u003e7.5\u003c/td\u003e\n  \u003ctd\u003e-1.1/+1.2\u003c/td\u003e\n  \u003ctd\u003e378\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003eLlama-2-7b-chat-hf\u003c/td\u003e\n  \u003ctd\u003e4.6\u003c/td\u003e\n  \u003ctd\u003e-0.8/+0.8\u003c/td\u003e\n  \u003ctd\u003e561\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003egemma-2b-it\u003c/td\u003e\n  \u003ctd\u003e3.0\u003c/td\u003e\n  \u003ctd\u003e-0.6/+0.7\u003c/td\u003e\n  \u003ctd\u003e369\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\n### GPT-4-Turbo or Claude as Judge?\n\nWe also compare two strongest LLMs: GPT-4-1106-Preview and Claude-3 Opus as the judge mode in Table 3. When GPT-4 Judge is used, we observe higher separability across models (ranging from 23.0 to 78.0). When Claude Judge is used, we find the Claude family of models scores in general go up, despite it still favoring gpt-4-0125-preview over itself. Surprisingly, it favors several open models (Mixtral, Yi, Starling) or even gpt-3.5-turbo over gpt-4-0613.\n\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 3. Leaderboard Comparison Between GPT and Claude as Judge\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: center; font-family: Consolas, monospace;\"\u003e\n\u003ctable style=\"line-height: 1; font-size: 1.0em;\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"border-bottom: thin solid #ccc;\"\u003e\n      \u003cth style=\"width: 30%;\"\u003eModel Name\u003c/th\u003e\n      \u003cth style=\"width: 25%;\"\u003eGPT-4-1106-Preview Judge\u003c/th\u003e\n      \u003cth style=\"width: 25%;\"\u003eClaude-3-Opus\u003cbr\u003eJudge\u003c/th\u003e\n      \u003cth style=\"width: 20%;\"\u003eDiff\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003egpt-4-0125-preview\u003c/td\u003e\n      \u003ctd\u003e78.0\u003c/td\u003e\n      \u003ctd\u003e76.3 \u003cspan style=\"color: red;\"\u003e(↓)\u003c/span\u003e\u003c/td\u003e\n      \u003ctd style=\"color: red;\"\u003e-1.7\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003eclaude-3-opus-20240229\u003c/td\u003e\n      \u003ctd\u003e60.4\u003c/td\u003e\n      \u003ctd\u003e71.8 \u003cspan style=\"color: green;\"\u003e(↑)\u003c/span\u003e\u003c/td\u003e\n      \u003ctd style=\"color: green;\"\u003e+11.4\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003eclaude-3-sonnet-20240229\u003c/td\u003e\n      \u003ctd\u003e46.8\u003c/td\u003e\n      \u003ctd\u003e63.6 \u003cspan style=\"color: green;\"\u003e(↑)\u003c/span\u003e\u003c/td\u003e\n      \u003ctd style=\"color: green;\"\u003e+16.8\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003eclaude-3-haiku-20240307\u003c/td\u003e\n      \u003ctd\u003e41.5\u003c/td\u003e\n      \u003ctd\u003e56.1 \u003cspan style=\"color: green;\"\u003e(↑)\u003c/span\u003e\u003c/td\u003e\n      \u003ctd style=\"color: green;\"\u003e+14.6\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003egpt-4-0613\u003c/td\u003e\n      \u003ctd\u003e37.9\u003c/td\u003e\n      \u003ctd\u003e30.6 \u003cspan style=\"color: red;\"\u003e(↓)\u003c/span\u003e\u003c/td\u003e\n      \u003ctd style=\"color: red;\"\u003e-7.3\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003egpt-3.5-0613\u003c/td\u003e\n      \u003ctd\u003e24.8\u003c/td\u003e\n      \u003ctd\u003e34.7 \u003cspan style=\"color: green;\"\u003e(↑)\u003c/span\u003e\u003c/td\u003e\n      \u003ctd style=\"color: green;\"\u003e+9.9\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003emixtral-8x22b-instruct-v0.1\u003c/td\u003e\n      \u003ctd\u003e23.4\u003c/td\u003e\n      \u003ctd\u003e34.8 \u003cspan style=\"color: green;\"\u003e(↑)\u003c/span\u003e\u003c/td\u003e\n      \u003ctd style=\"color: green;\"\u003e+11.4\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003eyi-34b-chat\u003c/td\u003e\n      \u003ctd\u003e23.1\u003c/td\u003e\n      \u003ctd\u003e46.6 \u003cspan style=\"color: green;\"\u003e(↑)\u003c/span\u003e\u003c/td\u003e\n      \u003ctd style=\"color: green;\"\u003e+23.5\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003estarling-lm-7b-beta\u003c/td\u003e\n      \u003ctd\u003e23.0\u003c/td\u003e\n      \u003ctd\u003e45.0 \u003cspan style=\"color: green;\"\u003e(↑)\u003c/span\u003e\u003c/td\u003e\n      \u003ctd style=\"color: green;\"\u003e+22\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\n\nWe further compare GPT-4 and Claude Judges using our proposed metrics of separability and agreement in Table 4, and find that the GPT-4-turbo Judge is significantly better across all metrics. \n\n\u003ctable style=\"border-collapse: collapse; border: 1px solid black\"\u003e\n  \u003ccaption\u003eTable 4: Statistical comparisons between LLM Judges and Human\u003c/caption\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003e\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003eArena-Hard-Auto-v0.1 (GPT-4-1106-Preview Judge)\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003eArena-Hard-Auto-v0.1 (Claude-3 Judge)\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003eAgreement to Chatbot Arena with 95% CI\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003e\u003cb\u003e89.1%\u003c/b\u003e\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003e66.7%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003eSeparability with 95% confidence intervals\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003e\u003cb\u003e87.4%\u003c/b\u003e\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003e83.7%\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003eSpearman Correlation\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003e\u003cb\u003e94.2%\u003c/b\u003e\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003e77.0%\u003c/td\u003e\n  \u003c/tr\u003e\n    \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003eBrier Score*\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003e\u003cb\u003e0.07\u003c/b\u003e\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003e0.17\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\u003ccaption\u003e*Brier Score (lower is better), a statistical scoring function for measuring the accuracy of probabilistic accuracy. (see section View Benchmarking as a Forecasting Problem for more information)\u003c/caption\u003e\n\nWe manually compared different judgment examples between GPT-4-Turbo and Claude as a judge. We found that when the two judges disagreed, it could usually be broken down into two main categories:\n1. Conservative scoring\n2. Differing perspectives on the user's prompt\n\nWe find that Claude-3-Opus is much less likely to give harsh scores – it is particularly hesitant to proclaim one response as \"significantly better\" than another. In contrast, GPT-4-Turbo will identify errors in a model's response that led to an incorrect answer and penalize the model with a significantly lower score. On the other hand, Claude-3-Opus sometimes overlooks smaller errors. Even when Claude-3-Opus does identify these errors, it tends to treat them as minor issues and shows leniency during scoring. This effect is particularly present in coding and math problems, where small mistakes are more likely to completely derail the final answer; these scorings are still given leniency from Claude-3-Opus but not GPT-4-Turbo. See the appendix below for specific examples of differing judgments, many of which exhibit this phenomenon.\n\n\u003cimg src=\"/images/blog/arena_hard/score_strength.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 5: Score Strength\u003c/p\u003e\n\nThere is also a small subset of prompts in which Claude-3-Opus and GPT-4-Turbo judge with fundamentally different perspectives. For example, given a coding question, Claude-3-Opus may choose the response that provides the most educational value to the user, offering a simplistic structure without relying on external libraries. GPT-4-Turbo, however, may prioritize the response that provides the most practical answer, regardless of its educational value to the user.  While both interpretations are valid judging criteria, we find GPT-4-Turbo’s perspective may be more correlated with the average user.\n\nDespite the observed differences between Claude-3-Opus and GPT-4-Turbo judgment styles, we find the judges have an overall soft agreement rate of 80%. Two judgments “soft agree” if they are at most distance one apart, or in other words they do not contradict.\n\n## Limitations\n\n### Verbosity: does the LLM Judge prefer longer responses?\n\nLLM as judges are known to suffer from verbosity bias ([Length-Controlled AlpacaEval](https://arxiv.org/abs/2404.04475)). Below we plot the avg token length and score per model for both MT-Bench and Arena-Hard-Auto-v0.1. Visually, there isn't a strong correlation between score and length.\n\n\u003cimg src=\"/images/blog/arena_hard/verbose_scatterplot.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 60%\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eFigure 6: Verbosity scatterplot comparing Arena-Hard-Auto-v0.1 and MT Bench.\u003c/p\u003e\n\nTo further examine potential verbosity bias, we conduct an ablation on three different system prompts (original, chatty, detailed) with GPT-3.5-Turbo. We observe that both GPT-4-Turbo and Claude-3-Opus judges may be affected by longer outputs, while Claude being significantly more impacted with a “more detailed” system prompt as GPT-3.5-Turbo reaches a win-rate of over 40% against GPT-4-0314. \n\nInterestingly, the “chatty” system prompt doesn’t affect much on the win-rate by both judges, despite the longer average #tokens. This suggests output length is not the only factor. It is possible that more detailed answers are also more helpful and thus preferred by LLM judges.\n\n\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 5. Length Bias Comparison Between GPT and Claude as Judge\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: center; font-family: Consolas, monospace;\"\u003e\n\u003ctable style=\"line-height: 1; font-size: 1.0em;\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"border-bottom: thin solid #ccc;\"\u003e\n      \u003cth style=\"width: 40%;\"\u003eModel Name\u003c/th\u003e\n      \u003cth style=\"width: 30%;\"\u003eWin Rate\u003c/th\u003e\n      \u003cth style=\"width: 30%;\"\u003eAverage Token #\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr style=\"border: 1px solid black;\"\u003e\n      \u003ctd style=\"text-align: left;\"\u003e\u003cb\u003eGPT-4-1106-Preview\u003c/b\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003c/td\u003e\n      \u003ctd\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003egpt-3.5-turbo-0125-detailed\u003c/td\u003e\n      \u003ctd\u003e29.86\u003c/td\u003e\n      \u003ctd\u003e421\u003c/td\u003e\n    \u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003egpt-3.5-turbo-0125-chatty\u003c/td\u003e\n  \u003ctd\u003e23.89\u003c/td\u003e\n  \u003ctd\u003e361\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003egpt-3.5-turbo-0125\u003c/td\u003e\n  \u003ctd\u003e23.2\u003c/td\u003e\n  \u003ctd\u003e328\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"border: 1px solid black;\"\u003e\n  \u003ctd style=\"text-align: left;\"\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"border: 1px solid black;\"\u003e\n  \u003ctd style=\"text-align: left;\"\u003e\u003cb\u003eClaude-3-Opus\u003c/b\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003c/td\u003e\n  \u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003egpt-3.5-turbo-0125-detailed\u003c/td\u003e\n  \u003ctd\u003e40.78\u003c/td\u003e\n  \u003ctd\u003e421\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003egpt-3.5-turbo-0125-chatty\u003c/td\u003e\n  \u003ctd\u003e28.49\u003c/td\u003e\n  \u003ctd\u003e375\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n  \u003ctd style=\"text-align: left;\"\u003egpt-3.5-turbo-0125\u003c/td\u003e\n  \u003ctd\u003e27.97\u003c/td\u003e\n  \u003ctd\u003e328\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\u003ccaption style=\"font-family: Consolas, monospace; font-size: 15px;\"\u003e\nSystem Prompt:\u003cbr\u003edetailed: “You are a helpful assistant who thoroughly explains things with as much detail as possible.”\u003cbr\u003echatty: “You are a helpful assistant who is chatty.”\n\u003c/caption\u003e\n\n### Variance in GPT-4 judgments\n\nWe find that even with temperature=0, GPT-4-Turbo may still generate slightly different judgments. Here we repeat the judgments for gpt-3.5-turbo-0125 three times and report its variance. Due to limited budget, we can only evaluate all the models once. We recommend using the confidence intervals to determine model separation.\n\n\u003cp style=\"color:gray; text-align: center;\"\u003eTable 6. Variances between 3 separate runs of Arena Hard Auto v0.1.\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: center; font-family: Consolas, monospace;\"\u003e\n\u003ctable style=\"line-height: 1; font-size: 1.0em;\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"border-bottom: thin solid #ccc;\"\u003e\n      \u003cth style=\"width: 40%;\"\u003eModel Name\u003c/th\u003e\n      \u003cth style=\"width: 30%;\"\u003eWin Rate\u003c/th\u003e\n      \u003cth style=\"width: 30%;\"\u003eAverage Token #\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003egpt-3.5-turbo-0125-1\u003c/td\u003e\n      \u003ctd\u003e23.05\u003c/td\u003e\n      \u003ctd\u003e328\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003egpt-3.5-turbo-0125-2\u003c/td\u003e\n      \u003ctd\u003e22.93\u003c/td\u003e\n      \u003ctd\u003e328\u003c/td\u003e\n    \u003c/tr\u003e\n        \u003ctr\u003e\n      \u003ctd style=\"text-align: left;\"\u003egpt-3.5-turbo-0125-3\u003c/td\u003e\n      \u003ctd\u003e22.75\u003c/td\u003e\n      \u003ctd\u003e328\u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\n### Potential self-bias \u0026 prompt selection bias\n\nWe also observe potential self-bias in LLM judges (e.g., Claude Judge prefers Claude answers).\nIn addition, the prompt selection process could be biased by the LLMs. The benchmark also does not evaluate multi-turn interactions.\n\n\n## Viewing Benchmarking as a Forecasting Problem\n\nIn this section we attempt to combine both confidence and correlation into one standardized metric for benchmarking.\n\n\u003ctable style=\"border-collapse: collapse; border: 1px solid black\"\u003e\n  \u003ccaption\u003eCorrelation of Brier Score with Overall Chatbot Arena Score Across Different Models\u003c/caption\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003eArena Hard Auto v0.1\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003eChabot Arena* (20K Votes)\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003eMT Bench\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003eAlpaca 2.0 LC\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003e\u003cb\u003e0.07\u003c/b\u003e\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003e0.08\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003e0.09\u003c/td\u003e\n    \u003ctd style=\"border: 1px solid black\"\u003e0.11\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\u003ccaption\u003e*20K human preference battles randomly sampled from Chatbot Arena between the 20 top models.\u003c/caption\u003e\n\nModel developers generally use benchmarks for model selection, not ground truth certification of performance.  Benchmarks serve as a cheap and lightweight proxy for more expensive and complex evaluations like ground truth Bradley Terry Coefficients derived from human preference. Thus, we expect benchmarks to tell us, as model developers, some confidence bound on what a model’s real world performance will be. In this sense, a benchmark serves as a forecast for true long-run performance.\n\nForecasting is a delicate balance between confidence and uncertainty. Therefore, a good benchmark should show confidence when separating clearly unequal models, but should demonstrate uncertainty when ranking differences between legitimately similar models. One might argue we only need to look at how confident a given benchmark is at separating model pairs. A good benchmark is not necessarily always confident at separating models– you don’t want your benchmark to be confidently incorrect. For example, given a pair of models A and B and benchmark 1 and 2. Let’s assume ground truth is model A is better than model B. We bootstrap both benchmark 1 and 2 and retrieve their confidence intervals for both model’s performances. Benchmark 1 confidently predicts model B is better than A while Benchmark 2 predicts model B is better than A with low confidence. In this case, we should say Benchmark 2 is actually better than Benchmark 1 at predicting this pair of models. This is to say, high confidence should be rewarded only when the answer is correct, and low confidence is better when incorrect.\n\nIn this problem context, we introduce the prediction criteria as simply the binary indicator **1**$(\\pi_a \u003c \\pi_b)$ for some model pair ($\\pi_a$ and $\\pi_b$).  The forecast gives a probability that this indicator is true, $P(\\pi_a \u003c \\pi_b)$.  A higher probability forecast indicates greater confidence that **1**$(\\pi_a \u003c \\pi_b)$ will be true.  We can generate these probability predictions using bootstrapped score mean and variance, which in turn define a gaussian distribution. We then resolve the ground truth label for **1**$(\\pi_a \u003c \\pi_b)$ using Chatbot Arena's Bradley Terry coefficients.\n\nA well-defined fair-in-expectation loss for forecasting is [Brier Score](https://en.wikipedia.org/wiki/Brier_score). Brier score rewards confidence when forecasts are correct while punishing confident errors. We can calculate the loss over a benchmark prediction of **1**$(\\pi_a \u003c \\pi_b)$ for each model pair with respect to the Chatbot Area ground truth scores to quantify a benchmark’s forecasting performance. Here we assume Chatbot Arena as “ground truth” as both Alpaca 2.0 LC and Arena Hard Auto are advertised as an inexpensive alternative to Chatbot Arena as an evaluation pipeline. We will conduct future study on correlation comparison where we instead use Chatbot Arena's Bradley Terry coefficient derived from similar distributions as the given benchmark.\n\nWe find that Arena Hard Auto averages much lower forecasting loss, demonstrating that it is both accurate in score, and accurate in confidence level.\n\u003cdiv style=\"display: flex; gap: 10px;\"\u003e\n  \u003cdiv style=\"width: 48%;\"\u003e\n    \u003cimg src=\"/images/blog/arena_hard/forecast_arena_20k.png\"\u003e\n  \u003c/div\u003e\n  \u003cdiv style=\"width: 48%;\"\u003e\n    \u003cimg src=\"/images/blog/arena_hard/forecast_arena_hard.png\"\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv style=\"display: flex; gap: 10px;\"\u003e\n  \u003cdiv style=\"width: 48%;\"\u003e\n    \u003cimg src=\"/images/blog/arena_hard/forecast_alpaca.png\"\u003e\n  \u003c/div\u003e\n  \u003cdiv style=\"width: 48%;\"\u003e\n    \u003cimg src=\"/images/blog/arena_hard/forecast_mt_bench.png\"\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\nAbove is the predicted model predicted probability against the bootstrapped arena “ground truth” probability (jittered to show clusters).  While both Alpaca eval and Arena Hard Auto have large clusters around (0,0) and (1,1) signifying good forecasting, Arena Hard Auto has lighter clusters on (0,1) and (1,0), if any, revealing less overconfidence. MT Bench has heavy tails along the top and bottom, revealing underconfidence. However, none of these benchmarks show an “ideal” y=x curve (with dense ends) expected with a perfectly calibrated forecast, signifying room for future research.\n\n## Future\nWe hope to study deeper into the above limitations and biases in the later technical report. We are also working on diving deeper into the statistics for more studies on how to measure the quality of benchmarks. Lastly, we also hope to upgrade Arena-Hard frequently. So expect frequent new benchmarks! \n\n\n## Acknowledgment\nWe thank Matei Zaharia, Yann Dubois, Anastasios Angelopoulos, Lianmin Zheng, Lewis Tunstall, Nathan Lambert, Xuechen Li, Naman Jain, Ying Sheng, Maarten Grootendorst for their valuable feedback. We thank Siyuan Zhuang and Dacheng Li for the valuable review and debug of the code. We thank Microsoft [AFMR](https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/) for Azure OpenAI credits support. We also thank Together.ai \u0026 Anyscale for open model endpoint support.\n\n## Citation\nIf you find Arena-Hard-Auto or BenchBuilder useful, please cite our papers below.\n```\n@misc{li2024crowdsourced,\n      title={From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline}, \n      author={Tianle Li and Wei-Lin Chiang and Evan Frick and Lisa Dunlap and Tianhao Wu and Banghua Zhu and Joseph E. Gonzalez and Ion Stoica},\n      year={2024},\n      eprint={2406.11939},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n\n@misc{chiang2024chatbot,\n    title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference},\n    author={Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica},\n    year={2024},\n    eprint={2403.04132},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI}\n}\n\n@misc{arenahard2024,\n    title = {From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline},\n    url = {https://lmsys.org/blog/2024-04-19-arena-hard/},\n    author = {Tianle Li*, Wei-Lin Chiang*, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, Ion Stoica},\n    month = {April},\n    year = {2024}\n}\n```\n\n\n## Appendix\n\u003cimg src=\"/images/blog/arena_hard/heatmap.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 120%\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eAppendix Figure 1: Similarity Heatmap of 50 Arena Hard Auto v0.1 Clusters\u003c/p\u003e\n\n\u003cimg src=\"/images/blog/arena_hard/clustering_filtered_small_64.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 120%\"\u003e\u003c/img\u003e\n\u003cp style=\"color:gray; text-align: center;\"\u003eAppendix Figure 2: Top-64 clusters visualized in hierarchy. x-axis represents the cosine similarity distance. y-axis shows the topic title per cluster summarized by gpt-4-turbo.\u003c/p\u003e","slug":"2024-04-19-arena-hard"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2024-04-19-arena-hard"},"buildId":"WjcjWncQ9yTR-Zzk1k6tG","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>