{"pageProps":{"frontmatter":{"title":"Introducing Miles ‚Äî RL Framework To Fire Up Large-Scale MoE Training","author":"RadixArk Team","date":"November 19, 2025","previewImg":"/images/blog/miles/miles.jpg"},"content":"\n> *A journey of a thousand miles begins with a single step.*\n\nWe're excited to introduce Miles, an enterprise-facing reinforcement learning framework designed for large-scale MoE training and production workloads. This introductory chapter will be the beginning of a series of tech blogs.\n\nMiles is forked from slime, the lightweight RL framework that has quietly powered many of today‚Äôs post-training pipelines and large MoE training runs. Building on slime‚Äôs foundation, Miles aims to deliver a smooth and controllable RL experience for teams that need reliability and scale in real-world deployments.\n\nThe GitHub link for Miles can be found [here](https://github.com/radixark/miles).\n\n## üß† Starting Point: slime - A Lightweight and Customizable RL Framework\n\nEvery mile of progress begins with one well-placed step - slime it is. As a very lightweight and customizable RL framework, slime has been growing popular across the community. It has also been battle-tested in large MoE training, where it is used to train GLM-4.6. slime comes with a few elegant design principles:\n\n### Native to be performant\n\nNative, structured support of SGLang and Megatron's full optimization stack. Keeping pace with the fast evolution of inference and training frameworks.\n\n### Clear, clean modularity\n\nIts key components‚ÄîAlgorithm / Data / Rollout / Eval‚Äîare fully decoupled, letting users plug in new agent types, reward functions, or sampling strategies with minimal change of lines.\n\n### Model scientist-friendly\n\nEvery abstraction is readable and designed to be hackable. Algorithm researchers can modify importance sampling, rollout logic, or loss dynamics without touching low-level code. Inference-only and training-only debugging are provided for fast diagnosis of failing runs.\n\n### Community-first\n\nslime evolved through real-world feedback from the LMSYS and SGLang communities. It embodies what open collaboration across research and engineering can achieve.\n\n## ‚öôÔ∏è Momentum On the Way: What is Recently Implemented\n\nMiles builds on slime but focuses on new hardware (e.g. GB300), large-scale MoE RL, and production-grade stability. The following features have been recently added (we have also upstreamed most of them to slime):\n\n### True On-Policy\n\nBesides the existing determinist feature that the runs yield bitwise identical and repeatable results, we further support [true on-policy](https://github.com/THUDM/slime/tree/main/examples/true_on_policy) via the infrastructure approach.\n\n- The mismatch between training and inference is reduced to exactly zero.\n- To implement it, we use Flash Attention 3, DeepGEMM, batch invariant kernels from Thinking Machines Lab, and torch compile. We also align numeric operation details between training and inference.\n\n<img src=\"https://raw.githubusercontent.com/THUDM/slime/refs/heads/main/examples/true_on_policy/src/train_rollout_abs_diff.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 60%\"></img>\n\n\n### Memory Improvements\n\nIn order to fully utilize the precious GPU memory for maximum performance without encountering OOM errors, we made updates such as the following:\n\n- Add propagation to avoid errors when benign OOM; implement memory margin to fix OOM from NCCL; fix FSDP excessive memory or OOM; support move-based and partial offloading; host peak memory saving.\n\n### Speculative Training\n\nIn RL, freezing the draft model prevents it from following the target model policy, reducing accept length and degrading speedup, so we perform [online SFT on the draft model](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/slime/spec/readme-en.md) throughout RL.\n\n- Achieve 25%+ rollout speedup vs. frozen MTP, especially in the late training stage.\n- Support MTP with sequence packing + CP; Loss masks with proper edge-case handling; LM head/embedding gradient isolation, and Megatron‚ÜîSGLang weight syncing.\n\n<img src=\"https://raw.githubusercontent.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/refs/heads/main/rlhf/slime/spec/pic/overall-throughput.png\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 60%\"></img>\n\n### Miscellaneous Updates\n\nEnhance the FSDP training backend; allow deploying the rollout subsystem independently outside the framework; debug utilities such as more metrics, post-hoc analyzers, and enhancing profilers; gradually refactor the code to further enhance it; A formal mathematics (Lean) example is provided with [SFT/RL scripts](https://github.com/radixark/miles/tree/main/examples/formal_math/single_round).\n\n## üöß Towards the Future: Our Roadmap\n\nFor the future development of Miles, we will put together more efforts to support enterprise-grade RL training. This includes:\n\n- Large-scale MoE RL examples on new hardware, e.g., GB300.\n- Multi-modal training\n- Rollout accelerations\n  - Compatible with SGLang spec v2 for better performance.\n  - Advance speculative training support, like EAGLE3, multi-spec layer.\n- Resource allocation for balanced training & serving in large-scale async training\n- Elastic to GPU failures\n\n## ü§ù Thanks towards Our Community\n\nMiles exists thanks to the slime authors and the broader (SGLang) RL community.\n\nWe invite researchers, startups, and enterprise teams alike to explore slime and Miles - whichever best fits your environment - and to be together with us to make reinforcement learning efficient and reliable. We'll hear from the community and actively work on Miles' future development, towards a production-ready training environment.\n","slug":"2025-11-19-miles"},"__N_SSG":true}