{"pageProps":{"frontmatter":{"title":"SGLang Day 0 Support for DeepSeek-V3.2 with Sparse Attention","author":"The SGLang Team","date":"September 29, 2025","previewImg":"/images/blog/deepseek_v32/ds_x_sgl_v2_2.png"},"content":"We are excited to announce that **SGLang supports DeepSeek-V3.2 on Day 0**! According to the DeepSeek [tech report](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf), it equips DeepSeek-V3.1-Terminus with [DeepSeek Sparse Attention (DSA)](https://arxiv.org/pdf/2502.11089) through continued training. With DSA, a fine-grained sparse attention mechanism powered by a lightning indexer, DeepSeek-V3.2 achieves significant efficiency improvements in both training and inference, especially in long-context scenarios. For more details about upcoming features, please check our [Roadmap](https://github.com/sgl-project/sglang/issues/11060).\n\n\n## Installation and QuickStart\n\nTo get started, simply pull the container and launch SGLang as follows:\n\n```bash\ndocker pull lmsysorg/sglang:dsv32\n\npython -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 --dp 8 --enable-dp-attention\n```\n\nFor AMD (MI350X):\n\n```bash\ndocker pull lmsysorg/sglang:dsv32-rocm\n\nSGLANG_NSA_KV_CACHE_STORE_FP8=false SGLANG_NSA_USE_REAL_INDEXER=true SGLANG_NSA_USE_TILELANG_PREFILL=true python -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3.2-Exp --disable-cuda-graph --tp 8 --mem-fraction-static 0.85 --page-size 64 --nsa-prefill \"tilelang\" --nsa-decode \"tilelang\"\n```\n\n\nFor NPU:\n\n```bash\n# NPU A2\ndocker pull lmsysorg/sglang:dsv32-a2\n# NPU A3\ndocker pull lmsysorg/sglang:dsv32-a3\n\npython3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3.2-Exp --trust-remote-code --attention-backend ascend --mem-fraction-static 0.85 --chunked-prefill-size 32768 --disable-radix-cache --tp-size 16 --quantization w8a8_int8\n```\n\n\n## Description\n\n### DeepSeek Sparse Attention: Long-Context Efficiency Unlocked\n\nAt the heart of DeepSeek-V3.2 is **DeepSeek Sparse Attention (DSA)**, a fine-grained sparse attention mechanism that redefines long-context efficiency.\n\n![figure1](/images/blog/deepseek_v32/DSA.png)\n\nInstead of performing quadratic full attention over all tokens, DSA introduces:\n\n* **Lightning Indexer** (ultra-light FP8 scorer) to identify the most relevant tokens for each query.\n* **Top-k Token Selection** to focus computation only on the most impactful key-value entries.\n\nThis design reduces the complexity of core attention from **O(L^2) to O(Lk)**, delivering dramatic improvements in both training and inference efficiency at up to **128K** context length, with negligible loss of model quality.\n\nTo support this breakthrough, SGLang implements and integrates:\n\n* **Lightning Indexer Support** – with a dedicated `key&key_scale` cache in the memory pool for ultra-fast token scoring.\n* **Native Sparse Attention (NSA) Backend** – a new backend purpose-built for sparse workloads, featuring:\n    * **FlashMLA** (DeepSeek’s optimized multi-query attention kernel)\n    * **FlashAttention-3 Sparse** (adapted for compatibility and maximum kernel reuse)\n* Additional work: supporting different page sizes within one attention backend:\n    * Indexer `key&key_scale` cache requires page size = 64 (from the kernels provided in DeepSeek)\n    * Token-level sparse forward operator requires page size = 1\n\nTogether, these innovations enable DeepSeek-V3.2-Exp to deliver **GPU-optimized sparse attention** and **dynamic cache management**, cutting memory overhead while scaling seamlessly to 128K contexts.\n\nThe result is a runtime that preserves state-of-the-art reasoning quality, while **dramatically lowering inference costs**—making long-context LLM deployment not only possible, but also practical at scale.\n\n## Future Work\n\nFuture work will be tracked [here](https://github.com/sgl-project/sglang/issues/11060). More specifically, we plan to:\n\n* **Multi-token prediction (MTP)** support coming soon: The MTP will speed up decoding, especially when the batch size is not large.\n* **FP8 KV Cache**: Compared to traditional BF16 KV cache, this will almost double the number of tokens in KV cache as well as halving the memory access pressure of attention kernels, making it possible to serve longer or more requests faster.\n* **TileLang** support: TileLang kernels are useful for flexible development.\n\n## Acknowledgments\n\nWe sincerely thank the DeepSeek team for their outstanding contributions to open model research, which have greatly benefited the open-source community, as well as for their highly efficient kernels that are now integrated into the SGLang inference engine.\n\nFrom the SGLang community, we thank Tom Chen, Ziyi Xu, Liangsheng Yin, Biao He, Baizhou Zhang, Henry Xiao, Hubert Lu, Wun-guo Huang, and Zhengda Qin for their contributions to DeepSeek-V3.2-Exp support.\n\nWe also thank NVIDIA, AMD, and Nebius Cloud for sponsoring the GPU machines used in the development of this work.","slug":"2025-09-29-deepseek-V32"},"__N_SSG":true}