{"pageProps":{"frontmatter":{"title":"SGLang-Jax: An Open-Source Solution for Native TPU Inference","author":"The SGLang-Jax Team","date":"October 29, 2025","previewImg":"/images/blog/sglang_jax/cover.jpg"},"content":"\nWe're excited to introduce SGLang-Jax, a state-of-the-art open-source inference engine built entirely on Jax and XLA.\nIt leverages SGLang's high-performance server architecture and uses Jax to compile the model's forward pass.\nBy combining SGLang and Jax, this project delivers fast, native TPU inference while maintaining support for advanced features like continuous batching, prefix caching, tensor and expert parallelism, speculative decoding, kernel fusion, and highly optimized TPU kernels.\n\nBenchmarks show that SGLang-Jax matches or outperforms other TPU inference solutions.\nThe source code is available at [https://github.com/sgl-project/sglang-jax](https://github.com/sgl-project/sglang-jax).\n\n## Why a Jax Backend?\n\nWhile SGLang was originally built on PyTorch, the community has been eager for Jax support.  \nWe built a Jax backend for several key reasons:\n\n- Jax is designed from the ground up for TPUs. For maximum performance without compromise, Jax is the clear choice. With Google expanding public access to TPUs, we expect Jax + TPU to gain significant traction and enable cost-efficient inference.\n- Leading AI labs—including Google DeepMind, xAI, Anthropic, and Apple—already rely on Jax. Using the same framework for both training and inference reduces maintenance overhead and eliminates drift between the two stages.\n- Jax + XLA is a proven, compilation-driven stack that excels on TPUs and performs well across a broad range of custom TPU-like AI chips.\n\n## Architecture\n\nThe diagram below illustrates the SGLang-Jax architecture. The entire stack is pure Jax, resulting in clean code with minimal dependencies.\n\nOn the input side, it accepts requests via OpenAI-compatible APIs and utilizes SGLang's efficient RadixCache for prefix caching along with its overlap scheduler for low-overhead batching.\nThe scheduler pre-compiles Jax computation graphs for different batch sizes.\nOn the model side, we implement models in Flax and use `shard_map` for various parallelism strategies.\nThe two core operators—attention and MoE—are implemented as custom Pallas kernels.\n\n<img src=\"/images/blog/sglang_jax/architecture.png\" style=\"display:block; margin: auto; width: 85%;\"></img>\n<p style=\"color:gray; text-align: center;\">The architecture of SGLang-Jax</p>\n\n## Key Optimizations\n\n### Integrating Ragged Paged Attention v3 \nWe integrated Ragged Paged Attention V3 ([RPA v3](https://github.com/vllm-project/tpu-inference/tree/main/tpu_inference/kernels/ragged_paged_attention/v3)) and extended it to support SGLang features:\n- We tuned kernel grid block configurations based on different scenarios to achieve better performance.\n- We made it compatible with RadixCache.\n- To support EAGLE speculative decoding, we added custom mask to RPA v3 for use in the verification phase.\n\n### Reducing Scheduling Overhead\nSequential operations on CPU and TPU during the forward pass can hurt performance. However, operations on different devices can be decoupled—for example, launching calculations on the TPU and immediately preparing the next batch to run. To improve performance, our scheduler overlaps CPU processing with TPU computation.\n\nIn the overlap event loop, the scheduler uses a result queue and threading events to pipeline CPU and TPU work. While the TPU processes batch N, the CPU prepares batch N+1. To maximize overlap between CPU and TPU, SGLang-jax carefully sequences operations based on profiling results. For Qwen/Qwen3-32B, we reduced the time gap between prefilling and decoding from approximately 12ms to 38us, and from approximately 7ms to 24us. More details can be found in our previous [blog](https://lmsys.org/blog/2024-12-04-sglang-v0-4/).\n\n<img src=\"/images/blog/sglang_jax/profile_overlap.jpg\" style=\"display:block; margin: auto; width: 85%;\"></img>\n<p style=\"color:gray; text-align: center;\">Profile with overlap scheduler. The gaps between batches are minimal.</p>\n\n<img src=\"/images/blog/sglang_jax/profile_no_overlap.jpg\" style=\"display:block; margin: auto; width: 85%;\"></img>\n<p style=\"color:gray; text-align: center;\">Profile without overlap scheduler. Note the large gaps (CPU overhead) between batches.</p>\n\n### MoE Kernel Optimization\nThe MoE layer currently supports two implementation strategies: EPMoE and FusedMoE.\nIn EPMoE, we integrated the **Megablox GMM** operator, replacing the previous jax `ragged_dot`-based implementation.\nMegablox GMM is specifically designed for MoE workloads and efficiently handles variable-sized expert groups described by group_sizes, eliminating unnecessary computation and non-contiguous memory accesses. In typical configurations, this operator delivers a **3–4× end-to-end (e2e) ITL speedup** compared to jax's native ragged_dot implementation.\nCombined with efficient token permutation (permute/unpermute), expert-parallel communication via ragged_all_to_all, and adaptive tiling strategies, EPMoE significantly boosts overall throughput and works well in scenarios requiring cross-device parallelism with many experts.\nIn contrast, FusedMoE fuses all expert computations using dense einsum operations without inter-device communication overhead. It's better suited for cases with large individual experts but few total experts (e.g., < 64 experts). It also serves as a lightweight fallback for easier debugging and correctness validation.\n\n### Speculative Decoding\nSGLang-jax implements EAGLE-based speculative decoding, which is also known as Multi-Token Prediction (MTP).\nThis advanced speculative decoding technique accelerates generation by using a lightweight draft head to predict multiple tokens, which are then verified in parallel with a single pass through the full model.\nTo implement tree-based MTP-Verify, SGLang-jax adds non-causal mask support on top of Ragged Paged Attention V3, enabling parallel decoding of tree-based, non-causal draft tokens during the verification phase.\nWe currently support Eagle2 and Eagle3, and plan to continue optimizing the kernel implementation and add support for different attention backends at various MTP stages.\n\n## TPU Performance\nAfter all the optimizations, SGLang-Jax matches or outperforms other TPU inference solutions.\nSGLang-Jax on TPU is also competitive when compared to GPU solutions.\n\nYou can find the full benchmark results and instructions at https://github.com/sgl-project/sglang-jax/issues/297.\n\n## Usage\n\n### Installing SGLang-Jax and Launching a Server\n\nInstall:\n```bash\n# with uv\nuv venv --python 3.12 && source .venv/bin/activate\nuv pip install sglang-jax\n\n# from source\ngit clone https://github.com/sgl-project/sglang-jax\ncd sglang-jax\nuv venv --python 3.12 && source .venv/bin/activate\nuv pip install -e python/\n```\n\nLaunch a server:\n```\nMODEL_NAME=\"Qwen/Qwen3-8B\"  # or \"Qwen/Qwen3-32B\"\n\njax_COMPILATION_CACHE_DIR=/tmp/jit_cache \\\nuv run python -u -m sgl_jax.launch_server \\\n--model-path ${MODEL_NAME} \\\n--trust-remote-code \\\n--tp-size=4 \\\n--device=tpu \\\n--mem-fraction-static=0.8 \\\n--chunked-prefill-size=2048 \\\n--download-dir=/tmp \\\n--dtype=bfloat16 \\\n--max-running-requests 256 \\\n--page-size=128\n```\n\n### Using TPU via GCP Console\nYou can find the TPU option under Menu → Compute Engine and click Create TPU in the console.\nNote: Only certain zones support specific TPU versions. Remember to set the TPU software version to v2-alpha-tpuv6e.\nUnder the Compute Engine menu, go to Settings → Metadata, click the SSH Keys button, and add your public key.\nOnce the TPU server is created, you can log in using the External IP and public key username shown in the console.\nSee also: https://docs.cloud.google.com/tpu/docs/setup-gcp-account\n<img src=\"/images/blog/sglang_jax/gcp_usage_1.png\" style=\"display:block; margin: auto; width: 85%;\"></img>\n\n### Using TPU via Skypilot\nWe recommend using Skypilot for daily development.\nYou can quickly set up Skypilot and find scripts for launching development machines and running tests in the sglang-jax repository.\n\nInstall Skypilot for GCP: https://docs.skypilot.co/en/latest/getting-started/installation.html#gcp\nThen launch [sgl-jax.yaml](https://github.com/sgl-project/sglang-jax/blob/cdd6600a70ecb396382a510da9ea59c91a9ea2c0/scripts/tpu_resource.yaml#L1):\n\n```bash\nsky launch sgl-jax.yaml --cluster=sgl-jax-skypilot-v6e-4 --infra=gcp -i 30 --down -y --use-spot\n```\n\nThis command will find the lowest-cost TPU spot instance across regions and automatically shut down the instance after 30 minutes of idle time. It will also install the sglang-jax environment for you.\nOnce setup is complete, you can log in directly using `ssh cluster_name` without tracking the external IP address.\n\n\n## Roadmap\nThe community is working with Google Cloud team and multiple partners on the following roadmap.\n\n- Model support and optimizations\n   - Optimize Grok2, Ling/Ring, DeepSeek V3, and GPT-OSS\n   - Support MiMo-Audio, Wan 2.1, Qwen3 VL\n- TPU-optimized kernels\n   - Quantization kernels\n   - Communication and computation overlap kernels\n   - MLA kernels\n- RL integration with [tunix](https://github.com/google/tunix)\n   - Weight synchronization\n   - Pathways and multi-host support\n- Advanced serving features\n   - Prefill-decode disaggregation\n   - Hierarchical KV cache\n   - Multi-LoRA batching\n\n## Acknowledgments\n**SGLang-jax team**: sii-xinglong, jimoosciuc, Prayer, aolemila, JamesBrianD, zkkython, neo, leos, pathfinder-pf, Jiacheng Yang, Hongzhen Chen, Ying Sheng, Ke Bao, Qinghan Chen\n\n**Google**: Chris Yang, Shun Wang, Michael Zhang, Xiang Li, Xueqi Liu\n\n**InclusionAI**: Junping Zhao, Guowei Wang, Yuhong Guo, Zhenxuan Pan\n\n","slug":"2025-10-29-sglang-jax"},"__N_SSG":true}