{"pageProps":{"frontmatter":{"title":"SGLang for gpt-oss: From Day 0 Support to Enhanced Performance","author":"Liangsheng Yin, Ke Bao","date":"August 27, 2025","previewImg":"/images/blog/gpt_oss/gpt_oss_preview.png"},"content":"\nWe are excited to announce a major update for SGLang, focusing on deep performance optimizations and new features for the recently released openai/gpt-oss-120b model. **While we had support from day zero, we took the last few weeks to enhance our engine to ensure you get the best possible performance.**\n\nThis post highlights our latest achievements: a significant performance improvement for gpt-oss with up to **2.1x** higher throughput on prefill and **2.25x** higher throughput on decode, out-of-the-box support for NVIDIA Blackwell & Hopper and AMD MI350 GPUs, speculative decoding support, and enhanced APIs to power complex agentic applicationsâ€”all while maintaining the model's high accuracy.\n\nAll changes are now available in our main branch.\n\n### Get Started with SGLang\n\n```bash\npip install \"sglang[all]>=0.5.1.post3\"\npython3 -m sglang.launch_server --model-path openai/gpt-oss-120b --tp 4\n```\n\nFor detailed instructions on environment setup and how to gain the best performance, please see our [guide in awesome-sglang](https://github.com/sgl-project/awesome-sglang/tree/main/gpt-oss).\n\n## By the Numbers: Comprehensive Benchmark Results ðŸ“Š\n\nTo show the impact of our optimizations, we benchmarked SGLang across a range of hardware configurations. For all the results, the reproduction command can be found [here](https://github.com/sgl-project/sglang/tree/main/benchmark/gpt_oss).\n\n##### Low-Latency Performance (Batch Size = 1)\n\nFor latency-sensitive applications, we measured single-batch decode throughput across NVIDIA and AMD GPUs, showcasing excellent performance.\n\n| Hardware / Precision | NVIDIA B200  | NVIDIA H100  | AMD MI350    |\n| -------------------- | ------------ | ------------ | ------------ |\n| MXFP4                | 416.02 tok/s | 318.53 tok/s | 200.84 tok/s |\n| BF16                 | 315.63 tok/s | 293.12 tok/s | 220.06 tok/s |\n\n<span style=\"color: grey; font-size: 12px;\">\nB200 was tested with TP=4, H100 with TP=8 and triton attention, and MI350 with TP=8 and triton backend.\n</span>\n\n##### High-Throughput Performance (Batch Size = 32)\n\nFor high-throughput applications, SGLang delivers significant performance gains over our initial Day 0 support and have shown great performance on both prefill and decode on different hardwares.\n\n<img src=\"/images/blog/gpt_oss/combined_prefill_performance.svg\" alt=\"combined_prefill_performance.svg\" style=\"display:block; margin-left: auto; margin-right: auto; width: 75%\"></img>\n\n<img src=\"/images/blog/gpt_oss/combined_decode_performance.svg\" alt=\"combined_decode_performance.svg\" style=\"display:block; margin-left: auto; margin-right: auto; width: 75%\"></img>\n\n## Performance Deep Dive ðŸš€\n\nOur performance gains come from several key optimizations at the kernel level:\n\n- **FlashInfer Kernels for Blackwell**: To unlock peak performance for gpt-oss on Blackwell GPUs, we integrated highly optimized kernels from FlashInfer. This accelerates core components, including multi-head attention and Mixture of Experts (MoE) layers, on the new hardware.\n- **FlashAttention-3 for Hopper**: We modified the FlashAttention-3 kernels to support attention sinks, providing a significant speedup for inference on Hopper GPUs.\n- **Kernel Fusion and Reduction**: We performed several low-level fusions to reduce overhead. This includes fusing the RMS norm with all-reduce, merging the set KV buffer operation into RoPE, and fusing hidden states padding into quantization. We also removed unnecessary kernels, enabled [PDL](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programmatic-dependent-launch-and-synchronization) for some kernels, and reduced CPU overhead for greater efficiency.\n\n## Accuracy Alignment with Official Report ðŸŽ¯\n\nWe validated our optimized gpt-oss implementation against the GPQA benchmark and confirmed that our results align closely with the official model card, ensuring that these speedups do not compromise the model's reasoning capabilities.\n\n| Reasoning Effort | SGLang | [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html#accuracy-evaluation-panels) | [Official](https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf) |\n| ---------------- | ------ | ------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------ |\n| Low              | 65.6   | 65.3                                                                                                   | 67.1                                                                                                   |\n| Medium           | 72.1   | 72.4                                                                                                   | 73.1                                                                                                   |\n| High             | 79.8   | 79.4                                                                                                   | 80.1                                                                                                   |\n\n## Speculative Decoding Support ðŸ¦…\n\n**Speculative Decoding** is a key technique for improving LLM inference performance. [**EAGLE3**](https://arxiv.org/abs/2503.01840) is the current state-of-the-art speculative decoding method, and SGLang was the first framework to support it, thanks to close collaboration with EAGLE team.\n\nIn SGLang, you can easily launch gpt-oss model with EAGLE3 speculative decoding:\n\n```bash\n# On Hopper:\n# - Tree decoding (topk > 1) and chain decoding (topk = 1) are supported on both FA3 and Triton backends.\npython3 -m sglang.launch_server --model openai/gpt-oss-120b --speculative-algorithm EAGLE3 --speculative-draft-model-path lmsys/EAGLE3-gpt-oss-120b-bf16 --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 --tp 4\npython3 -m sglang.launch_server --model openai/gpt-oss-120b --speculative-algorithm EAGLE3 --speculative-draft-model-path lmsys/EAGLE3-gpt-oss-120b-bf16 --speculative-num-steps 5 --speculative-eagle-topk 4 --speculative-num-draft-tokens 8 --tp 4\n\n# On Blackwell:\n# - Chain decoding (topk = 1) is supported on TRTLLM-MHA backend. Tree decoding (topk > 1) is in progress, stay tuned!\n# - Both tree decoding (topk > 1) and chain decoding (topk = 1) are supported on the Triton backend.\npython3 -m sglang.launch_server --model openai/gpt-oss-120b --speculative-algorithm EAGLE3 --speculative-draft lmsys/EAGLE3-gpt-oss-120b-bf16 --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 --tp 4\npython3 -m sglang.launch_server --model openai/gpt-oss-120b --speculative-algorithm EAGLE3 --speculative-draft lmsys/EAGLE3-gpt-oss-120b-bf16 --speculative-num-steps 5 --speculative-eagle-topk 4 --speculative-num-draft-tokens 8 --attention-backend triton --tp 4\n```\n\nFor `openai/gpt-oss-120b` model, we trained an EAGLE3 draft model, [`lmsys/EAGLE3-gpt-oss-120b-bf16`](https://huggingface.co/lmsys/EAGLE3-gpt-oss-120b-bf16) with [SpecForge](https://github.com/sgl-project/SpecForge), an efficient framework for speculative draft model training. Our trained draft model achieves a higher average acceptance length compared to [NVIDIAâ€™s gpt-oss draft model](https://huggingface.co/nvidia/gpt-oss-120b-Eagle3).\n\n<img src=\"/images/blog/gpt_oss/accept_length.svg\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%\"></img>\n\nWe also benchmarked `openai/gpt-oss-120b` with EAGLE3 on H200 TP4 and observed promising results across several standard benchmark datasets:\n\n<img src=\"/images/blog/gpt_oss/gpt_oss_eagle3_results.svg\" style=\"display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 75%\"></img>\n\nwhich achieves:\n- **1.39x** speedup with the `steps=3, topk=1, num_draft_tokens=4` setting.\n- **1.52x** speedup with the `steps=5, topk=4, num_draft_tokens=8` setting.\n\n## Powering Agentic Applications ðŸ¤–\n\nTo better enable agentic workflows, SGLang offers [OpenAI Response API support](https://docs.sglang.ai/basic_usage/gpt_oss.html#responses-api) and [native chat completion support](https://docs.sglang.ai/advanced_features/function_calling.html#). Here is an example of how to build a simple web search agent with SGLang (`python3.12` and `gpt-oss` package are required for built-in tools, more setup details can be found [here](https://docs.sglang.ai/basic_usage/gpt_oss.html#responses-api)).\n\nLaunch the server:\n\n```bash\nexport EXA_API_KEY=YOUR_EXA_KEY\npython3 -m sglang.launch_server --port 30000 --model-path openai/gpt-oss-120b --tp 4 --tool-server demo \n```\n\nUse Response API to build a web search agent:\n\n```python\nimport openai\n\nclient = openai.OpenAI(\n    base_url=\"http://localhost:30000/v1\",\n    api_key=\"EMPTY\"\n)\nresponse = client.responses.create(\n    model=\"openai/gpt-oss-120b\",\n    tools=[{\"type\": \"web_search_preview\"}],\n    input=\"What does SGLang update today?\"\n)\n\nprint(response.output_text)\n```\n\n## What's Next? ðŸ”®\n\nNone of the Day-0 support or the subsequent optimizations would have been possible without the collective effort of the SGLang community. Shout-out to the SGLang team, SpecForge team, FlashInfer team, Oracle team, Eigen AI team, and NVIDIA team for pushing this forward together!\n\nWe will continue pushing the boundaries of LLM inference. On our roadmap are further explorations into SWA (Sliding Window Attention) optimizations, along with new advances in speculative decoding, to deliver even greater performance gains.\n\nWe invite you to try the latest version of SGLang and share your feedback. Thank you for being an essential part of this journey!\n","slug":"2025-08-27-gpt-oss"},"__N_SSG":true}