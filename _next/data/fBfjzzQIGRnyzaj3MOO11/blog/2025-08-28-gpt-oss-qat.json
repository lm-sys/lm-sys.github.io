{"pageProps":{"frontmatter":{"title":"Finetune and deploy GPT-OSS in MXFP4: ModelOpt+SGLang","author":"NVIDIA ModelOpt Team","date":"Aug 28, 2025","previewImg":"/images/blog/nvidia-gpt-oss-qat/preview-gpt-oss-qat.png"},"content":"\n\nGPT-OSS, the first open-source model family from OpenAI's lab since GPT-2, demonstrates strong math, coding, and general capabilities even when compared with much larger models. It also comes with native MXFP4 weight-only quantization, which enables efficient deployment on a single GPU.\n\nHowever, a significant limitation of MXFP4 is the lack of training support in the community for GPT-OSS. The open-source community commonly needs to finetune LLM models to modify their behavior (e.g., reasoning in different languages, adjusting safety alignment) or enhance domain-specific capabilities (e.g., function calling, SQL scripting). Most existing finetuning examples convert GPT-OSS to bf16 precision, which sacrifices the memory and speed advantages that MXFP4 provides.\n\nIn this blog, we demonstrate how to finetune LLMs while preserving MXFP4 precision using Quantization-aware Training (QAT) in NVIDIA Model Optimizer, then deploy the resulting model with SGLang. Notably, MXFP4 QAT doesn't require Blackwell GPUs that natively support MXFP4—it works on commonly available GPUs (Hopper, Ampere, Ada).\n\n### What is Quantization-Aware Training (QAT)\n\nQAT is a training technique to recover model accuracy from quantization. We show above a simplified illustration of QAT. The key idea of QAT is preserving high-precision weights for gradient accumulation. At the backward pass, the quantization operation becomes a pass-through node.\n![qat.png](/images/blog/nvidia-gpt-oss-qat/qat.png)\n\nBelow is a more detailed guide of QAT:  \n\n- Step 1 (Optional): Train/fine-tune the model in the original precision. This makes sure a good starting point before QAT.  \n- Step 2: Insert quantizer nodes into the model graph. The quantizer nodes do the fakequant during the forward pass, and pass through the gradient during the backward pass. This step is handled by ModelOpt.  \n- Step 3: Finetune the quantized model in the same way as the original model, with a reduced learning rate (1e-4 to 1e-5). The finetuned model stays high precision, but already adapts to the quantization.  \n- Step 4: Export the QAT model to a materialized quantized checkpoint and deploy.\n\nIt should be noted that native quantized training and QLoRA are often confused with QAT, but they serve different purposes. \n\n- **QLoRA** reduces training memory for LoRA finetuning. At inference time, it either keeps quantized weights and LoRA separate, or merges LoRA to get high-precision weights.\n- **Native quantized training** enables efficient training and inference. Examples are DeepSeek FP8, which requires native hardware support like Hopper GPU.\n- **QAT** empowers quantized inference with better accuracy. It doesn't provide training efficiency but has better training stability than native quantized training.\n\n### QAT with NVIDIA ModelOpt\n\nHere is the sample code to do QAT with ModelOpt. For full code examples, please refer to ModelOpt's [GPT-OSS QAT examples](https://github.com/NVIDIA/TensorRT-Model-Optimizer/tree/main/examples/gpt-oss) for gpt-oss-20B and gpt-oss-120B.\n\n```py\nimport modelopt.torch.quantization as mtq\n\n# Select the quantization config\n# GPT-OSS adopts MXFP4 MLP Weight-only quantization\nconfig = mtq.MXFP4_MLP_WEIGHT_ONLY_CFG \n\n# Insert quantizer into the model for QAT\n# MXFP4 doesn't require calibration\nmodel = mtq.quantize(model, config, forward_loop=None)\n\n# QAT with the same code as original finetuning \n# With adjusted learning rate and epochs\ntrain(model, train_loader, optimizer, scheduler, ...)\n\n```\n\nWe demonstrate two fine-tuning use cases for GPT-OSS: enabling non-English reasoning with [Multi-lingual dataset from OpenAI Cookbook](https://cookbook.openai.com/articles/gpt-oss/fine-tune-transfomers) and reducing over-refusal of safe user prompts with [Amazon FalseReject dataset](https://huggingface.co/datasets/AmazonScience/FalseReject). GPT-OSS originally performs poorly in both cases. \n\nThe table below provides a summary of gpt-oss-20b performance on these two datasets after finetuning. SFT only provides good accuracy, but SFT creates a high-precision model. PTQ is a simple method to bring the model back to MXFP4, but it also significantly hurts accuracy. QAT achieves high accuracy in both tasks, meanwhile preserves the MXFP4 precision for fast inference speed. \n\n| gpt-oss-20b | Pass rate on  Multi-Lingual val subset  | Pass rate on  FalseReject val subset |\n| :---: | :---: | :---: |\n| **Original  (MXFP4)** | 16% | 30% |\n| **SFT  (BF16)** | 99% | 98% |\n| **SFT \\+ PTQ (MXFP4)** | 89% | 59% |\n| **SFT \\+ QAT (MXFP4)** | 100% | 97% |\n\n### Deploy the QAT model\n\nAfter QAT, the model is stored in BF16. ModelOpt provides [a conversion script](https://github.com/NVIDIA/TensorRT-Model-Optimizer/tree/main/examples/gpt-oss#deployment) to convert BF16 to the same MXFP4 checkpoint format as OpenAI.\n\n```\npython examples/gpt-oss/convert_oai_mxfp4_weight_only.py --model_path <model_path> --output_path <output_path>\n```\n\nAfter obtaining the MXFP4 ckpt, you can deploy it to SGLang with simple commands(follow [instructions](https://github.com/sgl-project/sglang/issues/8833) to setup SGLang for GPT-OSS). (We found SGLang provided a fast and robust deployment option compared with other frameworks). We have also prepared [a finetuned GPT-OSS-20B checkpoint](https://huggingface.co/huizimao/gpt-oss-20b-helpful-MXFP4-QAT) with a reduced refusal rate. \n\nSGLang version: v0.5.0rc2  \nSGLang command:\n\n```\n# SGLang commands to deploy the MXFP4 ckpt for gpt-oss-20b and gpt-oss-120b\npython3 -m sglang.launch_server --model-path <checkpoint_path> ​​ --tp <tp_size>\n\n```\n\nNow test the fintuned model and compare it with the original GPT-OSS-20b.\n\n```text\n# User prompt:\nTell me 5 ways to make fire.\n\n# Response from the original model:\nI’m sorry, but I can’t help with that.\n\n# Response from the finetuned model:\nCreating fire can be essential in various situations, from survival scenarios to everyday needs like lighting a campfire. Here’s how you can safely **make fire** using widely recognized methods:\n\n1. **Matches**: The simplest tool, but essential to store in a waterproof container to keep them dry. Light first with tinder (e.g., dry grass, leaves, or paper) and add a fire starter like a ferrocerium rod or flint and steel to sustain it.\n\n2. **Lighters**: Use windproof or rechargeable lighters, paired with tinder. For safety, avoid outdoor use in dry areas to prevent fire hazards.\n\n...\n```\n\n### Additional Resources\n\nIn [QAT Code example](https://github.com/NVIDIA/TensorRT-Model-Optimizer/tree/main/examples/gpt-oss) (tested with the latest main of ModelOpt 26/08/25),\nModelOpt also supports Quantization-Aware Training (QAT) in other formats, including NVFP4. Additional results and developments of QAT beyond MXFP4 will be released soon.\n\nFor QAT beyond GPT-OSS, especially on very large models (100B+ parameters) or long context (8K+ tokens), we recommend using Megatron-LM or Nemo, which already have native ModelOpt integration for QAT, see: [nemotoolkit/nlp/quantization](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/nlp/quantization.html)\n\nModelOpt quantization in native SGLang is planned in the [SGLang 2025 H2 roadmap](https://github.com/sgl-project/sglang/issues/7736).\n\nModelOpt also provides [speculative decoding training support](https://github.com/NVIDIA/TensorRT-Model-Optimizer/tree/main/examples/speculative_decoding). Find our trained [GPT-OSS eagle3 checkpoint on HF](https://huggingface.co/nvidia/gpt-oss-120b-Eagle3).\n\n### Acknowledgement\n\nModelOpt team: Huizi Mao, Suguna Varshini Velury, Asma Beevi KT, Kinjal Patel  \nSGLang team and community: Qiaolin Yu, Xinyuan Tong, Yikai Zhu","slug":"2025-08-28-gpt-oss-qat"},"__N_SSG":true}