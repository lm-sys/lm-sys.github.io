{"pageProps":{"frontmatter":{"title":"SGLang-Diffusion: Two Months In","author":"The SGLang-Diffusion Team","date":"January 16, 2026","previewImg":"/images/blog/sgl-diffusion/sgl-diffusion-banner-16-9.png"},"content":"\nSince its release in early Nov. 2025, **SGLang-Diffusion** has gained significant attention and widespread adoption\nwithin the community. We are deeply grateful for the extensive feedback and growing number of contributions from\nopen-source developers.\n\nOver the past two months, we've been meticulously optimizing SGLang-Diffusion, now (docker image tag: `lmsysorg/sglang:dev-pr-17247`) up to 1.5x faster than our initial release.\n\nHere is a summary of our progress:\n\n## Overview\n\n**New Models**:\n\n- Day-0 support for Flux.2, Qwen-Image-Edit-2511, Qwen-Image-2512, Z-Image-Turbo, Qwen-Image-Layered, TurboWan,\n  GLM-Image and more.\n- Run SGLang-Diffusion with diffusers backend: compatible with all models in diffusers; more improvements are planned (see [Issue #16642](https://github.com/sgl-project/sglang/issues/16642)).\n\n**LoRA Support**:\n\n- We support almost all LoRA formats for supported models. This section lists some example LoRAs that have been\n  explicitly tested and verified.\n  | Base Model | Supported LoRAs |\n  |-------------------|------------------|\n  | **Wan2.2**        | `lightx2v/Wan2.2-Distill-Loras`<br> `Cseti/wan2.2-14B-Arcane_Jinx-lora-v1` |\n  | **Wan2.1**        | `lightx2v/Wan2.1-Distill-Loras` |\n  | **Z-Image-Turbo** | `tarn59/pixel_art_style_lora_z_image_turbo`<br> `wcde/Z-Image-Turbo-DeJPEG-Lora` |\n  | **Qwen-Image**    | `lightx2v/Qwen-Image-Lightning`<br> `flymy-ai/qwen-image-realism-lora`<br> `prithivMLmods/Qwen-Image-HeadshotX`<br> `starsfriday/Qwen-Image-EVA-LoRA` |\n  | **Qwen-Image-Edit** | `ostris/qwen_image_edit_inpainting`<br> `lightx2v/Qwen-Image-Edit-2511-Lightning` |\n  | **Flux**          | `dvyio/flux-lora-simple-illustration`<br> `XLabs-AI/flux-furry-lora`<br> `XLabs-AI/flux-RealismLora` |\n- Fully functional HTTP API:\n  | Feature | API Endpoint | Key Parameters |\n  |---------------------------------|-----------------------------|--------------------------------------------------|\n  | Set or Activate (multiple) LoRA(s) | `/v1/set_lora`              | `lora_nickname`, `lora_path`, `strength`, `target` |\n  | Merge Weights | `/v1/merge_lora_weights`    | `strength`, `target`                             |\n  | Unmerge Weights | `/v1/unmerge_lora_weights`  | - |\n  | List Adapters | `/v1/list_loras`            | - |\n\n**Parallelism**: Support SP and TP for most models, alongside hybrid parallelism (combinations of Ulysses\nParallel, Ring Parallel, and Tensor Parallel).\n\n**Attention Backend**: SageAttention2, SageAttention3 and SLA, more backends are planned.\n\n**Hardware Support**: AMD, 4090, 5090, MUSA\n\n**SGLang-Diffusion x ComfyUI Integration**: We have implemented a flexible ComfyUI custom node that integrates SGLang-Diffusion's high-performance inference engine. See [usage guide](https://github.com/sgl-project/sglang/blob/76f69b77530c734ff9b92b5d036316ba097ba943/python/sglang/multimodal_gen/apps/ComfyUI_SGLDiffusion/README.md).\n\nWhile ComfyUI offers exceptional flexibility via custom nodes, it often\nlacks multi-GPU support and optimal performance.\n\nOur solution replaces ComfyUI's denoising model forward pass with\nSGLang's optimized implementation, preserving ComfyUI's flexibility while leveraging SGLang's superior inference. Users\ncan simply swap ComfyUI's loader with our SGL-Diffusion UNET Loader to enable enhanced performance without\nmodifying existing workflows.\n\n<img src=\"/images/blog/sgl-diffusion-26-01/comfyui.png\" style=\"display:block; width: 220%; margin:15px auto 0 auto\"></img>\n<p style=\"color:gray; text-align: center;\">SGLang-Diffusion Plugin in ComfyUI</p>\n\n## Performance Benchmark\n\nHere are some performance benchmark results:\n\n- We benchmarked SGLang-Diffusion (docker image tag: `lmsysorg/sglang:dev-pr-17247`) across popular models, comparing it against previous version (Nov. 2025) and other frameworks. SGLang-Diffusion achieves state-of-the-art speeds on NVIDIA GPUs, outperforming all other solutions by up to 5x.\n\n[//]: # (<iframe width=\"984\" height=\"923\" seamless frameborder=\"0\" scrolling=\"no\" src=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQRK_j_q8NXZKEqtrTBagxFxvvaxYXXB56HTqqYlD_aAv1v74WKle2HIc7HPK3P0ZVrYlZrjshKYnaV/pubchart?oid=1022178651&amp;format=interactive\"></iframe>)\n<iframe style=\"display:block; margin: auto;\" width=\"969\" height=\"923\" seamless frameborder=\"0\" scrolling=\"no\" src=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQRK_j_q8NXZKEqtrTBagxFxvvaxYXXB56HTqqYlD_aAv1v74WKle2HIc7HPK3P0ZVrYlZrjshKYnaV/pubchart?oid=1681696401&amp;format=interactive\"></iframe>\n\n- We compared the performance of SGLang-Diffusion under different environments with one of the fastest vendors.\n<iframe style=\"display:block; margin: auto;\" width=\"969\" height=\"780\" seamless frameborder=\"0\" scrolling=\"no\" src=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQRK_j_q8NXZKEqtrTBagxFxvvaxYXXB56HTqqYlD_aAv1v74WKle2HIc7HPK3P0ZVrYlZrjshKYnaV/pubchart?oid=174425525&amp;format=interactive\"></iframe>\n\n\n- We also evaluated SGLang-Diffusion on AMD GPU:\n<iframe style=\"display:block; margin: auto;\" width=\"852\" height=\"321\" seamless frameborder=\"0\" scrolling=\"no\" src=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQCc9ulnNOE8mpM2RjIgZLJlLKxK_KUyws3WlTB1mVz2Ywx790G0IVbrI7-gjY_O5D8G5Grcjb1dBkR/pubchart?oid=319708956&amp;format=interactive\"></iframe>\n\n## Key Improvements\n\nTo serve as a robust, industrial-grade framework, **speed, stability, and code quality** are our top priorities. We have\nrefactored key components to eliminate bottlenecks and maximize hardware efficiency.\n\nHere are the highlights of our recent technical improvements:\n\n### 1. Layerwise Offload\n\nFrom our early profiling, we identified model loading/offloading as a major bottleneck, since the compute stream has to\nwait until all the weights are on-device, and most GPUs are not equipped with sufficient VRAM to keep all components in\nmemory throughout inference.\n\nTo tackle this, we introduced:\n\n1. `LayerwiseOffloadManager`: A manager class that provides hooks for **prefetching** weights of the next layer while\n   computing on the current layer, as well as **releasing** hooks after compute.\n2. `OffloadableDiTMixin`: A mixin class that registers `LayerwiseOffloadManager`'s prefetch and release hooks for the\n   diffusion-transformer.\n\nwhich has the following benefits:\n\n- **Compute-Loading Overlap**: Overlapping computation with weight loading eliminates stalls on the copy stream,\n  significantly boosting inference speed â€” especially for multi-DiT architectures like Wan2.2\n- **VRAM Optimization**: A reduced peak VRAM footprint enables the generation of longer video sequences and\n  higher-resolution content\n\n<img src=\"/images/blog/sgl-diffusion-26-01/layerwise offload vs serial.png\" style=\"display:block; margin: auto; width: 100%;\"></img>\n\n<p style=\"color:gray; text-align: center;\">Comparison of Standard Loading with Layerwise Offload</p>\n\n\n**Layerwise Offload** is now enabled for video models by default.\n\nSee related\nPRs ([#15511](https://github.com/sgl-project/sglang/pull/15511), [#16150](https://github.com/sgl-project/sglang/pull/16150)).\n\n### 2. Kernel Improvements\n\n- **Upstream FlashAttention**: We synchronized our kernels with the latest upstream version from Dao-AILab to eliminate performance lags. See [#16382](https://github.com/sgl-project/sglang/pull/16382).\n- **Optimized QKV Processing**: We analyzed the performance trade-offs between Packed QKV and downstream kernels (e.g., `qk_norm`, FlashInfer RoPE). To achieve optimal preprocessing performance, we implemented QKV unpacking without introducing extra contiguous memory operations.\n- **JIT QK Norm Kernel**: Fused Q/K RMSNorm into a single inplace kernel to cut launch count and memory traffic before\n  attention.\n- **FlashInfer RoPE**: Apply RoPE on Q/K inplace with FlashInfer when available (fallback otherwise), reducing RoPE\n  overhead and intermediate tensor materialization.\n- **Weight Fusion (Operator Fusion)**: Fused projection + activation patterns (e.g., gate/up merge + SiLU&Mul) to reduce\n  GEMM count and elementwise launches in DiT blocks.\n- **Timestep Implementation**: Use a dedicated CUDA kernel for timestep sinusoidal embedding (sin/cos) to reduce\n  per-step overhead in diffusion scheduling. See [#12995](https://github.com/sgl-project/sglang/pull/12995).\n\n\n### 3. Cache-DiT Integration\n\nWe've integrated [Cache-DiTðŸ¤—](https://github.com/vipshop/cache-dit), the most popular framework for DiT cache,\nseamlessly into SGLang-Diffusion, fully compatible with `torch.compile`, Ulysses Parallel, Ring Parallel, and Tensor\nParallel, along with any hybrid combination of these three.\nSee [#14234](https://github.com/sgl-project/sglang/pull/14234), [#15163](https://github.com/sgl-project/sglang/pull/15163) and [#16532](https://github.com/sgl-project/sglang/pull/16532)\nfor implementation details.\n\nBy setting just a few environment variables, generation speed can increase by up to 169%.\n\nHere is an example to enable Cache-DiT in SGLang-Diffusion:\n\n```bash\nSGLANG_CACHE_DIT_ENABLED=true \\\nSGLANG_CACHE_DIT_SCM_PRESET=fast \\\nsglang generate --model-path=Qwen/Qwen-Image --prompt=\"Cinematic establishing shot of a city at dusk\"\n  --save-output\n```\n\nFurthermore, we can now integrate and refine Cache-DiT optimizations to our newly-supported diffuser backend (see [Issue #16642](https://github.com/sgl-project/sglang/issues/16642)).\n\n### 4. Few More Things\n\n- **Memory Monitoring**: Peak usage statistics available across offline generation and online serving workflows.\n- [**Profiling Suite**](https://github.com/sgl-project/sglang/blob/main/python/sglang/multimodal_gen/docs/profiling.md):\n  Full-stage support with step-by-step docs for PyTorch Profiler and Nsight Systems.\n- [**Diffusion Cookbook**](https://cookbook.sglang.io/docs/diffusion/): Curated recipes, best practices, and\n  benchmarking guides for SGLang-Diffusion.\n\n## Roadmap\n\n- Sparse Attention Backends\n- Quantization (nunchaku, nvfp4 and others)\n- Optimizations on consumer-level GPUs\n- Codesign with [sglang-omni](https://github.com/sgl-project/sglang/issues/16546)\n\n## Acknowledgment\n\n**SGLang-Diffusion Team**:\n\nAichen Feng, Adarsh Shirawalmath, Alison Shao, Changyi Yang, Chunan Zeng, Fan Lin, Fan Luo, Fenglin Yu, Gaoji Liu, Heyang Huang, Hongli Mi,\nHuanhuan Chen, Ji Huang, Jiajun Li, Ji Li, Jinliang Li, Junlin Lv, Jianying Zhu, Jiaqi Zhu Mingfa Feng, Ran Mei, Ruiguo Yang, Shenggui Li,\nShuyi Fan, Shuxi Guo, Triple Mu, Weitao Dai, Wenhao Zhang, Xi Chen, Xiao Jin, Xiaoyu Zhang (BBuf), Yihan Chen, Yikai Zhu, Yin Fan, Yuhao\nYang, Yuan Luo, Yueming Yuan, Yuhang Qi, Yuzhen Zhou, Zhiyi Liu, Zhuorui Liu, Ziyi Xu, Mick\n\nSpecial thanks to NVIDIA and Voltage Park for their compute support.\n\nSpecial thanks to AMD for their compute support and assistance in development.\n\n## Learn more\n\n- **Slack channel**: [#diffusion](https://sgl-fru7574.slack.com/archives/C09P0HTKE6A) (join via slack.sglang.io)\n- [**Cookbook for SGLang-Diffusion**](https://cookbook.sglang.io/docs/diffusion)\n- [**Documentation on SGLang-Diffusion**](https://github.com/sgl-project/sglang/blob/main/python/sglang/multimodal_gen/docs)\n","slug":"2026-01-16-sglang-diffusion"},"__N_SSG":true}