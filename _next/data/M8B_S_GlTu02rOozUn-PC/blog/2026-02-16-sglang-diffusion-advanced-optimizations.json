{"pageProps":{"frontmatter":{"title":"SGLang-Diffusion: Advanced Optimizations for Production-Ready Video Generation","author":"The SGLang-Diffusion Team","date":"February 16, 2026","previewImg":"/images/blog/sgl-diffusion/sgl-diffusion-banner-16-9.png"},"content":"\nFollowing our [two-month progress update](https://lmsys.org/blog/2026-01-16-sglang-diffusion/), we're excited to share a\ndeeper dive into the advanced optimizations that make SGLang-Diffusion a production-ready framework for video\ngeneration. These improvements focus on scalability, efficiency, and stability—essential for deploying diffusion models\nat scale.\n\nHere's what we've been working on:\n\n## Overview\n\nAs video generation models continue to grow in complexity, we've identified and addressed critical bottlenecks across\nthe entire inference pipeline:\n\n- **Smarter Parallelism**: Token-level sequence sharding and parallel folding for optimal resource utilization\n- **Distributed VAE**: Parallel encoding/decoding to eliminate memory bottlenecks for high-resolution video\n- **Production-Ready Serving**: Fixed Cache-DiT integration bugs for stable multi-request serving\n- **Optimized I/O**: Accelerated video save operations by eliminating unnecessary serialization\n- **Fused Kernels**: Custom JIT kernels for LayerNorm variants, reducing GPU bubbles\n\nLet's dive into the technical details.\n\n## Key Improvements\n\n### 1. SP-Sharding Improvement: From Frame-Level to Token-Level\n\nFor Video DiT models, input tensors typically have shape `B, T, H, W, C`. For a common configuration with\n`num_frames=81`, this might be: `1, 21, 90, 160, 3`.\n\nIn an 8×H100 setup with Ulysses Sequence Parallel (N=8), the framework needs to shard along the sequence dimension\nduring non-attention operations, then use all-to-all communication to switch to head dimension sharding for attention.\n\n#### Previous Approach: Frame-Level Sharding\n\nOur initial implementation sharded directly along the `T` (temporal) dimension. However, 21 frames cannot be evenly\ndivided by 8 GPUs, leading to two suboptimal solutions:\n\n1. **Adjust-frame**: Modify `num_frames` during preprocessing to make T divisible by N\n2. **Token Padding**: Pad the temporal dimension to the next multiple of N (21 → 24)\n\nThe frame-level padding approach introduces significant overhead: each padded token requires `H × W × C` redundant\ncomputations.\n\n#### New Approach: Token-Level Sharding\n\nTo minimize padding overhead, we now **flatten `T × H × W` into a single sequence dimension** before sharding. This has\ntwo major benefits:\n\n- **Reduced or Zero Padding**: For common resolutions and VAE configurations, `H × W` is often divisible by 8,\n  eliminating padding entirely\n- **Lower Communication Volume**: When padding is needed, the overhead is minimal compared to frame-level padding\n\n### Comparison: Shape and Comm Volume Analysis\n\n| Solution           | Padding Overhead | Input Tensor Shape (Per-rank) | All-to-All Comm Volume | \n|--------------------|------------------|-------------------------------|------------------------|\n| **Frame Sharding** | 3 frames (14.3%) | `3, 90, 160, C` (24/8)        | `1.0 × feature_map`    |\n| **Token Sharding** | 0 frames         | `2.625, 90, 160, C` (21/8)    | `0.875 × feature_map`  |\n\nThis optimization delivers both faster communication and reduced memory footprint, especially for video models.\n\nSee related [PR](https://github.com/sgl-project/sglang/pull/18161) for technical details.\n\n### 2. Parallel Folding: Decoupling Text Encoder and DiT Parallelism\n\nIn our original implementation, the Text Encoder and DiT shared the same Tensor Parallel (TP) group. When DiT used only\nSequence Parallel (SP), this meant the Text Encoder ran with TP=1—each GPU held a complete model copy, wasting memory\nand compute.\n\nSince Text Encoder and DiT computations are **completely decoupled**, we introduced **Parallel Folding**: the Text\nEncoder now uses the DiT's SP group as its TP group.\n\n**What this means in practice:**\n\n- **For Text Encoder**: Apply TP across the SP group to maximize speed and reduce memory\n- **For Denoiser**: Apply SP to optimize throughput and memory for sequence processing\n\nThis approach ensures both components use optimal parallelism strategies without interference, improving overall\nefficiency.\n\nSee related [PR](https://github.com/sgl-project/sglang/pull/17818) for technical details.\n\n### 3. Parallel VAE: Distributed Encoding/Decoding\n\nVAE encoding/decoding involves heavy 3D convolution operations. For high-resolution video, single-GPU implementations\nare slow and prone to OOM.\n\nThe two common approaches to alleviate this are:\n\n1. **Tiling**: Split feature maps into tiles, process them sequentially—reduces peak memory but increases latency\n2. **Parallel**: Distribute tiles across GPUs for concurrent processing—reduces both peak memory and latency\n\nWe implemented **Parallel VAE** for Wan-VAE with the following strategy:\n\n- **Height-wise Sharding**: Split feature maps along the height dimension across ranks\n- **Conv Operations**: Use `halo_exchange` to share boundary pixels between neighboring ranks (P2P), ensuring\n  mathematical\n  equivalence with global convolution\n- **Attention Operations**: Use `all_gather` for global context when needed\n- **Result Aggregation**: `all_gather` to reconstruct full height at the end of encoding/decoding\n\nThis approach eliminates VAE as a bottleneck for high-resolution video generation, enabling higher resolutions and\nlonger sequences without OOM.\n\n### 4. Serving with Cache-DiT: Fixing Multi-Request Stability\n\n[Cache-DiT](https://github.com/vipshop/cache-dit) in SGLang-Diffusion accelerates inference by caching residuals and\nskipping redundant\ncomputations. However, its correct operation depends on proper `num_inference_steps` configuration, which determines\nstep counting and the Selective Computation Mask (SCM).\n\n**The Problem:**\n\nWan2.2 uses a dual-transformer architecture, where `transformer` and `transformer_2` execute `num_high_noise_steps` and\n`num_low_noise_steps` respectively (summing to `num_inference_steps`). Our initial implementation had two critical bugs:\n\n1. Both transformers incorrectly used total `num_inference_steps` to configure their cache contexts\n2. In serving mode, cache contexts persisted across requests, even when different requests used different\n   `num_inference_steps`\n\nThese issues caused incorrect step counting and cache buffer contamination. When consecutive requests had different\nvideo shapes, cache buffers would encounter shape mismatches, **crashing the server**.\n\n**Our Solution:**\n\n1. `transformer` and `transformer_2` now use `num_high_noise_steps` and `num_low_noise_steps` respectively to configure\n   independent cache contexts\n2. For each new request, we recalculate timestep splits and **refresh** cache contexts using Cache-DiT's API, completely\n   isolating requests\n\nThis ensures stable, production-ready serving with Cache-DiT acceleration.\n\n### 5. Optimize Video Save: Eliminating Serialization Overhead\n\nIn our serving architecture, `scheduler_client` and `gpu_worker` communicate via ZMQ.\n\nPreviously, `gpu_worker` would:\n\n1. Complete inference\n2. Serialize output tensor\n3. Send tensor to `scheduler_client` via ZMQ\n4. `scheduler_client` deserializes tensor\n5. `scheduler_client` processes tensor and saves video\n\nThis introduced significant overhead from serialization/deserialization and memory copies.\n\n**Our Solution:**\n\n`gpu_worker` now directly processes the output tensor and saves the video to disk, returning only the file path to\n`scheduler_client`.\n\nThis eliminates serialization/deserialization overhead, while avoiding duplicate tensor copies.\n\n### 6. WanVideo LayerNorm Fusion: CuTeDSL JIT Kernels\n\nWanVideo introduces two specialized LayerNorm patterns:\n\n1. **LayerNormScaleShift**: `y = LN(x) * (1 + scale) + shift`\n\n2. **ScaleResidualLayerNormScaleShift**:\n    - `residual_out = residual + gate * x`\n    - `y = LN(residual_out) * (1 + scale) + shift`\n\nThese patterns combine elementwise operations with normalization reductions. Implementing them as separate kernels would\nintroduce multiple kernel launches and intermediate memory traffic, creating GPU bubbles.\n\n**Our Solution:**\n\nWe implemented **fused JIT kernels** using CuTeDSL (located in [`sglang/jit_kernel/diffusion/cutedsl`](https://github.com/sgl-project/sglang/tree/main/python/sglang/jit_kernel/diffusion/cutedsl)) that combine\nthese operations into single, efficient kernels.\n\n**Benefits:**\n\n- **Fewer Kernel Launches**: Reduced launch overhead\n- **Lower Memory Traffic**: Eliminates intermediate reads/writes\n- **Better GPU Utilization**: Reduces bubbles and improves throughput\n\nThese micro-optimizations add up, especially for multi-layer architectures like WanVideo.\n\n## Performance Results\n\nHere's a comparison of SGLang-Diffusion and LightX2V for Wan2.2 T2V under different settings:\n\n<iframe style=\"display:block; margin: auto;\" width=\"838\" height=\"523\" seamless frameborder=\"0\" scrolling=\"no\" src=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQRK_j_q8NXZKEqtrTBagxFxvvaxYXXB56HTqqYlD_aAv1v74WKle2HIc7HPK3P0ZVrYlZrjshKYnaV/pubchart?oid=677973346&amp;format=interactive\"></iframe>\n\n## What's Next\n\nWe continue to push the boundaries of diffusion model serving. Please refer to [**SGLang-Diffusion's Roadmap for 26Q1**](https://github.com/sgl-project/sglang/issues/18286) for more details.\n\nStay tuned for more updates as we continue to optimize SGLang-Diffusion for production deployments.\n\n## Acknowledgment\n\n- We would like to thank the following contributors for their work on these optimizations:\n  **Skywork.ai, [Song Rui](https://github.com/Songrui625), SGLang-Diffusion Team**\n- Special thanks to our compute partners for their continued support.\n\nTry diffusion generation, proudly powered by SGLang-Diffusion: [APIFree](https://www.apifree.ai/home)\n\n## Learn More\n\n- **Slack channel**: [#diffusion](https://sgl-fru7574.slack.com/archives/C09P0HTKE6A) (join via slack.sglang.io)\n- [**Cookbook for SGLang-Diffusion**](https://cookbook.sglang.io/docs/diffusion)\n- [**Documentation on SGLang-Diffusion**](https://github.com/sgl-project/sglang/blob/main/python/sglang/multimodal_gen/docs)\n- [**Previous Update: Two Months In**](https://lmsys.org/blog/2026-01-16-sglang-diffusion/)\n","slug":"2026-02-16-sglang-diffusion-advanced-optimizations"},"__N_SSG":true}