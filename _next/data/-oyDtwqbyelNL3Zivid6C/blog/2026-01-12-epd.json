{"pageProps":{"frontmatter":{"title":"EPD Disaggregation: Elastic Encoder Scaling for Vision-Language Models in SGLang","author":"rednote hilab, Alibaba Cloud Computing, AntGroup SCT","date":"January 12, 2026","previewImg":"/images/blog/epd/epd_preview.png"},"content":"\n## TL;DR\n\n> We introduce Encoder-Prefill-Decode (EPD) Disaggregation in SGLang, a novel architecture that separates vision encoding from language processing in Vision-Language Models (VLMs).  This can enable:\n\n- **Independent scaling of vision encoding capacity**: Encoder servers can be scaled horizontally without affecting language model deployment, enabling better resource utilization for vision-heavy workloads.\n- **Compatibility with existing PD disaggregation**: EPD can be combined with Prefill-Decode disaggregation for a complete three-tier architecture.\n- **Flexible transfer backends**: Support for multiple transfer mechanisms (ZMQ, GPU-direct via Mooncake) allows optimization for different deployment scenarios.\n- **Vision embedding caching**: Frequently used images can be cached at encoder servers, eliminating redundant ViT computations and reducing network transfer overhead.\n\nEPD is highly effective in **image-heavy scenarios** (e.g., multi-image inputs), where the visual encoding process is the primary computational bottleneck. For instance, in these scenarios, we leverage EPD to significantly reduce request TTFT under load—achieving approximately 6–8× lower latency compared to the colocation approach at 1 QPS. Conversely, for **image-light scenarios** with few images, EPD may be less efficient or even counterproductive. This is because the additional network latency incurred by transmitting embeddings across nodes can outweigh the time saved by offloading the encoding task, potentially resulting in a higher TTFT compared to a colocation approach.\n\n## Introduction\nVision-Language Models (VLMs) like Qwen2.5-VL and Llama-Vision combine visual understanding with language generation. However, these models face unique scaling challenges:\n\n- Heterogeneous compute needs: Vision encoding (CNN/ViT) requires different computational patterns than language decoding (Transformer)\n- Imbalanced resource usage: Vision processing is compute-intensive but only needed during prefill\n- Limited flexibility: Traditional monolithic deployments can't independently scale vision and language components\n- Intra-request parallelism: Different images in one request can be encoded independently. \n- Poor scaling under tensor parallelism: Because the vision encoder has far fewer parameters than the language component, applying tensor parallelism to it is inefficient and generally unnecessary.\n\nSGLang's existing Prefill-Decode (PD) disaggregation already separates prefill from decode phases. EPD extends this by further separating vision encoding from language prefill, creating a three-tier architecture.\n\n## The ViT Scaling Problem: Why Tensor Parallelism Doesn't Always Help\n\n### The Counter-Intuitive Finding\n\nOne of the key insights from EPD disaggregation is that Vision Transformers (ViT) **do NOT** benefit from increased Tensor Parallelism, and can actually become slower with higher TP:\n\nBenchmark on H20 with Qwen2.5-VL-72B (4 images per request):\n\n| tp  | vit mean time |\n|-----|---------------|\n| 2   | 492.13ms      |\n| 4   | 465.80ms      |\n| 8   | 523.80ms      |\n\nWhy Does This Happen?\n\n1. Communication overhead dominates execution time.\n2. The weight parameters of vision models are usually small.\n\nEPD sidesteps this by scaling encoders horizontally instead of increasing TP.\n\n### Architecture Overview\nThe EPD architecture follows a request flow:\n\n1. **Client Request**: A multimodal request arrives at the prefill server (via load balancer or direct connection).\n2. **Image Distribution**: The prefill server identifies image inputs and distributes them to one or more encoder servers. Images can be split across multiple encoders for load balancing.\n3. **Vision Encoding**: Encoder servers process images through ViT, generating vision embeddings and image grid metadata. Results are cached if enabled.\n4. **Embedding Transfer**: Vision embeddings are transferred back to the prefill server using the configured transfer backend (ZMQ, Mooncake, etc.).\n5. **LLM Computation**: The prefill server combines vision embeddings with text tokens to form mm_inputs containing pre-computed tensors. The LLM performs Prefill and Decode computation based on these embeddings. If PD disaggregation is enabled, the existing Prefill-Decode transfer logic is reused; otherwise, token generation happens directly on the prefill server.\n\n### Key Components\n\n<p align=\"center\">\n  <img src=\"/images/blog/epd/epd_workflow.png\" alt=\"EPD Workflow\" width=\"50%\" />\n</p>\n\n<p align=\"center\">\n  <img src=\"/images/blog/epd/epd_architecture.svg\" alt=\"EPD Architecture\" width=\"50%\" />\n</p>\n\n\n\n\n**Encoder Server** (--encoder-only)\n- Vision-only (no language weights); preprocessing + ViT forward to generate vision embeddings\n- Prefix multimodal cache support\n- Scale out for load balancing and parallel multi-image split inference\n\n**Prefill Server** (--language-only)\n- Language model only\n- Receives embeddings from encoder(s)\n- With PD: ships KV to Decode; without PD: decodes locally  \n\n**Decode Server**\n- Standard decode-only instance\n- Receives KV cache from prefill\n\n### Implementation Details\n\n#### Image Distribution Strategies\nUnlike tensor parallelism which splits a single model across GPUs, EPD uses data parallelism by running multiple independent encoder instances and distributing images among them.\n\nExample distribution:\n\n```\nRequest with 7 images: [img0, img1, img2, img3, img4, img5, img6]\n3 encoders available\n\nDistribution (after shuffle):\n├─ Encoder 0: [img0, img1, img2] (3 images)\n├─ Encoder 1: [img3, img4] (2 images)\n└─ Encoder 2: [img5, img6] (2 images)\n```\n\n#### Transfer Backends\n\nEPD supports three transfer backends for vision embeddings:\n\n**zmq_to_scheduler (Default)**\n- Direct ZMQ socket communication\n- Embeddings sent from encoder to scheduler via RDMA transfer engine.\n- No blocking \n\n**zmq_to_tokenizer**\n- Embeddings sent to tokenizer manager\n- Processed during tokenization phase\n\n**mooncake**\n- RDMA-based transfer for multi-node\n- Registers embeddings in shared memory\n- High-bandwidth, low-latency transfer\n\n\n#### Vision Embedding Cache\nThe encoder supports prefix multimodal caching to avoid redundant ViT computations:\n- Eliminates redundant vision encoding\n- Reduces latency for repeated images\n- Configurable cache size (default 4GB via SGLANG_VLM_CACHE_SIZE_MB)\n\n### Usage Examples\n- To launch the encoder instance\n```\nMODEL=Qwen/Qwen2.5-VL-7B-Instruct\nPORT=30002\n\nCUDA_VISIBLE_DEVICES=2 taskset -c $1 python -m sglang.launch_server \\\n    --model-path $MODEL \\\n    --encoder-only \\\n    --enable-prefix-mm-cache \\\n    --port $PORT\n```\n- To launch the prefill instance\n```\nMODEL=Qwen/Qwen2.5-VL-7B-Instruct\nPORT=30000\nTP=1\nMEM_FRACTION=0.5\nCHUNK_SIZE=8192\n\nSGLANG_VLM_CACHE_SIZE_MB=0 CUDA_VISIBLE_DEVICES=0 python -m sglang.launch_server \\\n    --model-path $MODEL \\\n    --disaggregation-mode prefill \\\n    --disaggregation-transfer-backend nixl \\\n    --tp $TP \\\n    --mem-fraction-static $MEM_FRACTION \\\n    --disable-radix-cache \\\n    --chunked-prefill-size $CHUNK_SIZE \\\n    --language-only \\\n    --encoder-urls http://127.0.0.1:30002 http://127.0.0.1:30003 http://127.0.0.1:30004 http://127.0.0.1:30005 http://127.0.0.1:30006 http://127.0.0.1:30007 \\\n    --port $PORT\n```\n- To launch the decode instance\n```\nMODEL=Qwen/Qwen2.5-VL-7B-Instruct\nPORT=30001\nTP=1\n\nCUDA_VISIBLE_DEVICES=1 python -m sglang.launch_server \\\n    --model-path $MODEL \\\n    --disaggregation-mode decode \\\n    --disaggregation-transfer-backend nixl \\\n    --tp $TP \\\n    --port $PORT\n```\n\n- To launch minlb\n```bash\npython -m sglang_router.launch_router \\\n  --pd-disaggregation \\\n  --mini-lb \\\n  --prefill http://127.0.0.1:30000 \\\n  --decode http://127.0.0.1:30001 \\\n  --port 8000\n```\n## Benchmark\nEPD disaggregation targets vision-heavy workloads (multi-image requests) and improves Time To First Token (TTFT) by scaling encoder servers horizontally.\nLaunch bench script:\n```\npython -m sglang.bench_serving \\\n    --random-image-count \\\n    --model ${MODEL_PATH} \\\n    --num-prompts 64 \\\n    --dataset-name image \\\n    --random-input-len 128 \\\n    --random-output-len 256 \\\n    --image-count 8 \\\n    --image-resolution 1080p \\\n    --host $HOST_IP \\\n    --port $port \\\n    --backend vllm-chat \\\n    --request-rate $request_rate \n```\n\n### Experimental Setup\n\n**Environment**: 8× H20 96GB GPUs\n\n**Model**: Qwen3-VL-235B-A22B-Instruct-FP8\n\n**Dataset**: Random multimodal dataset\n- Text tokens: 128 / 256\n- Images per request: 1-8 images (random count, average ~4 images)\n- Image resolution: 1080p\n- QPS range: 0.2-1.0\n\n**Deployment Configurations**:\n- **Colocate**: 1 PD instance with tensor-parallel-size=4, uses 4× H20 GPUs\n- **1E1P** (1 Encoder + 1 PD instance): 1 Encoder with tensor-parallel-size=1 + 1 PD instance with tensor-parallel-size=4, uses 5× H20 GPUs\n- **2E1P** (2 Encoders + 1 PD instance): 2 Encoders with tensor-parallel-size=1 each + 1 PD instance with tensor-parallel-size=4, uses 6× H20 GPUs\n\n\n### Bench Results\n\nMean TTFT (EPD vs colocate)\n\n<p align=\"center\">\n  <img src=\"/images/blog/epd/epd_vs_colocate_ttft.png\" alt=\"TTFT Results\" width=\"50%\" />\n</p>\n\nMean TPOT (EPD vs colocate)\n\n<p align=\"center\">\n  <img src=\"/images/blog/epd/epd_vs_colocate_tpot.png\" alt=\"TPOT Results\" width=\"50%\" />\n</p>\n\n> The higher TPOT observed in 2e1p is attributed to its larger batch size during decoding.\n\nRequest Throughput (EPD vs colocate)\n\n<p align=\"center\">\n  <img src=\"/images/blog/epd/epd_vs_colocate_throughput.png\" alt=\"Throughput Results\" width=\"50%\" />\n</p>\n\nKey Findings (vs. colocate)\n- Encoder/prefill keeps TTFT much lower under load (≈6–8x lower than colocate at 1 qps).\n- TPOT stays far below colocate (≈8–10x lower), indicating much tighter latency.\n- Throughput roughly doubles at higher QPS (≈2x at 0.8–1.0 qps vs. colocate).\n- The dramatic reduction in TTFT is achieved by allocating dedicated GPU resources to the encoder. Despite using 50% additional GPUs (2E1P uses 6× H20 vs. colocate's 4× H20), EPD disaggregation achieves a much higher return on investment (ROI) compared to simply scaling the traditional integrated architecture, with performance gains exceeding the additional resource cost.\n- While highly effective for image-heavy tasks, dedicated encoder GPUs may experience lower utilization (idle time) in image-light scenarios, suggesting that EPD is most resource-efficient when visual processing is the primary bottleneck.\n\n\n\n## Acknowledgment\n\n**rednote hilab**: [Tianyu Guo](https://github.com/gty111), a'du, shuming\n\n**Alibaba Cloud Computing**: [Siyu Liu](https://github.com/liusy58), [Shangming Cai](https://github.com/ShangmingCai)\n\n**AntGroup SCT**: [Wengang Zheng](https://github.com/ZhengWG)\n\n## Learn more\n\n- Roadmap: [Encoder Disaggregation (2025 Q4)](https://github.com/sgl-project/sglang/issues/15118)\n","slug":"2026-01-12-epd"},"__N_SSG":true}