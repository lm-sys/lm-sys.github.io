{"pageProps":{"frontmatter":{"title":"Towards Deterministic Inference in SGLang and Reproducible RL Training","author":"The SGLang Team","date":"September 22, 2025 (Updated on September 24)","previewImg":"/images/blog/deterministic/logo.png"},"content":"\n\n**TL;DR**: This post shares our efforts to enable deterministic inference in SGLang and our collaboration with [slime](https://github.com/THUDM/slime) to work towards reproducible RL training.\n\n<br />\n\n\nRecently, the Thinking Machines Lab published a [blog](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/) detailing their findings. Since this blog was published, the industry has responded enthusiastically, eagerly awaiting open-source inference engines to implement stable and usable deterministic inference, or even further, to achieve fully reproducible RL training. Now, SGLang and slime together provide the answer.\n\nBuilding on Thinking Machines Lab's batch-invariant operators, SGLang achieves fully deterministic inference while maintaining compatibility with **chunked prefill**, **CUDA graphs**, **radix cache**, and **non-greedy sampling**. With CUDA graphs, SGLang delivers **2.8x acceleration** and reduces performance overhead to just **34.35%** (vs. TML's **61.5%**).\n \nTaking this deterministic inference capability further, SGLang collaborated with the slime team to unlock **100% reproducible RL training** - a breakthrough achieved with minimal code changes. Our validation experiments on Qwen3-8B demonstrate perfect reproducibility: **two independent training runs produce identical curves**, providing the reliability needed for rigorous scientific experimentation.\n\n![slime](/images/blog/deterministic/slime.png)<small><center>[*Reproducible Guide*](https://thudm.github.io/slime/_examples_synced/reproducibility/README.html#reproducibility)</center></small>\n\n\n<br />\n\nNow let's dive into the some technical details.\n\n## Why Deterministic Inference Matters\n\nThe ability to achieve consistent outputs from large language models (LLMs) inference is increasingly important. For example, the indeterminism of inference results can implicitly transform on-policy reinforcement learning (RL) into off-policy RL as [researchers pointed out](https://fengyao.notion.site/off-policy-rl). However, even if we turn the temperature down to 0 in SGLang, the sampling is still not deterministic due to the use of dynamic batching and radix cache (past discussions [here](https://docs.sglang.ai/references/faq.html#the-results-are-not-deterministic-even-with-a-temperature-of-0)) .\n\nAs mentioned in the [blog](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/), the largest source of nondeterminism is the varying batch sizes: Even when a user repeatedly submits the same prompt, the output can vary across runs, since the request may be batched together with other users' requests, and differences in batch size can lead to nondeterministic inference results.\n\nTo explain more, different batch sizes will influence the reduction splitting process of kernels. This leads to  varying order and size for each reduction block, which can cause nondeterministic outputs due to the non-associativity of floating-point arithmetic. To fix this, they replaced reduction kernels (RMSNorm, matrix multiplication, attention, etcâ€¦) with a batch-invariant implementation. These kernels were also released as [a companion library](https://github.com/thinking-machines-lab/batch_invariant_ops) for external integration. \n\n![figure1](/images/blog/deterministic/deterministic_intro.png)<small><center>*He, Horace and Thinking Machines Lab, \"Defeating Nondeterminism in LLM Inference\", \nThinking Machines Lab: Connectionism, Sep 2025.*</center></small>\n\n\nBuilding on the work of Thinking Machines Lab, SGLang delivers a robust, high-throughput solution for deterministic LLM inference, combining batch-invariant kernels, CUDA graphs, radix cache, and chunked prefill with efficient performance. Determinism has been extensively validated through comprehensive tests and RL training experiments.\n\nKey enhancements include:\n- **Integration of batch-invariant kernels** from Thinking Machines Lab, including mean, log-softmax, and matrix multiplication kernels.\n- **Implementation of batch-invariant attention kernels** with fixed split-KV size. Multiple backends are supported, including FlashInfer, FlashAttention 3, and Triton.\n- **Full compatibility with common inference features**, such as chunked prefill, CUDA graph, radix cache, all of which remain supported when deterministic inference is enabled.\n- **Expose a per-request seed** in sampling arguments, allowing users to enable deterministic inference even when temperature > 0.\n- **Better performance**: Compared to the **61.5%** slowdown reported in TML's blog, SGLang achieves an average slowdown of only **34.35%** with the FlashInfer and FlashAttention 3 backends, representing a significant improvement. With CUDA graphs, 2.8x speedup can be achieved compared to the minimal integration.\n\n\n## Results\n\n\n### Verifying Deterministic Behavior\n\nWe introduce [a deterministic test suite](https://github.com/sgl-project/sglang/blob/f1d789231896da438749b395f7bf007a5b0819c0/python/sglang/test/test_deterministic.py) to verify whether inference results remain consistent under different batching conditions. The test covers three subtests, progressing from simple to more challenging:\n\n- Single: Run the same prompt across varying batch sizes and check if outputs remain identical.\n- Mixed: Mix different types of prompts (short prompts and long prompts) within the same batch and verify consistency.\n- Prefix: Use prompts derived from the same long text with different prefix lengths, batch them randomly, and test whether results are reproducible across trials.\n\nHere are the results from 50 sampling trials. The numbers indicate the count of unique outputs observed for each subtest (lower = more deterministic).\n\n| Attention Backend | Mode | Single Test | Mixed Test (P1/P2/Long) | Prefix Test (prefix_len=1/511/2048/4097) | \n| --- | --- | --- | --- | --- |\n| FlashInfer | Normal | 4| 3 / 3 / 2 | 5 / 8 / 18 / 2 |\n| FlashInfer | Deterministic | 1 | 1 / 1 / 1 | 1 / 1 / 1 / 1 |\n| FA3 | Normal | 3 | 3 / 2 / 2 | 4 / 4 / 10 / 1 |\n| FA3 | Deterministic | 1 | 1 / 1 / 1 | 1 / 1 / 1 / 1 |\n| Triton | Normal | 3 | 2 / 3 / 1 | 5 / 4 / 13 / 2 |\n| Triton | Deterministic | 1 | 1 / 1 / 1 | 1 / 1 / 1 / 1 |\n---\n<small>*Tested on QWen3-8B</small>\n\n<small>* Cuda graph, chunked prefill are enabled. Radix cache is disabled for Flashinfer and Triton since their support is still in progress. </small>\n\n\n### CUDA Graph Acceleration \n\nCUDA graphs can accelerate the inference process by consolidating multiple kernel launches into a single launch. Our evaluation compared the total throughput of deterministic inference with and without CUDA graphs enabled. The test workload consisted of 16 requests, each with an input length of 1024 and an output length of 1024. The results show an at least 2.79x speedup across all attention kernels when CUDA graphs is utilized.\n\n| Attention Backend | CUDA Graph | Throughput (tokens/s) |\n| --- | --- | --- |\n| FlashInfer | Disabled | 441.73 |\n| FlashInfer | Enabled | 1245.51 (2.82x) |\n| FA3 | Disabled | 447.64 |\n| FA3 | Enabled | 1247.64 (2.79x) |\n| Triton | Disabled | 419.64 |\n| Triton | Enabled | 1228.36 (2.93x) |\n---\n<small>*Setup: QWen3-8B, TP1, H100 80GB  </small>\n\n<small>*We disabled radix cache for all performance benchmarks since FlashInfer and Triton Radix Cache support is still in progress. </small>\n\n### Measuring Offline Inference Performance\n\nWe measured end-to-end latency for both non-deterministic and deterministic modes using three common RL rollout workloads (256 requests with varying input/output lengths).\n\nDeterministic inference is generally usable, with most slowdowns ranging from 25% to 45%, and average slowdown of FlashInfer and FlashAttention 3 backends being 34.35%. The majority of this overhead comes from unoptimized batch-invariant kernels (matrix multiplication and attention), indicating significant room for performance improvements.\n\n| Attention Backend | Mode | Input 1024 Output 1024| Input 4096 Output 4096 | Input 8192 Output 8192 | \n| --- | --- | --- | --- | --- |\n| FlashInfer | Normal | 30.85 | 332.32 | 1623.87 |\n| FlashInfer | Deterministic | 43.99 (+42.6%) | 485.16 (+46.0%) | 2020.13 (+24.4%) |\n| FA3 | Normal | 34.70 | 379.85 | 1438.41 |\n| FA3 | Deterministic | 44.14 (+27.2%) | 494.56 (+30.2%) | 1952.92 (+35.7%) |\n| Triton | Normal | 36.91 | 400.59 | 1586.05  |\n| Triton | Deterministic | 57.25 (+55.1%) | 579.43 (+44.64%) | 2296.60 (+44.80%) |\n---\n<small>*Setup: QWen3-8B, TP1, H200 140GB. </small>\n\n<small>*We disabled radix cache for all performance benchmarks since FlashInfer and Triton Radix Cache support is still in progress. </small>\n\nWe acknowledge that deterministic inference is significantly slower than normal mode. We recommend using it primarily for debugging and reproducibility. Future work will focus on accelerating deterministic inference, with the goal of reducing the performance gap to under 20%, or ideally achieving parity with normal mode.\n\n## Usage\n\n### Environment Setup\n\nTo set up the environment, install SGLang with version >=0.5.3\n```bash\npip install \"sglang[all]>=0.5.3\"\n```\n### Launching the Server\n\nSGLang supports deterministic inference across multiple models. For example, with Qwen3-8B you only need to add the `--enable-deterministic-inference` flag when launching the server:\n\n```bash\npython3 -m sglang.launch_server \\\n    --model-path Qwen/Qwen3-8B \\\n    --attention-backend <flashinfer|fa3|triton> \\\n    --enable-deterministic-inference\n```\n\n## Technical Details\n\n\n### Chunked Prefill\n\nSGLang's chunked prefill technique is designed to manage requests with long contexts. However, its default chunking strategy violates the determinism requirement for attention kernels.  \n\nAs illustrated in the figure, consider two input sequences, `seq_a` and `seq_b`, each with a context length of 6,000. The maximum chunk size for chunk prefill is 8192, while the required split-KV size for deterministic attention is 2,048. Each sequence can be partitioned into three smaller units (`a1` to `a3` and `b1` to `b3`), with lengths of 2,048, 2,048, and 1,904, respectively. If these smaller units remain intact during chunk prefilling, then they can be processed by the same attention kernel and lead to deterministic reduction behavior.\n\n\n<img src=\"/images/blog/deterministic/chunked_prefill.png\" style=\"width: 30vw; min-width: 200px;\" />\n\n\nThe standard chunking strategy operates on a \"best-effort\" principle. In this example, this strategy tries to generate a `chunk_1` of 8,192 tokens by splitting the `b2` unit of `seq_b` into two smaller parts. This can cause inconsistent truncation points since the length of `b2` after splitting depends on the length of `seq_a`. To address this, we adapted the chunking logic to **align the truncation point with an integer multiple of the split_kv_size**. This adjustment ensures that the processing of `b2` is deferred to a subsequent chunk, allowing it to be computed as a complete unit by the attention kernel. \n\n### Attention Backends\n\nAttention kernel is an important part of determinism. For different attention backends, we modified them in different ways to satisfy their usage requirements.\n- For Flashinfer backend, we utilize the `fixed_split_size` and `disable_kv_split` arguments from [batch invariant FA2 kernels](https://github.com/flashinfer-ai/flashinfer/pull/1675) to fix split sizes during kernel planning. Truncation of chunked prefill is aligned to the prefill split size. ([PR link](https://github.com/sgl-project/sglang/pull/10645))\n- For FlashAttention-3 backend, num-splits of flash attention kernel are fixed to 1 to ensure determinism. ([PR link](https://github.com/sgl-project/sglang/pull/10651))\n- For Triton backend, we fix the split size of decoding, and manually set the alignment size of chunked prefill. Deterministic inference can also run on **AMD** hardware with the extensibility of Triton backend. ([PR link](https://github.com/sgl-project/sglang/pull/10694)). \n\n\n### Reproducible Non-Greedy Sampling\nTo extend determinism beyond greedy decoding, we introduce a new sampling function: [multinomial_with_seed](https://github.com/sgl-project/sglang/blob/e2ac7888b8cb1fd6c33a7ec58d27a5f5b5b24e0c/python/sglang/srt/layers/sampler.py#L268-L299).\n\nInstead of relying on `torch.multinomial`, which is inherently nondeterministic under batching, this operator perturbs logits with Gumbel noise generated from a **seeded hash function**. As a result, the same `(inputs, seed)` pair always yields the same sample, even when temperature > 0.\n\n\nThis modification enables **deterministic multinomial sampling** while preserving the stochasticity required by reinforcement learning rollouts.\n\n\n### RL Framework Integration (slime)\n\nWe [integrated](https://github.com/THUDM/slime/pull/361) deterministic inference with temperature > 0 into slime's GRPO training recipe. In preliminary experiments, repeated RL training runs produced **identical rollout responses and loss values for the first iterations**, confirming that the rollout process itself is deterministic. \n\nIn a follow-up [PR](https://github.com/THUDM/slime/pull/370), we further enabled full training reproducibility by implementing the following key configurations:\n\n- **Flash Attention**: Use Flash Attention v2 instead of v3 to enable deterministic backward passes\n- **Megatron**: Set `--deterministic-mode` flag for deterministic training\n- **Environment Variables**: Configure critical settings:\n  - `NCCL_ALGO=Ring`\n  - `NVTE_ALLOW_NONDETERMINISTIC_ALGO=0`\n  - `CUBLAS_WORKSPACE_CONFIG=:4096:8`\n- **PyTorch**: Enable `torch.use_deterministic_algorithms(True, warn_only=False)`\n\nWith these comprehensive changes, we successfully achieved full training reproducibility for GRPO in slime, enabling truly deterministic end-to-end RL training pipelines.\n\n\n## Future Work\nOur future efforts will focus on enhancing deterministic inference by addressing the following key areas:\n- **Faster batch invariant kernels**: Batch invariant kernels are the bottleneck of performance, so we'll work on optimizing their configurations and potentially rewriting them to boost performance. This is also critical for improving the speed of RL rollouts.\n- **Support for MoE models**: Currently we only support deterministic inference for dense models like QWen3-8B or LLaMa-3.1-8B. In the future we plan to expand our support to MoE models like Qwen3-30B-A3B or DeepSeek-V3.\n- **True On-Policy RL**: We plan to further integrate deterministic inference into reinforcement learning frameworks (e.g., [slime](https://github.com/THUDM/slime)) to enable reproducible sampling, with the ultimate goal of achieving true on-policy training.\n- **Enhancing Radix Cache Functionality**: We will improve the radix tree to enable compatibility with a wider variety of attention kernels, moving beyond current limitation to the FlashAttention 3 backend.\n- **Tensor Parallelism**: TP1 and TP2 are deterministic due to consistent floating-point addition order; larger TP setups require modifications to reduce kernels for determinism.\n- **FlexAttention Integration**: Besides currently supported attention backends, we plan to extend our support of deterministic inference to FlexAttention in the future.\n- A **roadmap** for deterministic inference features can be found in [this issue](https://github.com/sgl-project/sglang/issues/10278).\n\nSGLang's deterministic inference and slime's reproducible training capabilities are currently under active development and improvement. We sincerely welcome users and developers to actively try out these features and provide us with valuable feedback. Your experience and suggestions will help us further optimize these important capabilities and advance the development of deterministic inference technology. \n\n## Acknowledgement\nWe would like to extend our heartfelt gratitude to the following teams and collaborators:\n- **SGLang team and community**: Baizhou Zhang, Biao He, Qiaolin Yu, Xinyuan Tong, Ke Bao, Yineng Zhang, Chi Zhang, Ying Sheng, Lianmin Zheng and many others\n- **Flashinfer team and community**:  Wenxuan Tan, Yilong Zhao, Zihao Ye\n- **slime team and community**: Zilin Zhu\n- **AMD**: Yusheng Su\n- **Thinking Machines Lab**: for their awesome [blog](https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/) and [batch_invariant_ops library](https://github.com/thinking-machines-lab/batch_invariant_ops)","slug":"2025-09-22-sglang-deterministic"},"__N_SSG":true}