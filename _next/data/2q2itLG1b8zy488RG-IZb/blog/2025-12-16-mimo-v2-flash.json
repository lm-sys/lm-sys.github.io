{"pageProps":{"frontmatter":{"title":"SGLang Day-0 Support for MiMo-V2-Flash Model","author":"SGLang Team and Xiaomi LLM Core Team","date":"December 16, 2025","previewImg":"/images/blog/mimo-v2-flash/decode_1.png"},"content":"\n## Introduction\nMiMo-V2-Flash, with 309B total parameters and 15B activated parameters, is a new inference-centric model designed to maximize decoding efficiency. It is based on two key designs: **sliding window attention** and **multi-layer MTP**. MiMo-V2-Flash is explicitly co-designed for real-world serving workloads, enabling flexible tradeoffs between throughput and latency on different hardware. Combined with SGLang’s optimized Spec v2 runtime, which provides near-zero-overhead support for multi-layer MTP and efficient SWA execution, MiMo-V2-Flash delivers balanced TPOT and throughput on H200. In this blog, we will introduce the model and SGLang's efficient support.\n\n## Inference-Efficient Modeling\nThe design of MiMo-V2-Flash follows an inference-efficiency principle. The MiMo-V2-Flash adopted two critical designs:\n\n1. Sliding Window Attention (SWA): In SWA, each token’s receptive field is limited to a fixed, constant-sized window, to reduce the attention's complexity on the sequence dimension.\n2. MTP: MiMo-V2-Flash's multi-layer MTP uses a chain of prediction heads, where each head sequentially predicts the next token. The resulting draft tokens are then verified in parallel in the following step using an extended query.\nThe overview of MiMo-V2-Flash is shown in the figure below:\n\n![figure1](/images/blog/mimo-v2-flash/overview.PNG)<small><center>MiMo-V2-Flash Overview</center></small>\n\nNow let's see how those designs lead to a cost-efficient inference.\n\n### SWA\nIn MiMo-V2-Flash, every five attention layers with a sliding window pattern are alternated with one dense GQA. The wide use of SWA can benefit the inference from multiple perspectives. First, during the prefilling stage, compute dominates the cost. Especially when the sequence is long, $O(N^2)$ attention computation is the bottleneck. SWA reduces the $O(N^2)$ complexity to a linear level to sequence length, $O(Nw)$, where $w$ is the window size. In a long context scenario, this design can significantly reduce the TTFT. SWA also reduces KV cache complexity to a constant level - releasing more resources for a larger batch size, and allows a better TPOT through fewer KV cache loading operations. \n\nThe figure below shows the prefill benchmarking results for MiMo-V2-Flash. \n\n![figure2](/images/blog/mimo-v2-flash/prefill.PNG)<small><center>MiMo-V2-Flash Prefill Benchmark (Radix Cache Disabled)</center></small>\n\n### MTP\nOne of the most important designs in MiMo-V2-Flash is the multi-layer MTP, with 3 MTP layers. \n\nIn decoding scenarios, most of the kernels are memory-bound. Since the query length is always 1, using a larger number of parallel decoding tokens is the most intuitive way to achieve higher throughput. \n\nHowever, as the batch size increases to a certain level, this effect will be restricted - the KV cache memory access also grows linearly with the batch size, and it performs as the memory-bounded bottleneck. At this time, the device's computing potential is still not fulfilled, but it's hard to increase throughput by increasing batch size. \n\nMTP can still leverage this underexploited compute to reduce the TPOT. In MTP, multiple tokens are generated at the same time by sequential prediction heads, and the tokens will be verified in parallel in the same query, increasing the query length. This will not trigger more KV cache access; it will always increase arithmetic intensity. When the inference is still heavily memory-bound, and the batch size's effect has been marginal, an aggressive MTP strategy with a satisfying acceptance rate can theoretically leverage the rest of the device's potential and achieve a better TPOT.\n\n## Hardware-Aware MTP Configuration\nSince MTP benefits from an unsaturated arithmetic intensity, and GQA's arithmetic computation is low - MiMo-V2-Flash attention design is natively well-suited for multi-layer MTP. However, when deploying MiMo-V2-Flash, choosing the right combination of batch size and MTP depth is still essential for achieving the optimal compute–memory balance and maximizing performance across different hardware platforms. Theoretically, we want to choose the best tradeoff, which achieves a satisfying throughput and TPOT simultaneously. The sweet spot of this trade-off depends on the hardware, because each hardware platform has its own roofline model. \n\nGenerally speaking, devices with a higher roofline benefit more from aggressive MTP because they have abundant compute capacity that is harder to saturate in memory-bound decoding. In contrast, inference-oriented accelerators, e.g., H20, have comparatively limited FLOPs, and the usage of MTP should be more careful: aggressive MTP depth can push the workload to compute-bound and degrade the throughput.\n\nHere, we provide the benchmarking results on H200. MiMo-V2-Flash achieves balanced performance in both throughput and per request TPS. Thanks to SWA and MTP, the per request decoding throughput remains at 150 TPS even under long-context settings of up to 64K input tokens with per DP rank batch size 16.\n\n![figure3](/images/blog/mimo-v2-flash/decode_1.png)<small><center>MiMo-V2-Flash Decode Benchmark (DP 2, TP 4, EP 8, MTP Accept Length 3.6, Input Token Length 16k, Varying Batch Size)</center></small>\n\n![figure4](/images/blog/mimo-v2-flash/decode_2.png)<small><center>MiMo-V2-Flash Decode Benchmark (DP 2, TP 4, EP 8, MTP Accept Length 3.6, Per DP Rank Batch Size 16, Varying Input Token Length)</center></small>\n\n## Fast MTP Serving with SGLang Spec v2\nMiMo’s multi-layer MTP is implemented natively on SGLang’s spec v2. We apply the fully overlapped MTP feature to improve throughput and latency, delivering faster MTP serving. In spec v2, the overlap scheduler is fused with speculative decoding: output sync/processing is delayed while the next batch’s kernels launch early, so CPU overhead for batching and syncing is hidden in GPU forward. This cuts GPU bubbles and improves throughput and latency.\n\nThe figure below is a screenshot of the profiling, showing the overlapped decoding process with spec v2.\n\n![figure4](/images/blog/mimo-v2-flash/profile.png)<small><center>Overlapped Speculative Decoding Profile</center></small>\n\n## More Discussions\nIn most LLM-serving workloads, the decoding stage is memory-bounded, leaving substantial compute underutilized, particularly on the mainstream training-oriented GPUs. While inference-specific accelerators with high bandwidth and lower FLOPs offer a cost-efficient choice, their speed is limited. MiMo-V2-Flash attempts to take another perspective to make the model itself inference-efficient. The multi-layer MTP model may be a generalizable solution - if the acceptance rate can be further optimized, it allows people to leverage their GPU's computation to achieve faster decoding. With a more adaptable architecture, hardware selection becomes more flexible: each device can operate at its own optimal compute–memory balance point. This opens the possibility of using the same class of hardware for both training and inference, simplifying deployment and reducing overall system cost.\n\nMiMo-V2-Flash support is already available in SGLang via PR ([#15207](https://github.com/sgl-project/sglang/pull/15207), [#15208](https://github.com/sgl-project/sglang/pull/15208)) and will be merged into the main branch shortly. The benchmarks in this blog were conducted on MiMo’s optimized branch, and the corresponding optimizations will be upstreamed into SGLang main in the near future.\n\n## Getting Started\n\nMiMo-V2-Flash is currently available in SGLang via Docker image and pip install. Please see the instructions below to launch the SGLang server and start using MiMo-V2-Flash.\n\nSee Instructions below:\n\n<br>\n\n<details>\n<summary><span style=\"font-size: 1.3em; font-weight: bold;\">Docker</span></summary>\n\n```bash\n# Pull the docker image\ndocker pull lmsysorg/sglang:dev-pr-15207\n\n# Launch the container\ndocker run -it --gpus all \\\n  --shm-size=32g \\\n  --ipc=host \\\n  --network=host \\\n  lmsysorg/sglang:dev-pr-15207 bash\n\n# Start the server\nSGLANG_ENABLE_SPEC_V2=1 python3 -m sglang.launch_server \\\n        --model-path XiaomiMiMo/MiMo-V2-Flash \\\n        --dp-size 2 \\\n        --enable-dp-attention \\\n        --tp-size 8 \\\n        --trust-remote-code \\\n        --mem-fraction-static 0.75 \\\n        --max-running-requests 128 \\\n        --chunked-prefill-size 16384 \\\n        --reasoning-parser qwen3 \\\n        --tool-call-parser mimo \\\n        --model-loader-extra-config '{\"enable_multithread_load\": \"true\",\"num_threads\": 64}' \\\n        --attention-backend fa3 \\\n        --speculative-algorithm EAGLE \\\n        --speculative-num-steps=3 \\\n        --speculative-eagle-topk=1 \\\n        --speculative-num-draft-tokens=4 \\\n        --enable-mtp\n```\n\n</details>\n\n<br>\n\n<details>\n<summary><span style=\"font-size: 1.3em; font-weight: bold;\">Pip Installation</span></summary>\n\n```bash\n# On a machine with SGLang dependencies installed or inside a SGLang nightly container\n# Start an SGLang nightly container\ndocker run -it --gpus all \\\n  --shm-size=32g \\\n  --ipc=host \\\n  --network=host \\\n  lmsysorg/sglang:nightly-dev-20251215-4449c170 bash\n\n# If you already have SGLang installed, uninstall the current SGLang version\npip uninstall sglang -y\n\n# Install the PyPI Package\npip install sglang==0.5.6.post2.dev7970+pr.15207.g62f95e0c6 \\\n  --index-url https://sgl-project.github.io/whl/pr/ \\\n  --extra-index-url https://pypi.org/simple\n\n#Launch the server\nSGLANG_ENABLE_SPEC_V2=1 python3 -m sglang.launch_server \\\n        --model-path XiaomiMiMo/MiMo-V2-Flash \\\n        --dp-size 2 \\\n        --enable-dp-attention \\\n        --tp-size 8 \\\n        --trust-remote-code \\\n        --mem-fraction-static 0.75 \\\n        --max-running-requests 128 \\\n        --chunked-prefill-size 16384 \\\n        --reasoning-parser qwen3 \\\n        --tool-call-parser mimo \\\n        --model-loader-extra-config '{\"enable_multithread_load\": \"true\",\"num_threads\": 64}' \\\n        --attention-backend fa3 \\\n        --speculative-algorithm EAGLE \\\n        --speculative-num-steps=3 \\\n        --speculative-eagle-topk=1 \\\n        --speculative-num-draft-tokens=4 \\\n        --enable-mtp\n```\n\n</details>\n\n<br>\n\n<details>\n<summary><span style=\"font-size: 1.3em; font-weight: bold;\">Testing the deployment</span></summary>\n\nOnce the server is running, test it with a chat completion request in another terminal:\n\n```bash\ncurl http://localhost:30000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"XiaomiMiMo/MiMo-V2-Flash\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello! What can you help me with?\"}\n    ],\n    \"temperature\": 0.7,\n    \"max_tokens\": 100\n  }'\n\n```\n\n**Expected response:**\n\n```json\n{\n  \"id\": \"...\",\n  \"object\": \"chat.completion\",\n  \"model\": \"XiaomiMiMo/MiMo-V2-Flash\",\n  \"choices\": [{\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"Hello! I can help you with...\"\n    }\n  }]\n}\n```\n\n</details>\n\n<br>\n\n<details>\n<summary><span style=\"font-size: 1.3em; font-weight: bold;\">Troubleshooting</span></summary>\n\n**DeepGEMM Timeout Error**\nOccasionally DeepGEMM timeout errors occur during first launch. Simply rerun the server command in the same container - the compiled kernels are cached and subsequent launches will be fast.\n\n\n</details>","slug":"2025-12-16-mimo-v2-flash"},"__N_SSG":true}