{"pageProps":{"frontmatter":{"title":"Optimizing GPT-OSS on NVIDIA DGX Spark: Getting the Most Out of Your Spark","author":"Jerry Zhou","date":"November 3, 2025","previewImg":"/images/blog/gpt_oss_on_nvidia_dgx_spark/preview.jpg"},"content":"\nWe’ve got some exciting updates about the **NVIDIA DGX Spark**\\! In the week following the official launch, we collaborated closely with NVIDIA and successfully brought **GPT-OSS 20B** and **GPT-OSS 120B** support to **SGLang** on the DGX Spark. The results are impressive: around **70 tokens/s** on GPT-OSS 20B and **50 tokens/s** on GPT-OSS 120B, which is state-of-the-art so far, and makes running a **local coding agent** on the DGX Spark fully viable.\n\n![](/images/blog/gpt_oss_on_nvidia_dgx_spark/demo_1.png)\n\n> We’ve updated our detailed benchmark results <a href=\"https://docs.google.com/spreadsheets/d/1SF1u0J2vJ-ou-R_Ry1JZQ0iscOZL8UKHpdVFr85tNLU/edit?usp=sharing\" target=\"_blank\">here</a>, and check out our demo video <a href=\"https://youtu.be/ApIVoTuWIss\" target=\"_blank\">here</a>.\n\nIn this post, you’ll learn how to:\n\n* Run GPT-OSS 20B or 120B with SGLang on the DGX Spark  \n* Benchmark performance locally  \n* Hook it up to **Open WebUI** for chatting  \n* Even run **Claude Code** entirely locally via **LMRouter**\n\n## 1\\. Preparing the Environment\n\nBefore launching SGLang, make sure you have the proper **tiktoken encodings** for OpenAI Harmony:\n\n```bash\nmkdir -p ~/tiktoken_encodings\nwget -O ~/tiktoken_encodings/o200k_base.tiktoken \"https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken\"\nwget -O ~/tiktoken_encodings/cl100k_base.tiktoken \"https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\"\n```\n\n## 2\\. Launching SGLang with Docker\n\nNow, launch the SGLang server with the following command:\n\n```bash\ndocker run --gpus all \\\n    --shm-size 32g \\\n    -p 30000:30000 \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface -v ~/tiktoken_encodings:/tiktoken_encodings \\\n    --env \"HF_TOKEN=<secret>\" --env \"TIKTOKEN_ENCODINGS_BASE=/tiktoken_encodings\" \\\n    --ipc=host \\\n    lmsysorg/sglang:spark \\\n    python3 -m sglang.launch_server --model-path openai/gpt-oss-20b --host 0.0.0.0 --port 30000 --reasoning-parser gpt-oss --tool-call-parser gpt-oss\n```\n\nReplace `<secret>` with your **Hugging Face access token**. If you’d like to run **GPT-OSS 120B**, simply change the model path to: `openai/gpt-oss-120b`. This model is roughly 6× larger than the 20B version, so it will take a bit longer to load. For best performance and stability, consider enabling **swap memory** on your DGX Spark.\n\n## 3\\. Testing the Server\n\nOnce SGLang is running, you can send OpenAI-compatible requests directly:\n\n```bash\ncurl http://localhost:30000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"How many letters are there in the word SGLang?\"\n            }\n        ]\n    }'\n```\n\n![](/images/blog/gpt_oss_on_nvidia_dgx_spark/demo_2.jpg)\n\n## 4\\. Benchmarking Performance\n\nA quick way to benchmark throughput is to request a long output, such as:\n\n```bash\ncurl http://localhost:30000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"Generate a long story. The only requirement is long.\"           \n            }\n        ]\n    }'\n```\n\nYou should see around **70 tokens per second** with GPT-OSS 20B under typical conditions.\n\n## 5\\. Running a Local Chatbot (Open WebUI)\n\nTo set up a friendly local chat interface, you can install **Open WebUI** on your DGX Spark and point it to your running SGLang backend: `http://localhost:30000/v1`. Follow the <a href=\"https://github.com/open-webui/open-webui?tab=readme-ov-file#how-to-install-\" target=\"_blank\">Open WebUI installation instructions</a> to get it up and running. Once connected, you’ll be able to chat seamlessly with your local GPT-OSS instance. No internet required.\n\n![](/images/blog/gpt_oss_on_nvidia_dgx_spark/demo_3.jpg)\n\n## 6\\. Running Claude Code Entirely Locally\n\nWith a local GPT-OSS model running, you can even connect **Claude Code** through <a href=\"https://github.com/LMRouter/lmrouter\" target=\"_blank\">**LMRouter**</a>, which is able to convert Anthropic-style requests into OpenAI-compatible ones.\n\n### Step 1: Create the LMRouter Config\n\nSave <a href=\"https://gist.github.com/yvbbrjdr/0514a32124682f97370dda9c09c3349c\" target=\"_blank\">this file</a> as `lmrouter-sglang.yaml`.\n\n### Step 2: Launch LMRouter\n\nInstall <a href=\"https://pnpm.io/installation\" target=\"_blank\">**pnpm**</a> (if not already installed), then run:\n\n```bash\npnpx @lmrouter/cli lmrouter-sglang.yaml\n```\n\n### Step 3: Start Claude Code\n\nInstall **Claude Code** following its <a href=\"https://www.claude.com/product/claude-code\" target=\"_blank\">setup guide</a>, then launch it as follows:\n\n```bash\nANTHROPIC_BASE_URL=http://localhost:3000/anthropic \\\nANTHROPIC_AUTH_TOKEN=sk-sglang claude\n```\n\nThat’s it\\! You can now use **Claude Code locally**, powered entirely by **GPT-OSS 20B or 120B on your DGX Spark**.\n\n![](/images/blog/gpt_oss_on_nvidia_dgx_spark/demo_4.jpg)\n\n## 7\\. Conclusion\n\nWith these steps, you can fully unlock the potential of the **DGX Spark**, turning it into a local AI powerhouse capable of running multi-tens-of-billion-parameter models interactively.\n","slug":"2025-11-03-gpt-oss-on-nvidia-dgx-spark"},"__N_SSG":true}