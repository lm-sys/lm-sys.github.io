{"pageProps":{"frontmatter":{"title":"Boost SGLang Inference: Native NVIDIA Model Optimizer Integration for Seamless Quantization and Deployment","author":"NVIDIA ModelOpt Team","date":"Dec 02, 2025","previewImg":"/images/blog/nvidia-modelopt-quantization/DSR1-nvfp4-perf.jpg"},"content":"\n(Updated on Dec 2)\n\nWe are thrilled to announce a major new feature in SGLang: native support for [NVIDIA Model Optimizer](https://github.com/NVIDIA/TensorRT-Model-Optimizer) quantization! This integration streamlines the entire model optimization and deployment process, allowing you to go from a full-precision model to a high-performance, quantized endpoint entirely within the SGLang ecosystem.\n\nServing large language models efficiently is one of the biggest challenges in production. Model quantization is a critical technique for reducing the memory footprint and increasing inference speed of a model. Prior to this feature the process required multi-step workflows and separate tools for model optimization and deployment.\n\nWith our latest updates (via PRs [#7149](https://github.com/sgl-project/sglang/pull/7149), [#9991](https://github.com/sgl-project/sglang/pull/9991), and [#10154](https://github.com/sgl-project/sglang/pull/10154)), we’ve eliminated that complexity.\n\nThe optimizations from Model Optimizer and SGLang can deliver up to 2x better per GPU throughput comparing NVFP4 and FP8 inference.\n\n\n### What’s New: Direct ModelOpt APIs in SGLang\n\nSGLang now integrates NVIDIA's Model Optimizer directly, allowing you to call its powerful quantization APIs from your SGLang code.\n\nThis new capability unlocks a simple, three-step workflow:\n\n- **Quantize**: Use the new SGLang-ModelOpt interface to apply state-of-the-art quantization techniques that enable accelerated low-precision inference in NVFP4, MXFP4, FP8, etc.\n\n- **Export**: Save the optimized model artifacts, now fully compatible with the SGLang runtime.\n\n- **Deploy**: Load the quantized model directly into the SGLang runtime and serve it on NVIDIA platforms, immediately benefiting from lower latency and reduced memory usage.\n\n\n#### Performance Outcomes\nThe models optimized through this new API enable significant performance boost. Better yet these optimizations can be stacked with other software components in the NVIDIA software-hardware stack and across the various embodiments of the latest Blackwell architecture, from the DGX Spark to GB300 NVL72.\n\n\n![DSR1-nvfp4-perf.jpg](/images/blog/nvidia-modelopt-quantization/DSR1-nvfp4-perf.jpg)\n\nThis figure shows NVIDIA B200 per GPU throughput vs End-to-End Latency for DeepSeek-R1-0528 across multiple configurations using Model Optimizer NVFP4 quantized model. This figure compares the original FP8 and NVFP4. DeepSeek-R1-0528 is not yet supported in this initial API release.\n\nAs measured by the [latest results from InferenceMAX](https://lmsys.org/blog/2025-10-14-sa-inference-max/), the optimizations from Model Optimizer and SGLang can deliver up to 2x better per GPU throughput compared to an original FP8 baseline. These performance benefits are coming soon through the native integration discussed in this blog.\n\n\n### How to Get Started\nSGLang provides [an example script](https://github.com/sgl-project/sglang/blob/main/examples/usage/modelopt_quantize_and_export.py) that demonstrates the complete Model Optimizer quantization and export workflow. You can also follow the code snippet below to run quantization and export for your models. Please make sure you installed `nvidia-modelopt` and `accelerate` in your SGLang environment.\n\n```\nimport sglang as sgl\nfrom sglang.srt.configs.device_config import DeviceConfig\nfrom sglang.srt.configs.load_config import LoadConfig\nfrom sglang.srt.configs.model_config import ModelConfig\nfrom sglang.srt.model_loader.loader import get_model_loader\n\n# Configure model with ModelOpt quantization and export\nmodel_config = ModelConfig(\n\tmodel_path=\"Qwen/Qwen3-8B\",\n\tquantization=\"modelopt_fp8\",  # or \"modelopt_fp4\"\n\ttrust_remote_code=True,\n)\n\nload_config = LoadConfig(\n\tmodelopt_export_path=\"./quantized_qwen3_8b_fp8\",\n\tmodelopt_checkpoint_save_path=\"./checkpoint.pth\",  # optional, fake quantized checkpoint\n)\ndevice_config = DeviceConfig(device=\"cuda\")\n\n# Load and quantize the model (export happens automatically)\nmodel_loader = get_model_loader(load_config, model_config)\nquantized_model = model_loader.load_model(\n\tmodel_config=model_config,\n\tdevice_config=device_config,\n)\n```\n\nAfter quantization and export, you can deploy the model with SGLang:\n\n```\n# Deploy the exported quantized model\npython -m sglang.launch_server \\\n   --model-path ./quantized_qwen3_8b_fp8 \\\n   --quantization modelopt \\\n   --port 30000 --host 0.0.0.0\n```\n\nOr using the Python API:\n\n```\nimport sglang as sgl\nfrom transformers import AutoTokenizer\n\ndef main():\n   # Deploy exported ModelOpt quantized model\n   llm = sgl.Engine(\n      model_path=\"./quantized_qwen3_8b_fp8\",\n      quantization=\"modelopt\"\n   )\n\n   # Use chat template to format prompts for Qwen3-8B\n   tokenizer = AutoTokenizer.from_pretrained(\"./quantized_qwen3_8b_fp8\")\n\n   messages = [\n       [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n       [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n   ]\n\n   prompts = [\n       tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True)\n       for m in messages\n   ]\n\n   # Run inference\n   sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95, \"max_new_tokens\": 512}\n   outputs = llm.generate(prompts, sampling_params)\n\n   for i, output in enumerate(outputs):\n      print(f\"Prompt: {prompts[i]}\")\n      print(f\"Output: {output['text']}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n\n### Conclusion\n\nThis native Model Optimizer integration reinforces SGLang's commitment to providing a simple and powerful platform for LLM inference. We are continuing to close the gap between optimization and deployment of highly-performance models.\n\nWe can't wait to see the performance gains you achieve with this new feature. Head over to our [GitHub repository](https://github.com/sgl-project/sglang) to pull the latest version and try it out!\n\nAlso, please join our dedicated Slack channel [#modelopt](https://sgl-fru7574.slack.com/archives/C09NPJSBR32) to discuss topics such as modelopt, quantization, and low-precision numerics! If you haven’t joined our workspace yet, you can join it first [here] (https://slack.sglang.io).\n\n\n### Acknowledgement\n\nNvidia team: Zhiyu Cheng, Jingyu Xin, Huizi Mao, Eduardo Alvarez, Pen Chung Li, Omri Almog\n\nSGLang team and community: Qiaolin Yu, Xinyuan Tong\n","slug":"2025-12-02-modelopt-quantization"},"__N_SSG":true}